This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, line numbers have been added, security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: plan.md, SUPABASE_IMPLEMENTATION.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Line numbers have been added to the beginning of each line
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
video_ingest_tool/
  __init__.py
  __main__.py
  auth.py
  cli.py
  config.py
  database_storage.py
  discovery.py
  embeddings.py
  extractors_extended.py
  extractors_hdr.py
  extractors.py
  hybrid_search.sql
  models.py
  output.py
  pipeline.py
  processor.py
  processors.py
  search.py
  supabase_config.py
  utils.py
  video_processor.py
.env.example
.gitattributes
.gitignore
.mcp.json
CLAUDE.md
complete_policy_reset.sql
DATABASE_SETUP_INSTRUCTIONS.md
database_setup.sql
disable_rls_for_testing.sql
fix_rls_policies.sql
fix_user_profiles.sql
fix_vectors_rls.sql
requirements.txt
simple_disable_rls.sql
SUPABASE_FIX.sql
test_supabase.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="video_ingest_tool/__init__.py">
1: """
2: AI-Powered Video Ingest & Catalog Tool
3: A tool for video content discovery, metadata extraction, and cataloging.
4: """
5: __version__ = "0.1.0"
</file>

<file path="video_ingest_tool/__main__.py">
1: """
2: Entry point for the video ingest tool.
3: Import and run the CLI when the package is invoked directly.
4: """
5: from .cli import app
6: if __name__ == "__main__":
7:     app()
</file>

<file path="video_ingest_tool/auth.py">
  1: """
  2: CLI Authentication module for Supabase integration.
  3: """
  4: import json
  5: import os
  6: import time
  7: import getpass
  8: from pathlib import Path
  9: from typing import Optional, Dict, Any
 10: import typer
 11: import structlog
 12: from supabase import Client
 13: from .supabase_config import get_supabase_client
 14: logger = structlog.get_logger(__name__)
 15: # Auth file location
 16: AUTH_FILE = Path.home() / ".video_ingest_auth.json"
 17: class AuthManager:
 18:     """Manages CLI authentication with Supabase."""
 19:     def __init__(self):
 20:         self.client: Optional[Client] = None
 21:     def login(self, email: str, password: str) -> bool:
 22:         """Login with email and password (based on official Supabase docs)."""
 23:         try:
 24:             self.client = get_supabase_client()
 25:             # Authenticate (official pattern from supabase-py docs)
 26:             response = self.client.auth.sign_in_with_password({
 27:                 "email": email, 
 28:                 "password": password
 29:             })
 30:             if response.user and response.session:
 31:                 # Save session
 32:                 self._save_session(response.session)
 33:                 logger.info(f"Successfully logged in as {email}")
 34:                 return True
 35:             else:
 36:                 logger.error("Login failed: No user or session returned")
 37:                 return False
 38:         except Exception as e:
 39:             logger.error(f"Login failed: {str(e)}")
 40:             return False    
 41:     def signup(self, email: str, password: str) -> bool:
 42:         """Sign up new user (based on official Supabase docs)."""
 43:         try:
 44:             self.client = get_supabase_client()
 45:             # Sign up (official pattern from supabase-py docs)
 46:             response = self.client.auth.sign_up({
 47:                 "email": email,
 48:                 "password": password
 49:             })
 50:             if response.user:
 51:                 logger.info(f"Successfully signed up {email}")
 52:                 return True
 53:             else:
 54:                 logger.error("Sign up failed: No user returned")
 55:                 return False
 56:         except Exception as e:
 57:             logger.error(f"Sign up failed: {str(e)}")
 58:             return False
 59:     def logout(self) -> bool:
 60:         """Logout and clear stored session."""
 61:         try:
 62:             # Clear local session file
 63:             if AUTH_FILE.exists():
 64:                 AUTH_FILE.unlink()
 65:                 logger.info("Successfully logged out")
 66:                 return True
 67:             else:
 68:                 logger.info("No active session found")
 69:                 return True
 70:         except Exception as e:
 71:             logger.error(f"Logout failed: {str(e)}")
 72:             return False
 73:     def get_current_session(self) -> Optional[Dict[str, Any]]:
 74:         """Get current session if valid."""
 75:         if not AUTH_FILE.exists():
 76:             return None
 77:         try:
 78:             session_data = json.loads(AUTH_FILE.read_text())
 79:             # Check if token is expired
 80:             expires_at = session_data.get('expires_at', 0)
 81:             if expires_at < time.time():
 82:                 # Try to refresh token
 83:                 return self._refresh_session(session_data)
 84:             return session_data
 85:         except Exception as e:
 86:             logger.error(f"Failed to load session: {str(e)}")
 87:             return None    
 88:     def _refresh_session(self, old_session: Dict[str, Any]) -> Optional[Dict[str, Any]]:
 89:         """Refresh an expired session using the refresh token.
 90:         Args:
 91:             old_session: The expired session data containing access and refresh tokens
 92:         Returns:
 93:             Optional[Dict[str, Any]]: The new session data if refresh successful, None on failure
 94:         """
 95:         try:
 96:             client = get_supabase_client()
 97:             # Set the expired session to use for refresh
 98:             client.auth.set_session(
 99:                 access_token=old_session['access_token'],
100:                 refresh_token=old_session['refresh_token']
101:             )
102:             # Attempt to refresh the session
103:             new_session = client.auth.refresh_session()
104:             if not new_session:
105:                 logger.error("Failed to refresh session - no new session data returned")
106:                 return None
107:             # Save the refreshed session
108:             session_data = {
109:                 'access_token': new_session.access_token,
110:                 'refresh_token': new_session.refresh_token,
111:                 'expires_at': time.time() + new_session.expires_in
112:             }
113:             AUTH_FILE.write_text(json.dumps(session_data))
114:             logger.info("Successfully refreshed session")
115:             return session_data
116:         except Exception as e:
117:             logger.error(f"Failed to refresh session: {str(e)}")
118:             return None
119:     def get_authenticated_client(self) -> Optional[Client]:
120:         """Get authenticated Supabase client."""
121:         session = self.get_current_session()
122:         if not session:
123:             return None
124:         try:
125:             client = get_supabase_client()
126:             # Set the session (official pattern)
127:             client.auth.set_session(
128:                 access_token=session['access_token'],
129:                 refresh_token=session['refresh_token']
130:             )
131:             return client
132:         except Exception as e:
133:             logger.error(f"Failed to create authenticated client: {str(e)}")
134:             return None
135:     def get_user_profile(self) -> Optional[Dict[str, Any]]:
136:         """Get current user profile."""
137:         client = self.get_authenticated_client()
138:         if not client:
139:             return None
140:         try:
141:             result = client.rpc('get_user_profile').execute()
142:             if result.data:
143:                 return result.data[0] if isinstance(result.data, list) else result.data
144:             return None
145:         except Exception as e:
146:             logger.error(f"Failed to get user profile: {str(e)}")
147:             return None
148:     def is_admin(self) -> bool:
149:         """Check if current user is admin.
150:         Returns:
151:             bool: True if user is an admin, False if not admin or if profile is missing/invalid
152:         """
153:         try:
154:             profile = self.get_user_profile()
155:             if not profile:
156:                 logger.warning("Could not determine admin status - profile not found")
157:                 return False
158:             profile_type = profile.get('profile_type')
159:             if not profile_type:
160:                 logger.warning("Could not determine admin status - profile_type missing")
161:                 return False
162:             return profile_type == 'admin'
163:         except Exception as e:
164:             logger.error(f"Error checking admin status: {str(e)}")
165:             return False
166:     def _save_session(self, session) -> None:
167:         """Save session to local file."""
168:         session_data = {
169:             "access_token": session.access_token,
170:             "refresh_token": session.refresh_token,
171:             "expires_at": session.expires_at,
172:             "user_id": session.user.id if session.user else None,
173:             "email": session.user.email if session.user else None
174:         }
175:         # Save with restricted permissions
176:         AUTH_FILE.write_text(json.dumps(session_data, indent=2))
177:         AUTH_FILE.chmod(0o600)  # Read/write for owner only
</file>

<file path="video_ingest_tool/cli.py">
  1: """
  2: Command-line interface for the video ingest tool.
  3: Contains the CLI application built with Typer.
  4: """
  5: import os
  6: import time
  7: import json
  8: import typer
  9: from typing import List, Dict, Optional
 10: from rich.console import Console
 11: from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
 12: from rich.panel import Panel
 13: from rich.table import Table
 14: from .config import setup_logging, console
 15: from .discovery import scan_directory
 16: from .processor import (
 17:     process_video_file, 
 18:     get_default_pipeline_config, 
 19:     get_available_pipeline_steps
 20: )
 21: from .output import save_to_json, save_run_outputs
 22: from .video_processor import DEFAULT_COMPRESSION_CONFIG
 23: from .utils import calculate_checksum
 24: # Create Typer app
 25: app = typer.Typer(help="AI-Powered Video Ingest & Catalog Tool")
 26: @app.command()
 27: def ingest(
 28:     directory: str = typer.Argument(..., help="Directory to scan for video files"),
 29:     recursive: bool = typer.Option(True, "--recursive/--no-recursive", "-r/-nr", help="Scan subdirectories"),
 30:     output_dir: str = typer.Option("output", "--output-dir", "-o", help="Base output directory for all processing runs"),
 31:     limit: int = typer.Option(0, "--limit", "-l", help="Limit number of files to process (0 = no limit)"),
 32:     disable_steps: List[str] = typer.Option(None, "--disable", "-d", help="Steps to disable in the pipeline"),
 33:     enable_steps: List[str] = typer.Option(None, "--enable", "-e", help="Steps to enable in the pipeline"),
 34:     config_file: Optional[str] = typer.Option(None, "--config", "-c", help="JSON configuration file for pipeline steps"),
 35:     compression_fps: int = typer.Option(DEFAULT_COMPRESSION_CONFIG['fps'], "--fps", help=f"Frame rate for compressed videos (default: {DEFAULT_COMPRESSION_CONFIG['fps']})"),
 36:     compression_bitrate: str = typer.Option(DEFAULT_COMPRESSION_CONFIG['video_bitrate'], "--bitrate", help=f"Video bitrate for compression (default: {DEFAULT_COMPRESSION_CONFIG['video_bitrate']})"),
 37:     store_database: bool = typer.Option(False, "--store-database", help="Store results in Supabase database (requires authentication)"),
 38:     generate_embeddings: bool = typer.Option(False, "--generate-embeddings", help="Generate vector embeddings for semantic search (requires authentication)"),
 39:     force_reprocess: bool = typer.Option(False, "--force-reprocess", "-f", help="Force reprocessing of files even if they already exist in database")
 40: ):
 41:     """
 42:     Scan a directory for video files and extract metadata.
 43:     """
 44:     start_time = time.time()
 45:     # Setup logging and get paths - this creates the run directory structure
 46:     logger, timestamp, json_dir, log_file = setup_logging()
 47:     # The run directory is already created by setup_logging
 48:     # Extract run directory from json_dir path
 49:     run_dir = os.path.dirname(json_dir)  # json_dir is run_dir/json, so get parent
 50:     # Create subdirectories for this run (json directory already created by setup_logging)
 51:     thumbnails_dir = os.path.join(run_dir, "thumbnails")
 52:     os.makedirs(thumbnails_dir, exist_ok=True)
 53:     # JSON directory already exists from setup_logging
 54:     # json_dir is already set to run_dir/json
 55:     # Create identifiable summary filename with timestamp
 56:     summary_filename = f"all_videos_{os.path.basename(directory)}_{timestamp}.json"
 57:     logger.info("Starting ingestion process", 
 58:                 directory=directory, 
 59:                 recursive=recursive,
 60:                 run_dir=run_dir,
 61:                 limit=limit,
 62:                 compression_fps=compression_fps,
 63:                 compression_bitrate=compression_bitrate)
 64:     # Set up pipeline configuration
 65:     pipeline_config = get_default_pipeline_config()
 66:     # Load config from file if specified
 67:     if config_file:
 68:         try:
 69:             with open(config_file, 'r') as f:
 70:                 file_config = json.load(f)
 71:                 pipeline_config.update(file_config)
 72:                 logger.info(f"Loaded pipeline configuration from {config_file}")
 73:         except Exception as e:
 74:             logger.error(f"Error loading config file: {str(e)}")
 75:             console.print(f"[bold red]Error loading config file:[/bold red] {str(e)}")
 76:     # Apply command-line overrides
 77:     if disable_steps:
 78:         for step in disable_steps:
 79:             if step in pipeline_config:
 80:                 pipeline_config[step] = False
 81:                 logger.info(f"Disabled step: {step}")
 82:             else:
 83:                 logger.warning(f"Unknown step to disable: {step}")
 84:                 console.print(f"[yellow]Warning:[/yellow] Unknown step '{step}'")
 85:     if enable_steps:
 86:         for step in enable_steps:
 87:             if step in pipeline_config:
 88:                 pipeline_config[step] = True
 89:                 logger.info(f"Enabled step: {step}")
 90:             else:
 91:                 logger.warning(f"Unknown step to enable: {step}")
 92:                 console.print(f"[yellow]Warning:[/yellow] Unknown step '{step}'")
 93:     # Handle database storage and embeddings
 94:     if store_database or generate_embeddings:
 95:         from .auth import AuthManager
 96:         from .supabase_config import verify_connection
 97:         # Check Supabase connection
 98:         if not verify_connection():
 99:             console.print("[bold red]Error:[/bold red] Cannot connect to Supabase database")
100:             console.print("Please check your .env file and Supabase configuration")
101:             raise typer.Exit(1)
102:         # Check authentication
103:         auth_manager = AuthManager()
104:         if not auth_manager.get_current_session():
105:             console.print("[bold red]Error:[/bold red] Database storage requires authentication")
106:             console.print("Please run: [cyan]python -m video_ingest_tool auth login[/cyan]")
107:             raise typer.Exit(1)
108:         # Enable database storage if requested
109:         if store_database:
110:             pipeline_config['database_storage'] = True
111:             logger.info("Enabled database storage")
112:             console.print("[green]✓[/green] Database storage enabled")
113:         # Enable embeddings if requested (also requires database storage)
114:         if generate_embeddings:
115:             pipeline_config['generate_embeddings'] = True
116:             pipeline_config['database_storage'] = True  # Embeddings require database
117:             logger.info("Enabled vector embeddings generation")
118:             console.print("[green]✓[/green] Vector embeddings enabled")
119:     # Save the active configuration to the run directory
120:     config_path = os.path.join(run_dir, "pipeline_config.json")
121:     with open(config_path, 'w') as f:
122:         json.dump(pipeline_config, f, indent=2)
123:     console.print(Panel.fit(
124:         "[bold blue]AI-Powered Video Ingest & Catalog Tool[/bold blue]\n"
125:         f"[cyan]Directory:[/cyan] {directory}\n"
126:         f"[cyan]Recursive:[/cyan] {recursive}\n"
127:         f"[cyan]Output Directory:[/cyan] {run_dir}\n"
128:         f"[cyan]File Limit:[/cyan] {limit if limit > 0 else 'No limit'}\n"
129:         f"[cyan]Log File:[/cyan] {log_file}\n"
130:         f"[cyan]Pipeline Config:[/cyan] {config_path}",
131:         title="Alpha Test",
132:         border_style="green"
133:     ))
134:     # Display active pipeline steps
135:     steps_table = Table(title="Active Pipeline Steps")
136:     steps_table.add_column("Step", style="cyan")
137:     steps_table.add_column("Status", style="green")
138:     steps_table.add_column("Description", style="yellow")
139:     for step in get_available_pipeline_steps():
140:         status = "[green]Enabled" if pipeline_config.get(step['name'], step['enabled']) else "[red]Disabled"
141:         steps_table.add_row(step['name'], status, step['description'])
142:     console.print(steps_table)
143:     console.print(f"[bold yellow]Step 1:[/bold yellow] Scanning directory for video files...")
144:     video_files = scan_directory(directory, recursive, logger)
145:     if limit > 0 and len(video_files) > limit:
146:         video_files = video_files[:limit]
147:         logger.info("Applied file limit", limit=limit)
148:     console.print(f"[green]Found {len(video_files)} video files[/green]")
149:     console.print(f"[bold yellow]Step 2:[/bold yellow] Processing video files...")
150:     processed_files = []
151:     failed_files = []
152:     skipped_files = []
153:     with Progress(
154:         SpinnerColumn(),
155:         TextColumn("[progress.description]{task.description}"),
156:         BarColumn(),
157:         TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
158:         TimeRemainingColumn(),
159:         console=console,  
160:         transient=True    
161:     ) as progress:
162:         task = progress.add_task("[green]Processing videos...", total=len(video_files))
163:         for file_path in video_files:
164:             progress.update(task, advance=0, description=f"[cyan]Processing {os.path.basename(file_path)}")
165:             try:
166:                 result = process_video_file(
167:                     file_path, 
168:                     thumbnails_dir, 
169:                     logger,
170:                     config=pipeline_config,
171:                     compression_fps=compression_fps,
172:                     compression_bitrate=compression_bitrate,
173:                     force_reprocess=force_reprocess
174:                 )
175:                 # Handle skipped files (duplicates)
176:                 if isinstance(result, dict) and result.get('skipped'):
177:                     skipped_files.append({
178:                         'file_path': file_path,
179:                         'reason': result.get('reason'),
180:                         'existing_clip_id': result.get('existing_clip_id'),
181:                         'existing_file_name': result.get('existing_file_name'),
182:                         'existing_processed_at': result.get('existing_processed_at')
183:                     })
184:                     logger.info("Skipped duplicate file", 
185:                                file=file_path, 
186:                                existing_id=result.get('existing_clip_id'))
187:                 else:
188:                     # Normal processing result
189:                     video_file = result
190:                     processed_files.append(video_file)
191:                     # Create filename with original name and UUID
192:                     base_name = os.path.splitext(os.path.basename(file_path))[0]
193:                     json_filename = f"{base_name}_{video_file.id}.json"
194:                     # Save individual JSON to run directory
195:                     individual_json_path = os.path.join(json_dir, json_filename)
196:                     save_to_json(video_file, individual_json_path, logger)
197:             except Exception as e:
198:                 failed_files.append(file_path)
199:                 logger.error("Error processing video file", path=file_path, error=str(e))
200:             progress.update(task, advance=1)
201:     # Save run outputs with directory name in the summary filename
202:     output_paths = save_run_outputs(
203:         processed_files,
204:         run_dir,
205:         summary_filename,
206:         json_dir,
207:         log_file,
208:         logger
209:     )
210:     # Check if we had skipped files and inform the user
211:     if skipped_files:
212:         console.print(f"[bold yellow]Info:[/bold yellow] Skipped {len(skipped_files)} duplicate file(s):", style="yellow")
213:         for skipped in skipped_files:
214:             console.print(f"  - {os.path.basename(skipped['file_path'])} (exists as {skipped['existing_file_name']})", style="yellow")
215:         if not force_reprocess:
216:             console.print(f"[dim]Use --force-reprocess to reprocess these files[/dim]")
217:     # Check if we had failed files and warn the user
218:     if failed_files:
219:         console.print(f"[bold red]Warning:[/bold red] Failed to process {len(failed_files)} file(s):", style="red")
220:         for f in failed_files:
221:             console.print(f"  - {f}", style="red")
222:     end_time = time.time()
223:     processing_time = end_time - start_time
224:     summary_table = Table(title="Processing Summary")
225:     summary_table.add_column("Metric", style="cyan")
226:     summary_table.add_column("Value", style="green")
227:     summary_table.add_row("Total files processed", str(len(processed_files)))
228:     if skipped_files:
229:         summary_table.add_row("Skipped files (duplicates)", str(len(skipped_files)))
230:     if failed_files:
231:         summary_table.add_row("Failed files", str(len(failed_files)))
232:     summary_table.add_row("Processing time", f"{processing_time:.2f} seconds")
233:     summary_table.add_row("Average time per file", f"{processing_time / len(processed_files):.2f} seconds" if processed_files else "N/A")
234:     summary_table.add_row("Run directory", run_dir)
235:     summary_table.add_row("Summary JSON", output_paths.get('run_summary', 'N/A'))
236:     summary_table.add_row("Log file", output_paths.get('run_log', 'N/A'))
237:     console.print(summary_table)
238:     logger.info("Ingestion process completed", 
239:                 files_processed=len(processed_files),
240:                 skipped_files=len(skipped_files),
241:                 failed_files=len(failed_files),
242:                 processing_time=processing_time,
243:                 run_directory=run_dir)
244: @app.command()
245: def list_steps():
246:     """
247:     List all available processing steps.
248:     """
249:     steps_table = Table(title="Available Pipeline Steps")
250:     steps_table.add_column("Step", style="cyan")
251:     steps_table.add_column("Default Status", style="green")
252:     steps_table.add_column("Description", style="yellow")
253:     for step in get_available_pipeline_steps():
254:         status = "[green]Enabled" if step['enabled'] else "[red]Disabled"
255:         steps_table.add_row(step['name'], status, step['description'])
256:     console.print(steps_table)
257:     console.print("\n[cyan]Example usage:[/cyan]")
258:     console.print("  python -m video_ingest_tool ingest /path/to/videos/ --disable=hdr_extraction,ai_focal_length")
259:     console.print("  python -m video_ingest_tool ingest /path/to/videos/ --enable=thumbnail_generation --disable=exposure_analysis")
260:     console.print("  python -m video_ingest_tool ingest /path/to/videos/ --config=my_config.json")
261: # Search commands
262: search_app = typer.Typer(help="Search video catalog")
263: app.add_typer(search_app, name="search")
264: @search_app.command("query")
265: def search_videos(
266:     query: str = typer.Argument(..., help="Search query"),
267:     search_type: str = typer.Option("hybrid", "--type", "-t", help="Search type: semantic, fulltext, hybrid, transcripts"),
268:     limit: int = typer.Option(10, "--limit", "-l", help="Maximum number of results"),
269:     show_scores: bool = typer.Option(True, "--scores/--no-scores", help="Show similarity/ranking scores"),
270:     summary_weight: float = typer.Option(1.0, "--summary-weight", help="Weight for summary embeddings (hybrid/semantic)"),
271:     keyword_weight: float = typer.Option(0.8, "--keyword-weight", help="Weight for keyword embeddings (hybrid/semantic)"),
272:     fulltext_weight: float = typer.Option(1.0, "--fulltext-weight", help="Weight for full-text search (hybrid)"),
273:     output_format: str = typer.Option("table", "--format", help="Output format: table, json")
274: ):
275:     """Search the video catalog using various search methods."""
276:     from .search import VideoSearcher, format_search_results, format_duration
277:     # Validate search type
278:     valid_types = ["semantic", "fulltext", "hybrid", "transcripts"]
279:     if search_type not in valid_types:
280:         console.print(f"[red]Error:[/red] Invalid search type. Must be one of: {', '.join(valid_types)}")
281:         raise typer.Exit(1)
282:     try:
283:         searcher = VideoSearcher()
284:         # Set weights for search
285:         weights = {
286:             'summary_weight': summary_weight,
287:             'keyword_weight': keyword_weight,
288:             'fulltext_weight': fulltext_weight
289:         }
290:         console.print(f"[cyan]Searching for:[/cyan] '{query}' [dim]({search_type} search)[/dim]")
291:         # Perform search
292:         results = searcher.search(
293:             query=query,
294:             search_type=search_type,
295:             match_count=limit,
296:             weights=weights
297:         )
298:         if not results:
299:             console.print("[yellow]No results found.[/yellow]")
300:             return
301:         # Format results
302:         formatted_results = format_search_results(results, search_type, show_scores)
303:         if output_format == "json":
304:             import json
305:             console.print(json.dumps(formatted_results, indent=2, default=str))
306:         else:
307:             # Display as table
308:             results_table = Table(title=f"Search Results ({len(results)} found)")
309:             results_table.add_column("File", style="cyan", max_width=30)
310:             results_table.add_column("Summary", style="green", max_width=50)
311:             results_table.add_column("Duration", style="blue")
312:             results_table.add_column("Category", style="magenta")
313:             if show_scores:
314:                 if search_type == "hybrid":
315:                     results_table.add_column("Score", style="yellow")
316:                     results_table.add_column("Type", style="dim")
317:                 elif search_type == "semantic":
318:                     results_table.add_column("Similarity", style="yellow")
319:                 elif search_type in ["fulltext", "transcripts"]:
320:                     results_table.add_column("Rank", style="yellow")
321:             for result in formatted_results:
322:                 row = [
323:                     result.get('file_name', 'Unknown'),
324:                     result.get('content_summary', 'No summary')[:100] + "..." if result.get('content_summary') else "No summary",
325:                     format_duration(result.get('duration_seconds', 0)),
326:                     result.get('content_category', 'Unknown')
327:                 ]
328:                 if show_scores:
329:                     if search_type == "hybrid":
330:                         row.extend([
331:                             f"{result.get('search_rank', 0):.3f}",
332:                             result.get('match_type', 'unknown')
333:                         ])
334:                     elif search_type == "semantic":
335:                         row.append(f"{result.get('combined_similarity', 0):.3f}")
336:                     elif search_type in ["fulltext", "transcripts"]:
337:                         row.append(f"{result.get('fts_rank', 0):.3f}")
338:                 results_table.add_row(*row)
339:             console.print(results_table)
340:             # Show example commands
341:             console.print(f"\n[dim]💡 To view details: python -m video_ingest_tool search show <clip_id>[/dim]")
342:     except ValueError as e:
343:         console.print(f"[red]Error:[/red] {str(e)}")
344:         console.print("Please run: [cyan]python -m video_ingest_tool auth login[/cyan]")
345:         raise typer.Exit(1)
346:     except Exception as e:
347:         console.print(f"[red]Search failed:[/red] {str(e)}")
348:         raise typer.Exit(1)
349: @search_app.command("similar")
350: def find_similar_videos(
351:     clip_id: str = typer.Argument(..., help="ID of the source clip"),
352:     limit: int = typer.Option(5, "--limit", "-l", help="Maximum number of similar clips"),
353:     threshold: float = typer.Option(0.5, "--threshold", "-t", help="Minimum similarity threshold"),
354:     output_format: str = typer.Option("table", "--format", help="Output format: table, json")
355: ):
356:     """Find videos similar to a given clip."""
357:     from .search import VideoSearcher, format_search_results, format_duration
358:     try:
359:         searcher = VideoSearcher()
360:         console.print(f"[cyan]Finding clips similar to:[/cyan] {clip_id}")
361:         results = searcher.find_similar(
362:             clip_id=clip_id,
363:             match_count=limit,
364:             similarity_threshold=threshold
365:         )
366:         if not results:
367:             console.print("[yellow]No similar clips found.[/yellow]")
368:             return
369:         formatted_results = format_search_results(results, "similar", True)
370:         if output_format == "json":
371:             import json
372:             console.print(json.dumps(formatted_results, indent=2, default=str))
373:         else:
374:             results_table = Table(title=f"Similar Clips ({len(results)} found)")
375:             results_table.add_column("File", style="cyan", max_width=30)
376:             results_table.add_column("Summary", style="green", max_width=50)
377:             results_table.add_column("Duration", style="blue")
378:             results_table.add_column("Category", style="magenta")
379:             results_table.add_column("Similarity", style="yellow")
380:             for result in formatted_results:
381:                 results_table.add_row(
382:                     result.get('file_name', 'Unknown'),
383:                     result.get('content_summary', 'No summary')[:100] + "..." if result.get('content_summary') else "No summary",
384:                     format_duration(result.get('duration_seconds', 0)),
385:                     result.get('content_category', 'Unknown'),
386:                     f"{result.get('similarity_score', 0):.3f}"
387:                 )
388:             console.print(results_table)
389:     except Exception as e:
390:         console.print(f"[red]Similar search failed:[/red] {str(e)}")
391:         raise typer.Exit(1)
392: @search_app.command("show")
393: def show_clip_details(
394:     clip_id: str = typer.Argument(..., help="ID of the clip to show"),
395:     show_transcript: bool = typer.Option(False, "--transcript", help="Show full transcript if available"),
396:     show_analysis: bool = typer.Option(False, "--analysis", help="Show AI analysis details")
397: ):
398:     """Show detailed information about a specific clip."""
399:     from .auth import AuthManager
400:     from .search import format_duration, format_file_size
401:     try:
402:         auth_manager = AuthManager()
403:         client = auth_manager.get_authenticated_client()
404:         if not client:
405:             console.print("[red]Error:[/red] Authentication required")
406:             console.print("Please run: [cyan]python -m video_ingest_tool auth login[/cyan]")
407:             raise typer.Exit(1)
408:         # Get clip details
409:         clip_result = client.table('clips').select('*').eq('id', clip_id).execute()
410:         if not clip_result.data:
411:             console.print(f"[red]Error:[/red] Clip with ID {clip_id} not found")
412:             raise typer.Exit(1)
413:         clip = clip_result.data[0]
414:         # Display clip information
415:         info_table = Table(title=f"Clip Details: {clip.get('file_name')}")
416:         info_table.add_column("Property", style="cyan")
417:         info_table.add_column("Value", style="green")
418:         info_table.add_row("ID", clip.get('id'))
419:         info_table.add_row("File Name", clip.get('file_name'))
420:         info_table.add_row("Local Path", clip.get('local_path'))
421:         info_table.add_row("Duration", format_duration(clip.get('duration_seconds', 0)))
422:         info_table.add_row("File Size", format_file_size(clip.get('file_size_bytes', 0)))
423:         info_table.add_row("Content Category", clip.get('content_category') or 'Unknown')
424:         info_table.add_row("Camera", f"{clip.get('camera_make', 'Unknown')} {clip.get('camera_model', '')}")
425:         info_table.add_row("Resolution", f"{clip.get('width')}x{clip.get('height')}" if clip.get('width') else 'Unknown')
426:         info_table.add_row("Frame Rate", f"{clip.get('frame_rate')} fps" if clip.get('frame_rate') else 'Unknown')
427:         info_table.add_row("Processed At", clip.get('processed_at'))
428:         console.print(info_table)
429:         # Show content summary
430:         if clip.get('content_summary'):
431:             console.print(f"\n[bold]Content Summary:[/bold]")
432:             console.print(clip['content_summary'])
433:         # Show content tags
434:         if clip.get('content_tags'):
435:             console.print(f"\n[bold]Content Tags:[/bold]")
436:             console.print(", ".join(clip['content_tags']))
437:         # Show transcript if requested
438:         if show_transcript:
439:             transcript_result = client.table('transcripts').select('full_text').eq('clip_id', clip_id).execute()
440:             if transcript_result.data and transcript_result.data[0].get('full_text'):
441:                 console.print(f"\n[bold]Transcript:[/bold]")
442:                 console.print(transcript_result.data[0]['full_text'])
443:             else:
444:                 console.print(f"\n[dim]No transcript available[/dim]")
445:         # Show AI analysis if requested
446:         if show_analysis:
447:             analysis_result = client.table('analysis').select('*').eq('clip_id', clip_id).execute()
448:             if analysis_result.data:
449:                 console.print(f"\n[bold]AI Analysis:[/bold]")
450:                 for analysis in analysis_result.data:
451:                     console.print(f"Type: {analysis.get('analysis_type')}")
452:                     console.print(f"Model: {analysis.get('ai_model')}")
453:                     console.print(f"Usability Rating: {analysis.get('usability_rating')}")
454:                     console.print(f"Speaker Count: {analysis.get('speaker_count')}")
455:             else:
456:                 console.print(f"\n[dim]No AI analysis available[/dim]")
457:     except Exception as e:
458:         console.print(f"[red]Failed to show clip details:[/red] {str(e)}")
459:         raise typer.Exit(1)
460: @search_app.command("stats")
461: def show_catalog_stats():
462:     """Show statistics about your video catalog."""
463:     from .search import VideoSearcher
464:     try:
465:         searcher = VideoSearcher()
466:         stats = searcher.get_user_stats()
467:         if not stats:
468:             console.print("[yellow]No statistics available.[/yellow]")
469:             return
470:         stats_table = Table(title="Video Catalog Statistics")
471:         stats_table.add_column("Metric", style="cyan")
472:         stats_table.add_column("Value", style="green")
473:         stats_table.add_row("Total Clips", str(stats.get('total_clips', 0)))
474:         stats_table.add_row("Total Duration", f"{stats.get('total_duration_hours', 0)} hours")
475:         stats_table.add_row("Total Storage", f"{stats.get('total_storage_gb', 0)} GB")
476:         stats_table.add_row("Clips with Transcripts", str(stats.get('clips_with_transcripts', 0)))
477:         stats_table.add_row("Clips with AI Analysis", str(stats.get('clips_with_ai_analysis', 0)))
478:         console.print(stats_table)
479:     except Exception as e:
480:         console.print(f"[red]Failed to get statistics:[/red] {str(e)}")
481:         raise typer.Exit(1)
482: if __name__ == "__main__":
483:     app()
484: # Authentication commands
485: auth_app = typer.Typer(help="Authentication commands")
486: app.add_typer(auth_app, name="auth")
487: @auth_app.command("login")
488: def auth_login():
489:     """Login to Supabase with email and password."""
490:     from .auth import AuthManager
491:     from .supabase_config import verify_connection
492:     # Check connection first
493:     if not verify_connection():
494:         console.print("[red]Unable to connect to Supabase. Please check your configuration.[/red]")
495:         raise typer.Exit(1)
496:     email = typer.prompt("Email")
497:     password = typer.prompt("Password", hide_input=True)
498:     auth_manager = AuthManager()
499:     if auth_manager.login(email, password):
500:         console.print(f"[green]Successfully logged in as {email}[/green]")
501:         # Get user profile
502:         profile = auth_manager.get_user_profile()
503:         if profile:
504:             console.print(f"Profile: {profile.get('display_name', 'Unknown')} ({profile.get('profile_type', 'user')})")
505:     else:
506:         console.print("[red]Login failed. Please check your credentials.[/red]")
507:         raise typer.Exit(1)
508: @auth_app.command("signup")
509: def auth_signup():
510:     """Sign up for a new account."""
511:     from .auth import AuthManager
512:     from .supabase_config import verify_connection
513:     # Check connection first
514:     if not verify_connection():
515:         console.print("[red]Unable to connect to Supabase. Please check your configuration.[/red]")
516:         raise typer.Exit(1)
517:     email = typer.prompt("Email")
518:     password = typer.prompt("Password", hide_input=True)
519:     confirm_password = typer.prompt("Confirm Password", hide_input=True)
520:     if password != confirm_password:
521:         console.print("[red]Passwords do not match.[/red]")
522:         raise typer.Exit(1)
523:     auth_manager = AuthManager()
524:     if auth_manager.signup(email, password):
525:         console.print(f"[green]Successfully signed up as {email}[/green]")
526:         console.print("[yellow]Please check your email for verification link.[/yellow]")
527:     else:
528:         console.print("[red]Sign up failed. Please try again.[/red]")
529:         raise typer.Exit(1)
530: @auth_app.command("logout")
531: def auth_logout():
532:     """Logout from current session."""
533:     from .auth import AuthManager
534:     auth_manager = AuthManager()
535:     if auth_manager.logout():
536:         console.print("[green]Successfully logged out.[/green]")
537:     else:
538:         console.print("[red]Logout failed.[/red]")
539:         raise typer.Exit(1)
540: @auth_app.command("status")
541: def auth_status():
542:     """Show current authentication status."""
543:     from .auth import AuthManager
544:     from .supabase_config import get_database_status
545:     auth_manager = AuthManager()
546:     session = auth_manager.get_current_session()
547:     status_table = Table(title="Authentication Status")
548:     status_table.add_column("Item", style="cyan")
549:     status_table.add_column("Status", style="green")
550:     if session:
551:         status_table.add_row("Logged in", "[green]Yes[/green]")
552:         status_table.add_row("Email", session.get('email', 'Unknown'))
553:         status_table.add_row("User ID", session.get('user_id', 'Unknown'))
554:         # Get profile info
555:         profile = auth_manager.get_user_profile()
556:         if profile:
557:             status_table.add_row("Display Name", profile.get('display_name', 'Not set'))
558:             status_table.add_row("Profile Type", profile.get('profile_type', 'user'))
559:     else:
560:         status_table.add_row("Logged in", "[red]No[/red]")
561:     console.print(status_table)
562:     # Database status
563:     db_status = get_database_status()
564:     db_table = Table(title="Database Status")
565:     db_table.add_column("Component", style="cyan")
566:     db_table.add_column("Status", style="green")
567:     db_table.add_row("Connection", "[green]Success[/green]" if db_status['connection'] == 'success' else "[red]Failed[/red]")
568:     db_table.add_row("URL", db_status['url'] or 'Not configured')
569:     if 'tables' in db_status:
570:         for table, status in db_status['tables'].items():
571:             db_table.add_row(f"Table: {table}", "[green]Exists[/green]" if status == 'exists' else "[red]Missing[/red]")
572:     console.print(db_table)
</file>

<file path="video_ingest_tool/config.py">
  1: """
  2: Configuration for the video ingest tool.
  3: Contains constants, settings, and logging setup.
  4: """
  5: import os
  6: import datetime
  7: import logging
  8: import structlog
  9: from rich.console import Console
 10: from rich.logging import RichHandler
 11: from logging import FileHandler
 12: # Initialize console for rich output
 13: console = Console()
 14: # Focal length category ranges (in mm, for full-frame equivalent)
 15: FOCAL_LENGTH_RANGES = {
 16:     "ULTRA-WIDE": (8, 18),    # Ultra wide-angle: 8-18mm
 17:     "WIDE": (18, 35),         # Wide-angle: 18-35mm
 18:     "MEDIUM": (35, 70),       # Standard/Normal: 35-70mm
 19:     "LONG-LENS": (70, 200),   # Short telephoto: 70-200mm
 20:     "TELEPHOTO": (200, 800)   # Telephoto: 200-800mm
 21: }
 22: def setup_logging():
 23:     """
 24:     Setup logging configurations for both file and console output.
 25:     Returns the logger and timestamp for current run.
 26:     """
 27:     # Get the package directory
 28:     package_dir = os.path.dirname(os.path.abspath(__file__))
 29:     parent_dir = os.path.dirname(package_dir)
 30:     # Configure logging
 31:     log_dir = os.path.join(parent_dir, "logs")
 32:     os.makedirs(log_dir, exist_ok=True)
 33:     # Create a timestamp for current run
 34:     timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
 35:     # Create consolidated output directory structure
 36:     output_dir = os.path.join(parent_dir, "output")
 37:     runs_dir = os.path.join(output_dir, "runs")
 38:     current_run_dir = os.path.join(runs_dir, f"run_{timestamp}")
 39:     # Create run-specific directories
 40:     os.makedirs(current_run_dir, exist_ok=True)
 41:     run_logs_dir = os.path.join(current_run_dir, "logs")
 42:     os.makedirs(run_logs_dir, exist_ok=True)
 43:     # Log file in run directory
 44:     log_file = os.path.join(run_logs_dir, f"ingestor_{timestamp}.log")
 45:     # Create JSON directory in run structure  
 46:     json_dir = os.path.join(current_run_dir, "json")
 47:     os.makedirs(json_dir, exist_ok=True)
 48:     # Configure structlog to integrate with standard logging
 49:     structlog.configure(
 50:         processors=[
 51:             structlog.contextvars.merge_contextvars,
 52:             structlog.stdlib.add_logger_name,
 53:             structlog.processors.add_log_level,
 54:             structlog.processors.TimeStamper(fmt="%Y-%m-%d %H:%M:%S.%f", utc=False),
 55:             structlog.processors.StackInfoRenderer(),
 56:             structlog.dev.set_exc_info,
 57:             structlog.processors.format_exc_info,
 58:             structlog.processors.UnicodeDecoder(),
 59:             structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
 60:         ],
 61:         logger_factory=structlog.stdlib.LoggerFactory(),
 62:         wrapper_class=structlog.make_filtering_bound_logger(min_level=logging.INFO),
 63:         context_class=dict,
 64:         cache_logger_on_first_use=True,
 65:     )
 66:     # Configure standard Python logging handlers
 67:     log_format = "%(message)s"
 68:     # Console Handler (using Rich for pretty output)
 69:     rich_console_handler = RichHandler(console=console, rich_tracebacks=True, markup=True, show_path=False)
 70:     rich_console_handler.setFormatter(logging.Formatter(log_format))
 71:     rich_console_handler.setLevel(logging.INFO)
 72:     # File Handler (plain text)
 73:     file_log_handler = FileHandler(log_file, mode='w', encoding='utf-8')
 74:     file_log_handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)-5.5s] [%(name)s] %(message)s"))
 75:     file_log_handler.setLevel(logging.INFO)
 76:     # Get the root logger and add handlers
 77:     std_root_logger = logging.getLogger()
 78:     std_root_logger.addHandler(rich_console_handler)
 79:     std_root_logger.addHandler(file_log_handler)
 80:     std_root_logger.setLevel(logging.INFO)
 81:     # Create a logger instance using structlog
 82:     logger = structlog.get_logger(__name__)
 83:     logger.info("Logging configured successfully for console and file.")
 84:     return logger, timestamp, json_dir, log_file
 85: # Check if required modules are available
 86: try:
 87:     from polyfile.magic import MagicMatcher
 88:     HAS_POLYFILE = True
 89: except ImportError:
 90:     HAS_POLYFILE = False
 91: try:
 92:     from transformers import pipeline
 93:     import torch
 94:     HAS_TRANSFORMERS = True
 95: except ImportError:
 96:     HAS_TRANSFORMERS = False
 97: class Config:
 98:     """
 99:     A simple configuration class to hold and provide settings.
100:     """
101:     def __init__(self, config_data: dict = None):
102:         self._config = config_data if config_data is not None else {}
103:     def get_setting(self, key: str, default=None):
104:         """
105:         Retrieves a setting value by key.
106:         Uses dot notation for nested keys (e.g., 'processors.video.enabled').
107:         """
108:         keys = key.split('.')
109:         value = self._config
110:         try:
111:             for k in keys:
112:                 value = value[k]
113:             return value
114:         except (KeyError, TypeError):
115:             return default
116:     def set_setting(self, key: str, value):
117:         """
118:         Sets a setting value by key.
119:         Uses dot notation for nested keys.
120:         """
121:         keys = key.split('.')
122:         d = self._config
123:         for k in keys[:-1]:
124:             d = d.setdefault(k, {})
125:         d[keys[-1]] = value
126:     def update_config(self, new_config_data: dict):
127:         """
128:         Merges new configuration data into the existing configuration.
129:         """
130:         def _deep_update(source, overrides):
131:             for key, value in overrides.items():
132:                 if isinstance(value, dict) and key in source and isinstance(source[key], dict):
133:                     _deep_update(source[key], value)
134:                 else:
135:                     source[key] = value
136:             return source
137:         self._config = _deep_update(self._config, new_config_data)
138:     def __repr__(self):
139:         return f"Config(config_data={self._config})"
</file>

<file path="video_ingest_tool/database_storage.py">
  1: """
  2: Database storage pipeline step for Supabase integration.
  3: """
  4: import os
  5: from typing import Dict, Any, Optional
  6: import structlog
  7: from .auth import AuthManager
  8: from .models import VideoIngestOutput
  9: logger = structlog.get_logger(__name__)
 10: def store_video_in_database(
 11:     video_data: VideoIngestOutput,
 12:     logger=None
 13: ) -> Dict[str, Any]:
 14:     """
 15:     Store processed video data in Supabase database.
 16:     Args:
 17:         video_data: Processed video data
 18:         logger: Optional logger
 19:     Returns:
 20:         Dict with storage results
 21:     """
 22:     auth_manager = AuthManager()
 23:     client = auth_manager.get_authenticated_client()
 24:     if not client:
 25:         raise ValueError("Authentication required for database storage")
 26:     try:
 27:         # Get current user ID
 28:         user_response = client.auth.get_user()
 29:         if not user_response.user or not user_response.user.id:
 30:             raise ValueError("Unable to get authenticated user ID")
 31:         user_id = user_response.user.id
 32:         # Prepare clip data
 33:         clip_data = {
 34:             "user_id": user_id,
 35:             "file_path": video_data.file_info.file_path,
 36:             "local_path": os.path.abspath(video_data.file_info.file_path),
 37:             "file_name": video_data.file_info.file_name,
 38:             "file_checksum": video_data.file_info.file_checksum,
 39:             "file_size_bytes": video_data.file_info.file_size_bytes,
 40:             "duration_seconds": video_data.video.duration_seconds,
 41:             "created_at": video_data.file_info.created_at.isoformat() if video_data.file_info.created_at else None,
 42:             "processed_at": video_data.file_info.processed_at.isoformat(),
 43:             # Technical metadata
 44:             "width": video_data.video.resolution.width if video_data.video.resolution else None,
 45:             "height": video_data.video.resolution.height if video_data.video.resolution else None,
 46:             "frame_rate": video_data.video.frame_rate,
 47:             "codec": video_data.video.codec.name if video_data.video.codec else None,
 48:             "camera_make": video_data.camera.make if video_data.camera else None,
 49:             "camera_model": video_data.camera.model if video_data.camera else None,
 50:             "container": video_data.video.container,
 51:             # AI analysis summaries
 52:             "content_category": None,
 53:             "content_summary": video_data.analysis.content_summary if video_data.analysis else None,
 54:             "content_tags": video_data.analysis.content_tags if video_data.analysis else [],
 55:             # Transcript data
 56:             "full_transcript": None,
 57:             "transcript_preview": None,
 58:             # Complex metadata as JSONB
 59:             "technical_metadata": {
 60:                 "codec_details": video_data.video.codec.model_dump() if video_data.video.codec else {},
 61:                 "color_details": video_data.video.color.model_dump() if video_data.video.color else {},
 62:                 "exposure_details": video_data.video.exposure.model_dump() if video_data.video.exposure else {}
 63:             },
 64:             "camera_details": video_data.camera.model_dump() if video_data.camera else {},
 65:             "audio_tracks": [track.model_dump() for track in video_data.audio_tracks] if video_data.audio_tracks else [],
 66:             "subtitle_tracks": [track.model_dump() for track in video_data.subtitle_tracks] if video_data.subtitle_tracks else [],
 67:             "thumbnails": video_data.thumbnails if video_data.thumbnails else []
 68:         }
 69:         # Extract AI analysis data if available
 70:         if video_data.analysis and video_data.analysis.ai_analysis:
 71:             ai_analysis = video_data.analysis.ai_analysis
 72:             # Extract content category and summary
 73:             if ai_analysis.summary:
 74:                 clip_data["content_category"] = ai_analysis.summary.content_category
 75:             # Extract transcript data
 76:             if ai_analysis.audio_analysis and ai_analysis.audio_analysis.transcript:
 77:                 transcript = ai_analysis.audio_analysis.transcript
 78:                 clip_data["full_transcript"] = transcript.full_text
 79:                 # Set transcript preview (first 500 chars)
 80:                 if transcript.full_text:
 81:                     clip_data["transcript_preview"] = transcript.full_text[:500]
 82:         # Check if this is a force reprocess - if so, check for existing clip with same checksum
 83:         existing_clip_id = None
 84:         if clip_data.get('file_checksum'):
 85:             existing_result = client.table('clips').select('id').eq('file_checksum', clip_data['file_checksum']).execute()
 86:             if existing_result.data:
 87:                 existing_clip_id = existing_result.data[0]['id']
 88:                 if logger:
 89:                     logger.info(f"Found existing clip for reprocessing: {existing_clip_id}")
 90:         if existing_clip_id:
 91:             # Update existing clip
 92:             clip_result = client.table('clips').update(clip_data).eq('id', existing_clip_id).execute()
 93:             clip_id = existing_clip_id
 94:             if logger:
 95:                 logger.info(f"Updated existing clip in database: {clip_id}")
 96:         else:
 97:             # Insert new clip
 98:             clip_result = client.table('clips').insert(clip_data).execute()
 99:             clip_id = clip_result.data[0]['id']
100:             if logger:
101:                 logger.info(f"Stored new clip in database: {clip_id}")
102:         # Store transcript if available
103:         if (video_data.analysis and video_data.analysis.ai_analysis and 
104:             video_data.analysis.ai_analysis.audio_analysis and 
105:             video_data.analysis.ai_analysis.audio_analysis.transcript):
106:             transcript = video_data.analysis.ai_analysis.audio_analysis.transcript
107:             speaker_analysis = video_data.analysis.ai_analysis.audio_analysis.speaker_analysis
108:             sound_events = video_data.analysis.ai_analysis.audio_analysis.sound_events
109:             transcript_data = {
110:                 "clip_id": clip_id,
111:                 "user_id": user_id,
112:                 "full_text": transcript.full_text or "",
113:                 "segments": [seg.model_dump() for seg in transcript.segments] if transcript.segments else [],
114:                 "speakers": [speaker.model_dump() for speaker in speaker_analysis.speakers] if speaker_analysis and speaker_analysis.speakers else [],
115:                 "non_speech_events": [event.model_dump() for event in sound_events] if sound_events else []
116:             }
117:             if existing_clip_id:
118:                 # Delete existing transcript and insert new one
119:                 client.table('transcripts').delete().eq('clip_id', clip_id).execute()
120:                 client.table('transcripts').insert(transcript_data).execute()
121:                 if logger:
122:                     logger.info(f"Updated transcript for clip: {clip_id}")
123:             else:
124:                 client.table('transcripts').insert(transcript_data).execute()
125:                 if logger:
126:                     logger.info(f"Stored transcript for clip: {clip_id}")
127:         # Store AI analysis
128:         if video_data.analysis and video_data.analysis.ai_analysis:
129:             ai_analysis = video_data.analysis.ai_analysis
130:             analysis_data = {
131:                 "clip_id": clip_id,
132:                 "user_id": user_id,
133:                 "analysis_type": "comprehensive",
134:                 "analysis_scope": "full_clip",
135:                 "ai_model": "gemini-flash-2.5",
136:                 "content_category": ai_analysis.summary.content_category if ai_analysis.summary else None,
137:                 "usability_rating": None,
138:                 "speaker_count": 0,
139:                 "visual_analysis": ai_analysis.visual_analysis.model_dump() if ai_analysis.visual_analysis else None,
140:                 "audio_analysis": ai_analysis.audio_analysis.model_dump() if ai_analysis.audio_analysis else None,
141:                 "content_analysis": ai_analysis.content_analysis.model_dump() if ai_analysis.content_analysis else None,
142:                 "analysis_summary": ai_analysis.summary.model_dump() if ai_analysis.summary else None,
143:                 "analysis_file_path": ai_analysis.analysis_file_path
144:             }
145:             # Extract usability rating if available
146:             if (ai_analysis.visual_analysis and 
147:                 ai_analysis.visual_analysis.technical_quality and 
148:                 ai_analysis.visual_analysis.technical_quality.usability_rating):
149:                 analysis_data["usability_rating"] = ai_analysis.visual_analysis.technical_quality.usability_rating
150:             # Extract speaker count if available
151:             if (ai_analysis.audio_analysis and 
152:                 ai_analysis.audio_analysis.speaker_analysis and 
153:                 ai_analysis.audio_analysis.speaker_analysis.speaker_count):
154:                 analysis_data["speaker_count"] = ai_analysis.audio_analysis.speaker_analysis.speaker_count
155:             if existing_clip_id:
156:                 # Delete existing analysis and insert new one
157:                 client.table('analysis').delete().eq('clip_id', clip_id).execute()
158:                 client.table('analysis').insert(analysis_data).execute()
159:                 if logger:
160:                     logger.info(f"Updated AI analysis for clip: {clip_id}")
161:             else:
162:                 client.table('analysis').insert(analysis_data).execute()
163:                 if logger:
164:                     logger.info(f"Stored AI analysis for clip: {clip_id}")
165:         return {
166:             'clip_id': clip_id,
167:             'stored_in_database': True,
168:             'database_url': f"https://supabase.com/dashboard/project/{os.getenv('SUPABASE_PROJECT_ID', 'unknown')}"
169:         }
170:     except Exception as e:
171:         if logger:
172:             logger.error(f"Failed to store video in database: {str(e)}")
173:         raise
</file>

<file path="video_ingest_tool/discovery.py">
 1: """
 2: File discovery module for the video ingest tool.
 3: Contains functions for scanning directories and identifying video files.
 4: """
 5: import os
 6: from typing import List
 7: from rich.progress import Progress
 8: from .config import console
 9: from .utils import is_video_file
10: def scan_directory(directory: str, recursive: bool = True, logger=None, has_polyfile: bool = False) -> List[str]:
11:     """
12:     Scan directory for video files.
13:     Args:
14:         directory: Directory to scan
15:         recursive: Whether to scan subdirectories
16:         logger: Logger instance
17:         has_polyfile: Whether polyfile module is available
18:     Returns:
19:         List[str]: List of video file paths
20:     """
21:     if logger:
22:         logger.info("Scanning directory", directory=directory, recursive=recursive)
23:     video_files = []
24:     with Progress(console=console, transient=True) as progress:
25:         task = progress.add_task("[cyan]Scanning directory...", total=None)
26:         for root, dirs, files in os.walk(directory):
27:             progress.update(task, advance=1, description=f"[cyan]Scanning {root}")
28:             for file in files:
29:                 file_path = os.path.join(root, file)
30:                 if is_video_file(file_path, has_polyfile):
31:                     video_files.append(file_path)
32:                     if logger:
33:                         logger.info("Found video file", path=file_path)
34:             if not recursive:
35:                 dirs.clear()
36:     if logger:
37:         logger.info("Directory scan complete", video_count=len(video_files))
38:     return video_files
</file>

<file path="video_ingest_tool/embeddings.py">
  1: """
  2: Vector embeddings generation using BAAI/bge-m3 via DeepInfra.
  3: Following Supabase best practices for hybrid search.
  4: """
  5: import os
  6: import openai
  7: import tiktoken
  8: from typing import List, Dict, Any, Optional, Tuple
  9: import structlog
 10: logger = structlog.get_logger(__name__)
 11: def get_embedding_client():
 12:     """Get OpenAI client configured for DeepInfra API."""
 13:     return openai.OpenAI(
 14:         api_key=os.getenv("DEEPINFRA_API_KEY"),
 15:         base_url="https://api.deepinfra.com/v1/openai"
 16:     )
 17: def count_tokens(text: str) -> int:
 18:     """Count tokens in text using tiktoken."""
 19:     try:
 20:         encoding = tiktoken.get_encoding("cl100k_base")
 21:         return len(encoding.encode(text))
 22:     except Exception as e:
 23:         logger.warning(f"Failed to count tokens: {str(e)}")
 24:         return len(text) // 4  # Rough estimate
 25: def truncate_text(text: str, max_tokens: int = 3500) -> Tuple[str, str]:
 26:     """Intelligently truncate text to fit token limit with sentence boundaries."""
 27:     token_count = count_tokens(text)
 28:     if token_count <= max_tokens:
 29:         return text, "none"
 30:     try:
 31:         encoding = tiktoken.get_encoding("cl100k_base")
 32:         tokens = encoding.encode(text)
 33:         # First, try token-based truncation
 34:         truncated_tokens = tokens[:max_tokens]
 35:         truncated_text = encoding.decode(truncated_tokens)
 36:         # Try to cut at sentence boundary to preserve meaning
 37:         # Look for sentence endings near the cut point
 38:         sentences = text.split('. ')
 39:         if len(sentences) > 1:
 40:             # Rebuild text sentence by sentence until we exceed token limit
 41:             rebuilt_text = ""
 42:             for i, sentence in enumerate(sentences):
 43:                 test_text = rebuilt_text + sentence
 44:                 if i < len(sentences) - 1:  # Add period back except for last sentence
 45:                     test_text += ". "
 46:                 if count_tokens(test_text) > max_tokens:
 47:                     # This sentence would exceed limit, stop at previous sentence
 48:                     if rebuilt_text:  # We have at least one complete sentence
 49:                         return rebuilt_text.rstrip() + "...", "sentence_boundary"
 50:                     else:
 51:                         # Even first sentence is too long, fall back to token truncation
 52:                         break
 53:                 rebuilt_text = test_text
 54:         # Fallback to token-based truncation with ellipsis
 55:         return truncated_text + "...", "token_boundary"
 56:     except Exception as e:
 57:         logger.warning(f"Failed to truncate with tiktoken: {str(e)}")
 58:         # Fallback: character-based truncation with sentence awareness
 59:         char_limit = max_tokens * 4  # Rough estimate
 60:         if len(text) <= char_limit:
 61:             return text, "none"
 62:         truncated = text[:char_limit]
 63:         # Try to cut at last sentence boundary
 64:         last_period = truncated.rfind('. ')
 65:         if last_period > char_limit * 0.7:  # Only if we keep at least 70% of content
 66:             return truncated[:last_period + 1] + "...", "char_sentence_boundary"
 67:         else:
 68:             return truncated + "...", "char_estimate"
 69: def prepare_embedding_content(video_data) -> Tuple[str, str, Dict[str, Any]]:
 70:     """
 71:     Prepare semantic content for embedding generation optimized for hybrid search.
 72:     Technical specs are handled by full-text search.
 73:     Args:
 74:         video_data: VideoIngestOutput model
 75:     Returns:
 76:         Tuple of (summary_content, keyword_content, metadata)
 77:     """
 78:     # SUMMARY EMBEDDING: Semantic narrative content
 79:     summary_parts = []
 80:     # Core content description
 81:     if video_data.analysis and video_data.analysis.ai_analysis and video_data.analysis.ai_analysis.summary:
 82:         summary = video_data.analysis.ai_analysis.summary
 83:         # Content category as context
 84:         if summary.content_category:
 85:             summary_parts.append(f"{summary.content_category} content")
 86:         # Main semantic description (the "what" and "why")
 87:         if summary.overall:
 88:             summary_parts.append(summary.overall)
 89:         # Key activities in natural language
 90:         if summary.key_activities:
 91:             activities_text = ", ".join(summary.key_activities)
 92:             summary_parts.append(f"Activities include {activities_text}")
 93:     # Location and setting context
 94:     if (video_data.analysis and video_data.analysis.ai_analysis and 
 95:         video_data.analysis.ai_analysis.content_analysis and
 96:         video_data.analysis.ai_analysis.content_analysis.entities and
 97:         video_data.analysis.ai_analysis.content_analysis.entities.locations):
 98:         locations = []
 99:         for loc in video_data.analysis.ai_analysis.content_analysis.entities.locations:
100:             if loc.name and loc.type:
101:                 locations.append(f"{loc.name} ({loc.type.lower()})")
102:             elif loc.name:
103:                 locations.append(loc.name)
104:         if locations:
105:             summary_parts.append(f"Filmed in {', '.join(locations)}")
106:     # Visual style and cinematography
107:     shot_style_parts = []
108:     if (video_data.analysis and video_data.analysis.ai_analysis and 
109:         video_data.analysis.ai_analysis.visual_analysis and
110:         video_data.analysis.ai_analysis.visual_analysis.shot_types):
111:         # Extract shot types and convert to natural language
112:         for shot in video_data.analysis.ai_analysis.visual_analysis.shot_types:
113:             if shot.shot_type:
114:                 # Convert technical terms to searchable concepts
115:                 shot_type = shot.shot_type.lower()
116:                 if "static" in shot_type or "locked" in shot_type:
117:                     shot_style_parts.append("stationary camera work")
118:                 elif "wide" in shot_type:
119:                     shot_style_parts.append("wide angle cinematography")
120:                 elif "close" in shot_type:
121:                     shot_style_parts.append("close-up footage")
122:                 else:
123:                     shot_style_parts.append(f"{shot_type} cinematography")
124:     if shot_style_parts:
125:         summary_parts.append(f"Features {', '.join(set(shot_style_parts))}")
126:     # Content purpose and educational value
127:     if (video_data.analysis and video_data.analysis.ai_analysis and 
128:         video_data.analysis.ai_analysis.content_analysis and
129:         video_data.analysis.ai_analysis.content_analysis.activity_summary):
130:         purposes = []
131:         for activity in video_data.analysis.ai_analysis.content_analysis.activity_summary:
132:             activity_text = activity.activity.lower()
133:             if "technical" in activity_text or "commentary" in activity_text:
134:                 purposes.append("educational technical demonstration")
135:             elif "landscape" in activity_text or "scenic" in activity_text:
136:                 purposes.append("scenic documentation")
137:             elif "demonstration" in activity_text:
138:                 purposes.append("instructional content")
139:         if purposes:
140:             summary_parts.append(f"Serves as {', '.join(set(purposes))}")
141:     summary_content = ". ".join(summary_parts)
142:     # KEYWORD EMBEDDING: Concept tags and semantic keywords
143:     keyword_concepts = []
144:     # Core semantic concepts from transcript
145:     if (video_data.analysis and video_data.analysis.ai_analysis and 
146:         video_data.analysis.ai_analysis.audio_analysis and 
147:         video_data.analysis.ai_analysis.audio_analysis.transcript and
148:         video_data.analysis.ai_analysis.audio_analysis.transcript.full_text):
149:         # Include transcript for semantic concept extraction
150:         transcript = video_data.analysis.ai_analysis.audio_analysis.transcript.full_text
151:         keyword_concepts.append(transcript)
152:     # Visual and environmental concepts
153:     visual_concepts = []
154:     # Location concepts
155:     if (video_data.analysis and video_data.analysis.ai_analysis and 
156:         video_data.analysis.ai_analysis.content_analysis and
157:         video_data.analysis.ai_analysis.content_analysis.entities):
158:         entities = video_data.analysis.ai_analysis.content_analysis.entities
159:         # Location-based concepts
160:         if entities.locations:
161:             for loc in entities.locations:
162:                 if loc.name:
163:                     visual_concepts.append(loc.name.lower())
164:                 if loc.type:
165:                     visual_concepts.append(loc.type.lower())
166:         # Object-based concepts
167:         if entities.objects_of_interest:
168:             for obj in entities.objects_of_interest:
169:                 if obj.object:
170:                     visual_concepts.append(obj.object.lower())
171:     # Shot style concepts
172:     if (video_data.analysis and video_data.analysis.ai_analysis and 
173:         video_data.analysis.ai_analysis.visual_analysis and
174:         video_data.analysis.ai_analysis.visual_analysis.shot_types):
175:         for shot in video_data.analysis.ai_analysis.visual_analysis.shot_types:
176:             if shot.shot_type:
177:                 # Extract key concepts from shot types
178:                 shot_words = shot.shot_type.lower().replace("/", " ").replace("-", " ").split()
179:                 visual_concepts.extend([word for word in shot_words if len(word) > 2])
180:     # Activity and purpose concepts
181:     activity_concepts = []
182:     if (video_data.analysis and video_data.analysis.ai_analysis and 
183:         video_data.analysis.ai_analysis.content_analysis and
184:         video_data.analysis.ai_analysis.content_analysis.activity_summary):
185:         for activity in video_data.analysis.ai_analysis.content_analysis.activity_summary:
186:             if activity.activity:
187:                 # Extract key concepts from activities
188:                 activity_words = activity.activity.lower().split()
189:                 activity_concepts.extend([word for word in activity_words if len(word) > 3])
190:     # Content category concepts
191:     category_concepts = []
192:     if video_data.analysis and video_data.analysis.ai_analysis and video_data.analysis.ai_analysis.summary:
193:         if video_data.analysis.ai_analysis.summary.content_category:
194:             category_concepts.append(video_data.analysis.ai_analysis.summary.content_category.lower())
195:     # Combine all concept lists
196:     all_concepts = []
197:     if visual_concepts:
198:         all_concepts.append(" ".join(set(visual_concepts)))
199:     if activity_concepts:
200:         all_concepts.append(" ".join(set(activity_concepts)))
201:     if category_concepts:
202:         all_concepts.append(" ".join(set(category_concepts)))
203:     keyword_concepts.extend(all_concepts)
204:     keyword_content = " ".join(keyword_concepts)
205:     # Truncate both contents
206:     summary_content, summary_truncation = truncate_text(summary_content, 3500)
207:     keyword_content, keyword_truncation = truncate_text(keyword_content, 3500)
208:     metadata = {
209:         "summary_tokens": count_tokens(summary_content),
210:         "keyword_tokens": count_tokens(keyword_content),
211:         "summary_truncation": summary_truncation,
212:         "keyword_truncation": keyword_truncation,
213:         "original_transcript_length": len(video_data.analysis.ai_analysis.audio_analysis.transcript.full_text) if (
214:             video_data.analysis and video_data.analysis.ai_analysis and 
215:             video_data.analysis.ai_analysis.audio_analysis and 
216:             video_data.analysis.ai_analysis.audio_analysis.transcript and
217:             video_data.analysis.ai_analysis.audio_analysis.transcript.full_text
218:         ) else 0
219:     }
220:     return summary_content, keyword_content, metadata
221: def generate_embeddings(
222:     summary_content: str,
223:     keyword_content: str,
224:     logger=None
225: ) -> Tuple[List[float], List[float]]:
226:     """Generate embeddings using BAAI/bge-m3 via DeepInfra."""
227:     try:
228:         client = get_embedding_client()
229:         # Generate summary embedding
230:         summary_response = client.embeddings.create(
231:             input=summary_content,
232:             model="BAAI/bge-m3",
233:             encoding_format="float"
234:         )
235:         summary_embedding = summary_response.data[0].embedding
236:         # Generate keyword embedding
237:         keyword_response = client.embeddings.create(
238:             input=keyword_content,
239:             model="BAAI/bge-m3",
240:             encoding_format="float"
241:         )
242:         keyword_embedding = keyword_response.data[0].embedding
243:         if logger:
244:             logger.info(f"Generated embeddings - Summary: {len(summary_embedding)}D, Keywords: {len(keyword_embedding)}D")
245:         return summary_embedding, keyword_embedding
246:     except Exception as e:
247:         if logger:
248:             logger.error(f"Failed to generate embeddings: {str(e)}")
249:         raise
250: def store_embeddings(
251:     clip_id: str,
252:     summary_embedding: List[float],
253:     keyword_embedding: List[float],
254:     summary_content: str,
255:     keyword_content: str,
256:     original_content: str,
257:     metadata: Dict[str, Any],
258:     logger=None
259: ) -> bool:
260:     """Store embeddings in Supabase database following pgvector patterns."""
261:     from .auth import AuthManager
262:     auth_manager = AuthManager()
263:     client = auth_manager.get_authenticated_client()
264:     if not client:
265:         raise ValueError("Authentication required for storing embeddings")
266:     try:
267:         # Get user ID
268:         user_response = client.auth.get_user()
269:         user_id = user_response.user.id
270:         vector_data = {
271:             "clip_id": clip_id,
272:             "user_id": user_id,
273:             "embedding_type": "full_clip",
274:             "embedding_source": "combined",
275:             "summary_vector": summary_embedding,
276:             "keyword_vector": keyword_embedding,
277:             "embedded_content": f"Summary: {summary_content}\nKeywords: {keyword_content}",
278:             "original_content": original_content,
279:             "token_count": metadata["summary_tokens"] + metadata["keyword_tokens"],
280:             "original_token_count": count_tokens(original_content),
281:             "truncation_method": metadata["summary_truncation"]
282:         }
283:         result = client.table('vectors').insert(vector_data).execute()
284:         if logger:
285:             logger.info(f"Stored embeddings for clip: {clip_id}")
286:         return True
287:     except Exception as e:
288:         if logger:
289:             logger.error(f"Failed to store embeddings: {str(e)}")
290:         raise
</file>

<file path="video_ingest_tool/extractors_extended.py">
  1: """
  2: Additional extractors for the video ingest tool.
  3: Contains more specialized metadata extraction functions.
  4: """
  5: import exiftool
  6: import av
  7: import pymediainfo
  8: from typing import Any, Dict, List, Optional, Tuple, Union
  9: from .utils import categorize_focal_length, parse_datetime_string, map_exposure_mode, map_white_balance
 10: from .config import FOCAL_LENGTH_RANGES
 11: def extract_exiftool_info(file_path: str, logger=None) -> Dict[str, Any]:
 12:     """
 13:     Extract metadata using ExifTool.
 14:     Args:
 15:         file_path: Path to the video file
 16:         logger: Logger instance
 17:     Returns:
 18:         Dict: Technical metadata
 19:     """
 20:     if logger:
 21:         logger.info("Extracting ExifTool metadata", path=file_path)
 22:     try:
 23:         with exiftool.ExifToolHelper() as et:
 24:             metadata = et.get_metadata(file_path)[0]
 25:             # Get the raw focal length
 26:             focal_length_raw = metadata.get('EXIF:FocalLength')
 27:             # Map numeric focal length to categories using the utility function
 28:             focal_length_category = None
 29:             if focal_length_raw is not None:
 30:                 focal_length_category = categorize_focal_length(focal_length_raw, FOCAL_LENGTH_RANGES)
 31:             exif_data = {
 32:                 'camera_make': metadata.get('EXIF:Make'),
 33:                 'camera_model': metadata.get('EXIF:Model'),
 34:                 'focal_length_mm': focal_length_raw if isinstance(focal_length_raw, (int, float)) else None,
 35:                 'focal_length_category': focal_length_category,
 36:                 # Keep focal_length for backward compatibility
 37:                 'focal_length': focal_length_category,
 38:                 'created_at': parse_datetime_string(metadata.get('EXIF:CreateDate') or metadata.get('QuickTime:CreateDate') or metadata.get('QuickTime:CreationDate')),
 39:                 'gps_latitude': metadata.get('EXIF:GPSLatitude'),
 40:                 'gps_longitude': metadata.get('EXIF:GPSLongitude'),
 41:             }
 42:             exif_data = {k: v for k, v in exif_data.items() if v is not None}
 43:             if logger:
 44:                 logger.info("ExifTool extraction successful", path=file_path)
 45:             return exif_data
 46:     except Exception as e:
 47:         if logger:
 48:             logger.error("ExifTool extraction failed", path=file_path, error=str(e))
 49:         return {}
 50: def extract_extended_exif_metadata(file_path: str, logger=None) -> Dict[str, Any]:
 51:     """
 52:     Extract extended EXIF metadata from video files.
 53:     Args:
 54:         file_path: Path to the video file
 55:         logger: Logger instance
 56:     Returns:
 57:         Dict: Extended EXIF metadata including GPS and advanced camera settings
 58:     """
 59:     if logger:
 60:         logger.info("Extracting extended EXIF metadata", path=file_path)
 61:     try:
 62:         with exiftool.ExifToolHelper() as et:
 63:             metadata = et.get_metadata(file_path)[0]
 64:             # Initialize the result dict
 65:             extended_metadata = {}
 66:             # Extract GPS coordinates
 67:             if 'EXIF:GPSLatitude' in metadata and 'EXIF:GPSLongitude' in metadata:
 68:                 try:
 69:                     extended_metadata['gps_latitude'] = float(metadata['EXIF:GPSLatitude'])
 70:                     extended_metadata['gps_longitude'] = float(metadata['EXIF:GPSLongitude'])
 71:                     # Add altitude if available
 72:                     if 'EXIF:GPSAltitude' in metadata:
 73:                         extended_metadata['gps_altitude'] = float(metadata['EXIF:GPSAltitude'])
 74:                     # Try to get location name if available
 75:                     if 'XMP:Location' in metadata:
 76:                         extended_metadata['location_name'] = metadata['XMP:Location']
 77:                     elif 'IPTC:City' in metadata:
 78:                         city = metadata['IPTC:City']
 79:                         country = metadata.get('IPTC:Country', '')
 80:                         if country:
 81:                             extended_metadata['location_name'] = f"{city}, {country}"
 82:                         else:
 83:                             extended_metadata['location_name'] = city
 84:                 except (ValueError, TypeError) as e:
 85:                     if logger:
 86:                         logger.warning(f"Error parsing GPS coordinates: {e}", path=file_path)
 87:             # Advanced camera metadata
 88:             # Camera serial number
 89:             if 'EXIF:SerialNumber' in metadata:
 90:                 extended_metadata['camera_serial_number'] = str(metadata['EXIF:SerialNumber'])
 91:             # Lens model
 92:             if 'EXIF:LensModel' in metadata:
 93:                 extended_metadata['lens_model'] = metadata['EXIF:LensModel']
 94:             # ISO
 95:             if 'EXIF:ISO' in metadata:
 96:                 try:
 97:                     extended_metadata['iso'] = int(metadata['EXIF:ISO'])
 98:                 except (ValueError, TypeError):
 99:                     pass
100:             # Shutter speed
101:             if 'EXIF:ShutterSpeedValue' in metadata:
102:                 extended_metadata['shutter_speed'] = str(metadata['EXIF:ShutterSpeedValue'])
103:             # Aperture (f-stop)
104:             if 'EXIF:FNumber' in metadata:
105:                 try:
106:                     extended_metadata['f_stop'] = float(metadata['EXIF:FNumber'])
107:                 except (ValueError, TypeError):
108:                     pass
109:             # Exposure mode
110:             if 'EXIF:ExposureMode' in metadata:
111:                 extended_metadata['exposure_mode'] = map_exposure_mode(metadata.get('EXIF:ExposureMode'))
112:             # White balance
113:             if 'EXIF:WhiteBalance' in metadata:
114:                 extended_metadata['white_balance'] = map_white_balance(metadata.get('EXIF:WhiteBalance'))
115:             if logger:
116:                 logger.info("Extended EXIF metadata extraction successful", path=file_path)
117:             return extended_metadata
118:     except Exception as e:
119:         if logger:
120:             logger.error("Extended EXIF metadata extraction failed", path=file_path, error=str(e))
121:         return {}
122: def extract_audio_tracks(file_path: str, logger=None) -> List[Dict[str, Any]]:
123:     """
124:     Extract audio track information from video files.
125:     Args:
126:         file_path: Path to the video file
127:         logger: Logger instance
128:     Returns:
129:         List[Dict]: List of audio track metadata
130:     """
131:     if logger:
132:         logger.info("Extracting audio tracks", path=file_path)
133:     audio_tracks = []
134:     try:
135:         media_info = pymediainfo.MediaInfo.parse(file_path)
136:         for track in media_info.tracks:
137:             if track.track_type == 'Audio':
138:                 audio_track = {
139:                     'track_id': str(track.track_id) if hasattr(track, 'track_id') and track.track_id is not None else None,
140:                     'codec': track.format if hasattr(track, 'format') else None,
141:                     'codec_id': track.codec_id if hasattr(track, 'codec_id') else None,
142:                     'duration_seconds': float(track.duration) / 1000 if hasattr(track, 'duration') and track.duration else None,
143:                     'bit_rate_kbps': int(float(str(track.bit_rate).replace('kb/s', '').strip())) if hasattr(track, 'bit_rate') and track.bit_rate else None,
144:                     'channels': int(track.channel_s) if hasattr(track, 'channel_s') and track.channel_s else None,
145:                     'channel_layout': track.channel_layout if hasattr(track, 'channel_layout') else None,
146:                     'sample_rate': int(float(str(track.sampling_rate).replace('Hz', '').strip())) if hasattr(track, 'sampling_rate') and track.sampling_rate else None,
147:                     'bit_depth': int(track.bit_depth) if hasattr(track, 'bit_depth') and track.bit_depth else None,
148:                     'language': track.language if hasattr(track, 'language') else None
149:                 }
150:                 # Filter out None values
151:                 audio_track = {k: v for k, v in audio_track.items() if v is not None}
152:                 audio_tracks.append(audio_track)
153:         if logger:
154:             logger.info("Audio track extraction successful", path=file_path, track_count=len(audio_tracks))
155:         return audio_tracks
156:     except Exception as e:
157:         if logger:
158:             logger.error("Audio track extraction failed", path=file_path, error=str(e))
159:         return []
</file>

<file path="video_ingest_tool/extractors_hdr.py">
  1: """
  2: Special extractors for the video ingest tool.
  3: Contains functions for extracting specialized metadata like HDR information and subtitles.
  4: """
  5: import pymediainfo
  6: import av
  7: from typing import Any, Dict, List, Optional
  8: def extract_subtitle_tracks(file_path: str, logger=None) -> List[Dict[str, Any]]:
  9:     """
 10:     Extract subtitle track information from video files.
 11:     Args:
 12:         file_path: Path to the video file
 13:         logger: Logger instance
 14:     Returns:
 15:         List[Dict]: List of subtitle track metadata
 16:     """
 17:     if logger:
 18:         logger.info("Extracting subtitle tracks", path=file_path)
 19:     subtitle_tracks = []
 20:     try:
 21:         media_info = pymediainfo.MediaInfo.parse(file_path)
 22:         for track in media_info.tracks:
 23:             if track.track_type == 'Text':
 24:                 subtitle_track = {
 25:                     'track_id': str(track.track_id) if hasattr(track, 'track_id') and track.track_id is not None else None,
 26:                     'format': track.format if hasattr(track, 'format') else None,
 27:                     'codec_id': track.codec_id if hasattr(track, 'codec_id') else None,
 28:                     'language': track.language if hasattr(track, 'language') else None,
 29:                     'embedded': True if hasattr(track, 'muxing_mode') and track.muxing_mode == 'muxed' else None
 30:                 }
 31:                 # Filter out None values
 32:                 subtitle_track = {k: v for k, v in subtitle_track.items() if v is not None}
 33:                 subtitle_tracks.append(subtitle_track)
 34:         if logger:
 35:             logger.info("Subtitle track extraction successful", path=file_path, track_count=len(subtitle_tracks))
 36:         return subtitle_tracks
 37:     except Exception as e:
 38:         if logger:
 39:             logger.error("Subtitle track extraction failed", path=file_path, error=str(e))
 40:         return []
 41: def extract_codec_parameters(file_path: str, logger=None) -> Dict[str, Any]:
 42:     """
 43:     Extract detailed codec parameters from video files.
 44:     Args:
 45:         file_path: Path to the video file
 46:         logger: Logger instance
 47:     Returns:
 48:         Dict: Detailed codec parameters
 49:     """
 50:     if logger:
 51:         logger.info("Extracting codec parameters", path=file_path)
 52:     try:
 53:         # Try MediaInfo first for more detailed codec parameters
 54:         media_info = pymediainfo.MediaInfo.parse(file_path)
 55:         video_track = next((track for track in media_info.tracks if track.track_type == 'Video'), None)
 56:         codec_params = {}
 57:         if video_track:
 58:             # Extract profile info
 59:             if hasattr(video_track, 'format_profile') and video_track.format_profile:
 60:                 parts = str(video_track.format_profile).split('@')
 61:                 if len(parts) > 0:
 62:                     codec_params['profile'] = parts[0].strip()
 63:                     if len(parts) > 1 and 'L' in parts[1]:
 64:                         level_part = parts[1].strip()
 65:                         codec_params['level'] = level_part.replace('L', '')
 66:             # Extract pixel format
 67:             if hasattr(video_track, 'pixel_format'):
 68:                 codec_params['pixel_format'] = video_track.pixel_format
 69:             # Extract chroma subsampling
 70:             if hasattr(video_track, 'chroma_subsampling'):
 71:                 codec_params['chroma_subsampling'] = video_track.chroma_subsampling
 72:             # Extract bitrate mode
 73:             if hasattr(video_track, 'bit_rate_mode'):
 74:                 codec_params['bitrate_mode'] = video_track.bit_rate_mode
 75:             # Extract scan type and field order
 76:             if hasattr(video_track, 'scan_type'):
 77:                 codec_params['scan_type'] = video_track.scan_type
 78:             if hasattr(video_track, 'scan_order'):
 79:                 codec_params['field_order'] = video_track.scan_order
 80:         # Try PyAV for additional codec parameters if MediaInfo doesn't provide enough
 81:         if not codec_params or len(codec_params) < 3:
 82:             try:
 83:                 with av.open(file_path) as container:
 84:                     for stream in container.streams.video:
 85:                         if not 'profile' in codec_params and hasattr(stream.codec_context, 'profile'):
 86:                             codec_params['profile'] = stream.codec_context.profile
 87:                         if not 'pixel_format' in codec_params and hasattr(stream.codec_context, 'pix_fmt'):
 88:                             codec_params['pixel_format'] = stream.codec_context.pix_fmt
 89:                         # Get GOP size if available
 90:                         if hasattr(stream.codec_context, 'gop_size'):
 91:                             codec_params['gop_size'] = stream.codec_context.gop_size
 92:                         # Get reference frames if available
 93:                         if hasattr(stream.codec_context, 'refs'):
 94:                             codec_params['ref_frames'] = stream.codec_context.refs
 95:                         # Check for CABAC (H.264 specific)
 96:                         if hasattr(stream.codec_context, 'flags') and \
 97:                            hasattr(stream.codec_context.flags, 'CABAC'):
 98:                             codec_params['cabac'] = bool(stream.codec_context.flags.CABAC)
 99:                         break  # Only process the first video stream
100:             except Exception as av_error:
101:                 if logger:
102:                     logger.warning("PyAV codec parameter extraction failed", path=file_path, error=str(av_error))
103:         if logger:
104:             logger.info("Codec parameter extraction successful", path=file_path, params_count=len(codec_params))
105:         return codec_params
106:     except Exception as e:
107:         if logger:
108:             logger.error("Codec parameter extraction failed", path=file_path, error=str(e))
109:         return {}
110: def extract_hdr_metadata(file_path: str, logger=None) -> Dict[str, Any]:
111:     """
112:     Extract HDR-related metadata from video files.
113:     Args:
114:         file_path: Path to the video file
115:         logger: Logger instance
116:     Returns:
117:         Dict: HDR metadata including format, mastering display info, and light levels
118:     """
119:     if logger:
120:         logger.info("Extracting HDR metadata", path=file_path)
121:     try:
122:         media_info = pymediainfo.MediaInfo.parse(file_path)
123:         video_track = next((track for track in media_info.tracks if track.track_type == 'Video'), None)
124:         hdr_metadata = {}
125:         if video_track:
126:             # Check for HDR format based on transfer characteristics
127:             if hasattr(video_track, 'transfer_characteristics') and video_track.transfer_characteristics:
128:                 transfer = str(video_track.transfer_characteristics).lower()
129:                 hdr_metadata['transfer_characteristics'] = video_track.transfer_characteristics
130:                 if 'pq' in transfer or 'smpte st 2084' in transfer or 'smpte2084' in transfer:
131:                     hdr_metadata['hdr_format'] = 'HDR10'
132:                 elif 'hlg' in transfer or 'hybrid log' in transfer or 'arib std b67' in transfer:
133:                     hdr_metadata['hdr_format'] = 'HLG'
134:             # Check for HDR10+ and Dolby Vision
135:             commercial_id = ''
136:             if hasattr(video_track, 'hdr_format_commercial') and video_track.hdr_format_commercial:
137:                 commercial_id = str(video_track.hdr_format_commercial).lower()
138:                 if 'dolby vision' in commercial_id:
139:                     hdr_metadata['hdr_format'] = 'Dolby Vision'
140:                 elif 'hdr10+' in commercial_id:
141:                     hdr_metadata['hdr_format'] = 'HDR10+'
142:             # Store color info
143:             if hasattr(video_track, 'color_primaries'):
144:                 hdr_metadata['color_primaries'] = video_track.color_primaries
145:             if hasattr(video_track, 'matrix_coefficients'):
146:                 hdr_metadata['matrix_coefficients'] = video_track.matrix_coefficients
147:             if hasattr(video_track, 'color_range'):
148:                 hdr_metadata['color_range'] = video_track.color_range
149:             # Get master display information (typically for HDR10)
150:             if hasattr(video_track, 'mastering_display_color_primaries'):
151:                 hdr_metadata['master_display'] = video_track.mastering_display_color_primaries
152:             # Get content light level
153:             if hasattr(video_track, 'maximum_content_light_level'):
154:                 try:
155:                     hdr_metadata['max_cll'] = int(video_track.maximum_content_light_level)
156:                 except (ValueError, TypeError):
157:                     pass
158:             if hasattr(video_track, 'maximum_frame_light_level'):
159:                 try:
160:                     hdr_metadata['max_fall'] = int(video_track.maximum_frame_light_level)
161:                 except (ValueError, TypeError):
162:                     pass
163:         if hdr_metadata and logger:
164:             logger.info("HDR metadata extraction successful", path=file_path, 
165:                      format=hdr_metadata.get('hdr_format', 'unknown'))
166:         elif logger:
167:             logger.info("No HDR metadata found", path=file_path)
168:         return hdr_metadata
169:     except Exception as e:
170:         if logger:
171:             logger.error("HDR metadata extraction failed", path=file_path, error=str(e))
172:         return {}
</file>

<file path="video_ingest_tool/extractors.py">
  1: """
  2: Metadata extractors for the video ingest tool.
  3: Contains functions for extracting metadata from video files using various tools.
  4: """
  5: import os
  6: import av
  7: import pymediainfo
  8: import exiftool
  9: from typing import Any, Dict, List, Optional, Tuple, Union
 10: from .config import HAS_TRANSFORMERS, FOCAL_LENGTH_RANGES
 11: from .utils import parse_datetime_string, map_exposure_mode, map_white_balance, categorize_focal_length
 12: def extract_mediainfo(file_path: str, logger=None) -> Dict[str, Any]:
 13:     """
 14:     Extract technical metadata using pymediainfo.
 15:     Args:
 16:         file_path: Path to the video file
 17:         logger: Logger instance
 18:     Returns:
 19:         Dict: Technical metadata
 20:     """
 21:     if logger:
 22:         logger.info("Extracting MediaInfo metadata", path=file_path)
 23:     try:
 24:         media_info = pymediainfo.MediaInfo.parse(file_path)
 25:         general_track = next((track for track in media_info.tracks if track.track_type == 'General'), None)
 26:         video_track = next((track for track in media_info.tracks if track.track_type == 'Video'), None)
 27:         metadata = {}
 28:         if general_track:
 29:             # Extract bit rate information from general track if available
 30:             if hasattr(general_track, 'overall_bit_rate') and general_track.overall_bit_rate:
 31:                 try:
 32:                     # Convert to int and handle different formats (sometimes includes 'kb/s')
 33:                     bit_rate_str = str(general_track.overall_bit_rate).lower().replace('kb/s', '').strip()
 34:                     metadata['bit_rate_kbps'] = int(float(bit_rate_str))
 35:                     if logger:
 36:                         logger.info(f"MediaInfo bit rate (general): {metadata['bit_rate_kbps']} kbps", path=file_path)
 37:                 except (ValueError, TypeError):
 38:                     if logger:
 39:                         logger.warning("Could not parse general track bit rate", path=file_path)
 40:             metadata.update({
 41:                 'container': general_track.format,
 42:                 'duration_seconds': float(general_track.duration) / 1000 if general_track.duration else None,
 43:                 'file_size_bytes': general_track.file_size,
 44:                 'created_at': parse_datetime_string(general_track.encoded_date)
 45:             })
 46:         if video_track:
 47:             # Extract bit rate information from video track if available
 48:             if hasattr(video_track, 'bit_rate') and video_track.bit_rate:
 49:                 try:
 50:                     # Convert to int and handle different formats
 51:                     bit_rate_str = str(video_track.bit_rate).lower().replace('kb/s', '').strip()
 52:                     video_bit_rate = int(float(bit_rate_str))
 53:                     # Only update if not already set or if video track bit rate is more specific
 54:                     if 'bit_rate_kbps' not in metadata or video_bit_rate > 0:
 55:                         metadata['bit_rate_kbps'] = video_bit_rate
 56:                         if logger:
 57:                             logger.info(f"MediaInfo bit rate (video): {metadata['bit_rate_kbps']} kbps", path=file_path)
 58:                 except (ValueError, TypeError):
 59:                     if logger:
 60:                         logger.warning("Could not parse video track bit rate", path=file_path)
 61:             metadata.update({
 62:                 'codec': video_track.codec_id or video_track.format,
 63:                 'width': video_track.width,
 64:                 'height': video_track.height,
 65:                 'frame_rate': float(video_track.frame_rate) if video_track.frame_rate else None,
 66:                 'bit_depth': video_track.bit_depth,
 67:                 'color_space': video_track.color_space
 68:             })
 69:         if logger:
 70:             logger.info("MediaInfo extraction successful", path=file_path)
 71:         return metadata
 72:     except Exception as e:
 73:         if logger:
 74:             logger.error("MediaInfo extraction failed", path=file_path, error=str(e))
 75:         return {}
 76: def extract_ffprobe_info(file_path: str, logger=None) -> Dict[str, Any]:
 77:     """
 78:     Extract technical metadata using PyAV (which uses FFmpeg libraries).
 79:     Args:
 80:         file_path: Path to the video file
 81:         logger: Logger instance
 82:     Returns:
 83:         Dict: Technical metadata
 84:     """
 85:     if logger:
 86:         logger.info("Extracting PyAV metadata", path=file_path)
 87:     try:
 88:         with av.open(file_path) as container:
 89:             duration_seconds = None
 90:             if container.duration is not None:
 91:                 duration_seconds = float(container.duration) / 1000000.0
 92:             # Get file size
 93:             file_size = os.path.getsize(file_path)
 94:             metadata = {
 95:                 'duration_seconds': duration_seconds,
 96:                 'file_size_bytes': file_size
 97:             }
 98:             # Calculate bit rate if duration is available
 99:             if duration_seconds and duration_seconds > 0:
100:                 # Calculate bit rate in bits per second
101:                 bit_rate = (file_size * 8) / duration_seconds
102:                 # Convert to kbps
103:                 metadata['bit_rate_kbps'] = int(bit_rate / 1000)
104:                 if logger:
105:                     logger.info(f"Calculated bit rate: {metadata['bit_rate_kbps']} kbps", path=file_path)
106:             video_streams = [s for s in container.streams.video if s.type == 'video']
107:             if video_streams:
108:                 video_stream = video_streams[0]
109:                 # Try to get bit rate from stream if available
110:                 if hasattr(video_stream, 'bit_rate') and video_stream.bit_rate:
111:                     metadata['bit_rate_kbps'] = int(video_stream.bit_rate / 1000)
112:                     if logger:
113:                         logger.info(f"Stream bit rate: {metadata['bit_rate_kbps']} kbps", path=file_path)
114:                 codec_ctx = getattr(video_stream, 'codec_context', None)
115:                 codec_name_val = 'unknown'
116:                 if codec_ctx:
117:                     codec_name_val = getattr(codec_ctx, 'name', None)
118:                     if not codec_name_val:
119:                         codec_name_val = getattr(codec_ctx, 'long_name', 'unknown')
120:                     # Try to get bit rate from codec context if available
121:                     if hasattr(codec_ctx, 'bit_rate') and codec_ctx.bit_rate:
122:                         metadata['bit_rate_kbps'] = int(codec_ctx.bit_rate / 1000)
123:                         if logger:
124:                             logger.info(f"Codec bit rate: {metadata['bit_rate_kbps']} kbps", path=file_path)
125:                 frame_rate = None
126:                 if video_stream.average_rate:
127:                     frame_rate = float(video_stream.average_rate)
128:                 bit_depth = None
129:                 if hasattr(video_stream, 'bits_per_coded_sample'):
130:                     bit_depth = video_stream.bits_per_coded_sample
131:                 metadata.update({
132:                     'format_name': container.format.name,
133:                     'format_long_name': container.format.long_name,
134:                     'codec': codec_name_val,
135:                     'width': video_stream.width,
136:                     'height': video_stream.height,
137:                     'frame_rate': frame_rate,
138:                     'bit_depth': bit_depth
139:                 })
140:             if logger:
141:                 logger.info("PyAV extraction successful", path=file_path)
142:             return metadata
143:     except Exception as e:
144:         if logger:
145:             logger.error("PyAV extraction failed", path=file_path, error=str(e))
146:         return {}
</file>

<file path="video_ingest_tool/hybrid_search.sql">
  1: -- =====================================================
  2: -- HYBRID SEARCH FUNCTIONS FOR VIDEO CATALOG
  3: -- =====================================================
  4: -- Function for basic semantic search using vector similarity
  5: CREATE OR REPLACE FUNCTION semantic_search_clips(
  6:   query_summary_embedding vector(1024),
  7:   query_keyword_embedding vector(1024),
  8:   user_id_filter UUID,
  9:   match_count INT DEFAULT 10,
 10:   summary_weight FLOAT DEFAULT 1.0,
 11:   keyword_weight FLOAT DEFAULT 0.8,
 12:   similarity_threshold FLOAT DEFAULT 0.0
 13: )
 14: RETURNS TABLE (
 15:   id UUID,
 16:   file_name TEXT,
 17:   local_path TEXT,
 18:   content_summary TEXT,
 19:   content_tags TEXT[],
 20:   duration_seconds NUMERIC,
 21:   camera_make TEXT,
 22:   camera_model TEXT,
 23:   content_category TEXT,
 24:   processed_at TIMESTAMPTZ,
 25:   summary_similarity FLOAT,
 26:   keyword_similarity FLOAT,
 27:   combined_similarity FLOAT
 28: )
 29: LANGUAGE SQL
 30: AS $$
 31: WITH summary_search AS (
 32:   SELECT
 33:     c.id, c.file_name, c.local_path, c.content_summary, 
 34:     c.content_tags, c.duration_seconds, c.camera_make, c.camera_model,
 35:     c.content_category, c.processed_at,
 36:     (v.summary_vector <#> query_summary_embedding) * -1 as summary_similarity,
 37:     ROW_NUMBER() OVER (ORDER BY v.summary_vector <#> query_summary_embedding) as rank_ix
 38:   FROM clips c
 39:   JOIN vectors v ON c.id = v.clip_id
 40:   WHERE c.user_id = user_id_filter
 41:     AND v.embedding_type = 'full_clip'
 42:     AND v.summary_vector IS NOT NULL
 43:   ORDER BY v.summary_vector <#> query_summary_embedding
 44:   LIMIT LEAST(match_count * 2, 50)
 45: ),
 46: keyword_search AS (
 47:   SELECT
 48:     c.id,
 49:     (v.keyword_vector <#> query_keyword_embedding) * -1 as keyword_similarity
 50:   FROM clips c
 51:   JOIN vectors v ON c.id = v.clip_id
 52:   WHERE c.user_id = user_id_filter
 53:     AND v.embedding_type = 'full_clip'
 54:     AND v.keyword_vector IS NOT NULL
 55:   ORDER BY v.keyword_vector <#> query_keyword_embedding
 56:   LIMIT LEAST(match_count * 2, 50)
 57: )
 58: SELECT
 59:   ss.id,
 60:   ss.file_name,
 61:   ss.local_path,
 62:   ss.content_summary,
 63:   ss.content_tags,
 64:   ss.duration_seconds,
 65:   ss.camera_make,
 66:   ss.camera_model,
 67:   ss.content_category,
 68:   ss.processed_at,
 69:   ss.summary_similarity,
 70:   COALESCE(ks.keyword_similarity, 0.0) as keyword_similarity,
 71:   (ss.summary_similarity * summary_weight + COALESCE(ks.keyword_similarity, 0.0) * keyword_weight) as combined_similarity
 72: FROM summary_search ss
 73: LEFT JOIN keyword_search ks ON ss.id = ks.id
 74: WHERE ss.summary_similarity >= similarity_threshold
 75: ORDER BY combined_similarity DESC
 76: LIMIT match_count;
 77: $$;
 78: -- Function for hybrid search combining full-text and semantic search using RRF
 79: CREATE OR REPLACE FUNCTION hybrid_search_clips(
 80:   query_text TEXT,
 81:   query_summary_embedding vector(1024),
 82:   query_keyword_embedding vector(1024),
 83:   user_id_filter UUID,
 84:   match_count INT DEFAULT 10,
 85:   fulltext_weight FLOAT DEFAULT 1.0,
 86:   summary_weight FLOAT DEFAULT 1.0,
 87:   keyword_weight FLOAT DEFAULT 0.8,
 88:   rrf_k INT DEFAULT 50
 89: )
 90: RETURNS TABLE (
 91:   id UUID,
 92:   file_name TEXT,
 93:   local_path TEXT,
 94:   content_summary TEXT,
 95:   content_tags TEXT[],
 96:   duration_seconds NUMERIC,
 97:   camera_make TEXT,
 98:   camera_model TEXT,
 99:   content_category TEXT,
100:   processed_at TIMESTAMPTZ,
101:   transcript_preview TEXT,
102:   similarity_score FLOAT,
103:   search_rank FLOAT,
104:   match_type TEXT
105: )
106: LANGUAGE SQL
107: AS $$
108: WITH fulltext AS (
109:   SELECT
110:     c.id, c.file_name, c.local_path, c.content_summary, 
111:     c.content_tags, c.duration_seconds, c.camera_make, c.camera_model,
112:     c.content_category, c.processed_at, c.transcript_preview,
113:     ts_rank_cd(c.fts, websearch_to_tsquery('english', query_text)) as fts_score,
114:     ROW_NUMBER() OVER(ORDER BY ts_rank_cd(c.fts, websearch_to_tsquery('english', query_text)) DESC) as rank_ix
115:   FROM clips c
116:   WHERE c.user_id = user_id_filter
117:     AND c.fts @@ websearch_to_tsquery('english', query_text)
118:   ORDER BY ts_rank_cd(c.fts, websearch_to_tsquery('english', query_text)) DESC
119:   LIMIT LEAST(match_count * 2, 30)
120: ),
121: summary_semantic AS (
122:   SELECT
123:     c.id, c.file_name, c.local_path, c.content_summary,
124:     c.content_tags, c.duration_seconds, c.camera_make, c.camera_model,
125:     c.content_category, c.processed_at, c.transcript_preview,
126:     (v.summary_vector <#> query_summary_embedding) * -1 as similarity_score,
127:     ROW_NUMBER() OVER (ORDER BY v.summary_vector <#> query_summary_embedding) as rank_ix
128:   FROM clips c
129:   JOIN vectors v ON c.id = v.clip_id
130:   WHERE c.user_id = user_id_filter
131:     AND v.embedding_type = 'full_clip'
132:     AND v.summary_vector IS NOT NULL
133:   ORDER BY v.summary_vector <#> query_summary_embedding
134:   LIMIT LEAST(match_count * 2, 30)
135: ),
136: keyword_semantic AS (
137:   SELECT
138:     c.id,
139:     (v.keyword_vector <#> query_keyword_embedding) * -1 as keyword_similarity,
140:     ROW_NUMBER() OVER (ORDER BY v.keyword_vector <#> query_keyword_embedding) as rank_ix
141:   FROM clips c
142:   JOIN vectors v ON c.id = v.clip_id
143:   WHERE c.user_id = user_id_filter
144:     AND v.embedding_type = 'full_clip'
145:     AND v.keyword_vector IS NOT NULL
146:   ORDER BY v.keyword_vector <#> query_keyword_embedding
147:   LIMIT LEAST(match_count * 2, 30)
148: )
149: SELECT
150:   COALESCE(ft.id, ss.id) as id,
151:   COALESCE(ft.file_name, ss.file_name) as file_name,
152:   COALESCE(ft.local_path, ss.local_path) as local_path,
153:   COALESCE(ft.content_summary, ss.content_summary) as content_summary,
154:   COALESCE(ft.content_tags, ss.content_tags) as content_tags,
155:   COALESCE(ft.duration_seconds, ss.duration_seconds) as duration_seconds,
156:   COALESCE(ft.camera_make, ss.camera_make) as camera_make,
157:   COALESCE(ft.camera_model, ss.camera_model) as camera_model,
158:   COALESCE(ft.content_category, ss.content_category) as content_category,
159:   COALESCE(ft.processed_at, ss.processed_at) as processed_at,
160:   COALESCE(ft.transcript_preview, ss.transcript_preview) as transcript_preview,
161:   COALESCE(ss.similarity_score, 0.0) as similarity_score,
162:   -- RRF SCORING WITH DUAL VECTORS AND FULL-TEXT
163:   COALESCE(1.0 / (rrf_k + ft.rank_ix), 0.0) * fulltext_weight +
164:   COALESCE(1.0 / (rrf_k + ss.rank_ix), 0.0) * summary_weight +
165:   COALESCE(1.0 / (rrf_k + ks.rank_ix), 0.0) * keyword_weight as search_rank,
166:   CASE 
167:     WHEN ft.id IS NOT NULL AND ss.id IS NOT NULL THEN 'hybrid'
168:     WHEN ft.id IS NOT NULL THEN 'fulltext'
169:     ELSE 'semantic'
170:   END as match_type
171: FROM fulltext ft
172: FULL OUTER JOIN summary_semantic ss ON ft.id = ss.id
173: FULL OUTER JOIN keyword_semantic ks ON COALESCE(ft.id, ss.id) = ks.id
174: ORDER BY search_rank DESC
175: LIMIT match_count;
176: $$;
177: -- Function for full-text search only
178: CREATE OR REPLACE FUNCTION fulltext_search_clips(
179:   query_text TEXT,
180:   user_id_filter UUID,
181:   match_count INT DEFAULT 10
182: )
183: RETURNS TABLE (
184:   id UUID,
185:   file_name TEXT,
186:   local_path TEXT,
187:   content_summary TEXT,
188:   content_tags TEXT[],
189:   duration_seconds NUMERIC,
190:   camera_make TEXT,
191:   camera_model TEXT,
192:   content_category TEXT,
193:   processed_at TIMESTAMPTZ,
194:   transcript_preview TEXT,
195:   fts_rank FLOAT
196: )
197: LANGUAGE SQL
198: AS $$
199: SELECT
200:   c.id,
201:   c.file_name,
202:   c.local_path,
203:   c.content_summary,
204:   c.content_tags,
205:   c.duration_seconds,
206:   c.camera_make,
207:   c.camera_model,
208:   c.content_category,
209:   c.processed_at,
210:   c.transcript_preview,
211:   ts_rank_cd(c.fts, websearch_to_tsquery('english', query_text)) as fts_rank
212: FROM clips c
213: WHERE c.user_id = user_id_filter
214:   AND c.fts @@ websearch_to_tsquery('english', query_text)
215: ORDER BY ts_rank_cd(c.fts, websearch_to_tsquery('english', query_text)) DESC
216: LIMIT match_count;
217: $$;
218: -- Function to search transcripts specifically
219: CREATE OR REPLACE FUNCTION search_transcripts(
220:   query_text TEXT,
221:   user_id_filter UUID,
222:   match_count INT DEFAULT 10,
223:   min_content_length INT DEFAULT 50
224: )
225: RETURNS TABLE (
226:   clip_id UUID,
227:   file_name TEXT,
228:   local_path TEXT,
229:   content_summary TEXT,
230:   full_text TEXT,
231:   transcript_preview TEXT,
232:   duration_seconds NUMERIC,
233:   processed_at TIMESTAMPTZ,
234:   fts_rank FLOAT
235: )
236: LANGUAGE SQL
237: AS $$
238: SELECT
239:   t.clip_id,
240:   c.file_name,
241:   c.local_path,
242:   c.content_summary,
243:   t.full_text,
244:   c.transcript_preview,
245:   c.duration_seconds,
246:   c.processed_at,
247:   ts_rank_cd(t.fts, websearch_to_tsquery('english', query_text)) as fts_rank
248: FROM transcripts t
249: JOIN clips c ON t.clip_id = c.id
250: WHERE t.user_id = user_id_filter
251:   AND LENGTH(t.full_text) >= min_content_length
252:   AND t.fts @@ websearch_to_tsquery('english', query_text)
253: ORDER BY ts_rank_cd(t.fts, websearch_to_tsquery('english', query_text)) DESC
254: LIMIT match_count;
255: $$;
256: -- Function to find similar clips based on existing clip
257: CREATE OR REPLACE FUNCTION find_similar_clips(
258:   source_clip_id UUID,
259:   user_id_filter UUID,
260:   match_count INT DEFAULT 5,
261:   similarity_threshold FLOAT DEFAULT 0.5
262: )
263: RETURNS TABLE (
264:   id UUID,
265:   file_name TEXT,
266:   local_path TEXT,
267:   content_summary TEXT,
268:   content_tags TEXT[],
269:   duration_seconds NUMERIC,
270:   content_category TEXT,
271:   similarity_score FLOAT
272: )
273: LANGUAGE SQL
274: AS $$
275: WITH source_vector AS (
276:   SELECT v.summary_vector
277:   FROM vectors v
278:   WHERE v.clip_id = source_clip_id
279:     AND v.embedding_type = 'full_clip'
280:     AND v.summary_vector IS NOT NULL
281:   LIMIT 1
282: )
283: SELECT
284:   c.id,
285:   c.file_name,
286:   c.local_path,
287:   c.content_summary,
288:   c.content_tags,
289:   c.duration_seconds,
290:   c.content_category,
291:   (v.summary_vector <#> sv.summary_vector) * -1 as similarity_score
292: FROM clips c
293: JOIN vectors v ON c.id = v.clip_id
294: CROSS JOIN source_vector sv
295: WHERE c.user_id = user_id_filter
296:   AND c.id != source_clip_id
297:   AND v.embedding_type = 'full_clip'
298:   AND v.summary_vector IS NOT NULL
299:   AND (v.summary_vector <#> sv.summary_vector) * -1 >= similarity_threshold
300: ORDER BY v.summary_vector <#> sv.summary_vector
301: LIMIT match_count;
302: $$;
</file>

<file path="video_ingest_tool/models.py">
  1: """
  2: Models for the video ingest tool.
  3: Contains all Pydantic models used for data validation and JSON serialization.
  4: """
  5: import uuid
  6: import datetime
  7: from typing import Any, Dict, List, Optional, Tuple, Union
  8: from pydantic import BaseModel, Field, validator
  9: # ===== Video Metadata Models =====
 10: class AudioTrack(BaseModel):
 11:     """Audio track metadata"""
 12:     track_id: Optional[str] = None
 13:     codec: Optional[str] = None
 14:     codec_id: Optional[str] = None
 15:     channels: Optional[int] = None
 16:     channel_layout: Optional[str] = None
 17:     sample_rate: Optional[int] = None
 18:     bit_depth: Optional[int] = None
 19:     bit_rate_kbps: Optional[int] = None
 20:     language: Optional[str] = None
 21:     duration_seconds: Optional[float] = None
 22: class SubtitleTrack(BaseModel):
 23:     """Subtitle track metadata"""
 24:     track_id: Optional[str] = None
 25:     format: Optional[str] = None
 26:     language: Optional[str] = None
 27:     codec_id: Optional[str] = None
 28:     embedded: Optional[bool] = None
 29: class FileInfo(BaseModel):
 30:     file_path: str
 31:     file_name: str
 32:     file_checksum: str
 33:     file_size_bytes: int
 34:     created_at: Optional[datetime.datetime] = None
 35:     processed_at: datetime.datetime = Field(default_factory=datetime.datetime.now)
 36: class VideoCodecDetails(BaseModel):
 37:     name: Optional[str] = None
 38:     profile: Optional[str] = None
 39:     level: Optional[str] = None
 40:     bitrate_kbps: Optional[int] = None
 41:     bit_depth: Optional[int] = None
 42:     chroma_subsampling: Optional[str] = None
 43:     pixel_format: Optional[str] = None
 44:     bitrate_mode: Optional[str] = None
 45:     cabac: Optional[bool] = None
 46:     ref_frames: Optional[int] = None
 47:     gop_size: Optional[int] = None
 48:     scan_type: Optional[str] = None
 49:     field_order: Optional[str] = None
 50: class VideoResolution(BaseModel):
 51:     width: Optional[int] = None
 52:     height: Optional[int] = None
 53:     aspect_ratio: Optional[str] = None
 54: class VideoHDRDetails(BaseModel):
 55:     is_hdr: bool = False
 56:     format: Optional[str] = None # Corresponds to old hdr_format
 57:     master_display: Optional[str] = None
 58:     max_cll: Optional[int] = None
 59:     max_fall: Optional[int] = None
 60: class VideoColorDetails(BaseModel):
 61:     color_space: Optional[str] = None
 62:     color_primaries: Optional[str] = None
 63:     transfer_characteristics: Optional[str] = None
 64:     matrix_coefficients: Optional[str] = None
 65:     color_range: Optional[str] = None
 66:     hdr: VideoHDRDetails
 67: class VideoExposureDetails(BaseModel):
 68:     warning: Optional[bool] = None
 69:     stops: Optional[float] = None
 70:     overexposed_percentage: Optional[float] = None
 71:     underexposed_percentage: Optional[float] = None
 72: class VideoDetails(BaseModel):
 73:     duration_seconds: Optional[float] = None
 74:     codec: VideoCodecDetails
 75:     container: Optional[str] = None
 76:     resolution: VideoResolution
 77:     frame_rate: Optional[float] = None
 78:     color: VideoColorDetails
 79:     exposure: VideoExposureDetails
 80: class CameraFocalLength(BaseModel):
 81:     value_mm: Optional[float] = None
 82:     category: Optional[str] = None
 83:     source: Optional[str] = None  # "EXIF" or "AI"
 84: class CameraSettings(BaseModel):
 85:     iso: Optional[int] = None
 86:     shutter_speed: Optional[Union[str, float]] = None
 87:     f_stop: Optional[float] = None
 88:     exposure_mode: Optional[str] = None
 89:     white_balance: Optional[str] = None
 90: class CameraLocation(BaseModel):
 91:     gps_latitude: Optional[float] = None
 92:     gps_longitude: Optional[float] = None
 93:     gps_altitude: Optional[float] = None
 94:     location_name: Optional[str] = None
 95: class CameraDetails(BaseModel):
 96:     make: Optional[str] = None
 97:     model: Optional[str] = None
 98:     lens_model: Optional[str] = None
 99:     focal_length: CameraFocalLength
100:     settings: CameraSettings
101:     location: CameraLocation
102: # ===== AI Analysis Models =====
103: class ShotType(BaseModel):
104:     """Individual shot type detection"""
105:     timestamp: str
106:     duration_seconds: Optional[float] = None
107:     shot_type: str
108:     description: str
109:     confidence: Optional[float] = None
110: class TechnicalQuality(BaseModel):
111:     """Technical quality assessment"""
112:     overall_focus_quality: Optional[str] = None
113:     stability_assessment: Optional[str] = None
114:     detected_artifacts: List[Dict[str, Any]] = Field(default_factory=list)
115:     usability_rating: Optional[str] = None
116: class DetectedText(BaseModel):
117:     """Detected text element"""
118:     timestamp: str
119:     text_content: Optional[str] = None
120:     text_type: Optional[str] = None
121:     readability: Optional[str] = None
122: class DetectedLogo(BaseModel):
123:     """Detected logo or icon"""
124:     timestamp: str
125:     description: str
126:     element_type: str
127:     size: Optional[str] = None
128: class TextAndGraphics(BaseModel):
129:     """Text and graphics analysis"""
130:     detected_text: List[DetectedText] = Field(default_factory=list)
131:     detected_logos_icons: List[DetectedLogo] = Field(default_factory=list)
132: class RecommendedKeyframe(BaseModel):
133:     """Recommended keyframe for thumbnails"""
134:     timestamp: str
135:     reason: str
136:     visual_quality: str
137: class KeyframeAnalysis(BaseModel):
138:     """Keyframe analysis results"""
139:     recommended_keyframes: List[RecommendedKeyframe] = Field(default_factory=list)
140: class VisualAnalysis(BaseModel):
141:     """Complete visual analysis results"""
142:     shot_types: List[ShotType] = Field(default_factory=list)
143:     technical_quality: Optional[TechnicalQuality] = None
144:     text_and_graphics: Optional[TextAndGraphics] = None
145:     keyframe_analysis: Optional[KeyframeAnalysis] = None
146: class TranscriptSegment(BaseModel):
147:     """Individual transcript segment"""
148:     timestamp: str
149:     speaker: Optional[str] = None
150:     text: str
151:     confidence: Optional[float] = None
152: class Transcript(BaseModel):
153:     """Complete transcript"""
154:     full_text: Optional[str] = None
155:     segments: List[TranscriptSegment] = Field(default_factory=list)
156: class Speaker(BaseModel):
157:     """Speaker information"""
158:     speaker_id: str
159:     speaking_time_seconds: float
160:     segments_count: Optional[int] = None
161: class SpeakerAnalysis(BaseModel):
162:     """Speaker analysis results"""
163:     speaker_count: int = 0
164:     speakers: List[Speaker] = Field(default_factory=list)
165: class SoundEvent(BaseModel):
166:     """Detected sound event"""
167:     timestamp: str
168:     event_type: str
169:     description: str
170:     duration_seconds: Optional[float] = None
171:     prominence: Optional[str] = None
172: class AudioQuality(BaseModel):
173:     """Audio quality assessment"""
174:     clarity: Optional[str] = None
175:     background_noise_level: Optional[str] = None
176:     dialogue_intelligibility: Optional[str] = None
177: class AudioAnalysis(BaseModel):
178:     """Complete audio analysis results"""
179:     transcript: Optional[Transcript] = None
180:     speaker_analysis: Optional[SpeakerAnalysis] = None
181:     sound_events: List[SoundEvent] = Field(default_factory=list)
182:     audio_quality: Optional[AudioQuality] = None
183: class PersonDetail(BaseModel):
184:     """Individual person details"""
185:     description: str
186:     role: Optional[str] = None
187:     visibility_duration: Optional[str] = None
188: class Location(BaseModel):
189:     """Location information"""
190:     name: str
191:     type: str
192:     description: Optional[str] = None
193: class ObjectOfInterest(BaseModel):
194:     """Object of interest"""
195:     object: str
196:     significance: str
197:     timestamp: Optional[str] = None
198: class Entities(BaseModel):
199:     """Entity detection results"""
200:     people_count: int = 0
201:     people_details: List[PersonDetail] = Field(default_factory=list)
202:     locations: List[Location] = Field(default_factory=list)
203:     objects_of_interest: List[ObjectOfInterest] = Field(default_factory=list)
204: class Activity(BaseModel):
205:     """Activity or action"""
206:     activity: str
207:     timestamp: str
208:     duration: Optional[str] = None
209:     importance: str
210: class ContentWarning(BaseModel):
211:     """Content warning"""
212:     type: str
213:     description: str
214:     timestamp: Optional[str] = None
215: class ContentAnalysis(BaseModel):
216:     """Complete content analysis results"""
217:     entities: Optional[Entities] = None
218:     activity_summary: List[Activity] = Field(default_factory=list)
219:     content_warnings: List[ContentWarning] = Field(default_factory=list)
220: class AIAnalysisSummary(BaseModel):
221:     """AI analysis summary"""
222:     overall: Optional[str] = None
223:     key_activities: List[str] = Field(default_factory=list)
224:     content_category: Optional[str] = None
225: class ComprehensiveAIAnalysis(BaseModel):
226:     """Complete AI analysis results from Gemini"""
227:     summary: Optional[AIAnalysisSummary] = None
228:     visual_analysis: Optional[VisualAnalysis] = None
229:     audio_analysis: Optional[AudioAnalysis] = None
230:     content_analysis: Optional[ContentAnalysis] = None
231:     analysis_file_path: Optional[str] = None  # Path to detailed JSON file
232: class AnalysisDetails(BaseModel):
233:     """Analysis details including both basic and AI analysis"""
234:     scene_changes: List[float] = Field(default_factory=list)
235:     content_tags: List[str] = Field(default_factory=list)
236:     content_summary: Optional[str] = None
237:     ai_analysis: Optional[ComprehensiveAIAnalysis] = None  # New comprehensive AI analysis
238: class VideoIngestOutput(BaseModel):
239:     id: str = Field(default_factory=lambda: str(uuid.uuid4()))
240:     file_info: FileInfo
241:     video: VideoDetails
242:     audio_tracks: List[AudioTrack] = Field(default_factory=list)
243:     subtitle_tracks: List[SubtitleTrack] = Field(default_factory=list)
244:     camera: CameraDetails
245:     thumbnails: List[str] = Field(default_factory=list)
246:     analysis: AnalysisDetails
</file>

<file path="video_ingest_tool/output.py">
 1: """
 2: Output handling for the video ingest tool.
 3: Contains functions for saving data to JSON and potentially other formats in the future.
 4: """
 5: import os
 6: import json
 7: import shutil
 8: from typing import Any, Dict, List, Optional
 9: from pydantic import BaseModel
10: def save_to_json(data: Any, filename: str, logger=None) -> None:
11:     """
12:     Save data to JSON file.
13:     Args:
14:         data: Data to save (can be a Pydantic model or dictionary)
15:         filename: Output filename
16:         logger: Logger instance
17:     """
18:     if logger:
19:         logger.info("Saving data to JSON", filename=filename)
20:     # Handle Pydantic models
21:     if isinstance(data, BaseModel):
22:         data = data.model_dump()
23:     elif isinstance(data, list) and all(isinstance(item, BaseModel) for item in data):
24:         data = [item.model_dump() for item in data]
25:     # Create directory if it doesn't exist
26:     os.makedirs(os.path.dirname(filename), exist_ok=True)
27:     with open(filename, 'w') as f:
28:         json.dump(data, f, indent=2, default=str)
29:     if logger:
30:         logger.info("Data saved to JSON", filename=filename)
31: def save_run_outputs(processed_files: List[Any], run_dir: str, summary_filename: str, json_dir: str,
32:                     log_file: str, logger=None) -> Dict[str, str]:
33:     """
34:     Save all outputs from a processing run.
35:     Args:
36:         processed_files: List of processed video data
37:         run_dir: Directory for this run
38:         run_timestamp: Timestamp string for this run
39:         json_dir: Global JSON output directory
40:         log_file: Path to log file
41:         logger: Logger instance
42:     Returns:
43:         Dict[str, str]: Paths to key output files
44:     """
45:     # Save summary JSON to run-specific directory with the provided filename
46:     run_summary_path = os.path.join(run_dir, "json", summary_filename)
47:     save_to_json(processed_files, run_summary_path, logger)
48:     # Also save to global JSON directory with same filename
49:     global_summary_path = os.path.join(json_dir, summary_filename)
50:     save_to_json(processed_files, global_summary_path, logger)
51:     # Create a copy of the log file in the run directory
52:     run_log_file = os.path.join(run_dir, "ingestor.log")
53:     try:
54:         shutil.copy2(log_file, run_log_file)
55:         if logger:
56:             logger.info("Copied log file to run directory", source=log_file, destination=run_log_file)
57:     except Exception as e:
58:         if logger:
59:             logger.error("Failed to copy log file to run directory", error=str(e))
60:     return {
61:         'run_summary': run_summary_path,
62:         'global_summary': global_summary_path,
63:         'run_log': run_log_file
64:     }
65: # Placeholder for future database output handler
66: # This can be expanded later to save data to a database
67: class DatabaseOutputHandler:
68:     """Placeholder for database output functionality."""
69:     def __init__(self, connection_string: str = None):
70:         self.connection_string = connection_string
71:         self.connected = False
72:     def connect(self) -> bool:
73:         """Connect to the database. Will be implemented in the future."""
74:         # This is just a placeholder for future implementation
75:         self.connected = True
76:         return True
77:     def save(self, data: Any) -> bool:
78:         """Save data to the database. Will be implemented in the future."""
79:         # This is just a placeholder for future implementation
80:         if not self.connected:
81:             return False
82:         return True
83:     def close(self) -> None:
84:         """Close the database connection. Will be implemented in the future."""
85:         # This is just a placeholder for future implementation
86:         self.connected = False
</file>

<file path="video_ingest_tool/pipeline.py">
  1: """
  2: Pipeline manager for the video ingest tool.
  3: Defines the processing pipeline with configurable steps.
  4: """
  5: from typing import List, Dict, Any, Callable, Optional, Union
  6: import structlog
  7: import inspect
  8: class ProcessingStep:
  9:     """
 10:     Represents a single step in the video processing pipeline.
 11:     Each step has a name, function to execute, and can be enabled/disabled.
 12:     """
 13:     def __init__(self, name: str, func: Callable, enabled: bool = True, description: str = ""):
 14:         """
 15:         Initialize a processing step.
 16:         Args:
 17:             name: Name of the step
 18:             func: Function to execute for this step
 19:             enabled: Whether this step is enabled by default
 20:             description: Description of what this step does
 21:         """
 22:         self.name = name
 23:         self.func = func
 24:         self.enabled = enabled
 25:         self.description = description
 26:         # Store the parameter names this function accepts
 27:         self.param_names = set(inspect.signature(func).parameters.keys())
 28:     def execute(self, *args, **kwargs):
 29:         """
 30:         Execute this step if it's enabled.
 31:         Args:
 32:             *args: Arguments to pass to the function
 33:             **kwargs: Keyword arguments to pass to the function
 34:         Returns:
 35:             The result of the function or None if the step is disabled
 36:         """
 37:         if not self.enabled:
 38:             return None
 39:         # Filter kwargs to only include those the function accepts
 40:         filtered_kwargs = {k: v for k, v in kwargs.items() if k in self.param_names}
 41:         return self.func(*args, **filtered_kwargs)
 42:     def __repr__(self):
 43:         return f"<ProcessingStep name={self.name} enabled={self.enabled}>"
 44: class ProcessingPipeline:
 45:     """
 46:     Manages a pipeline of processing steps for video files.
 47:     Allows for configurable steps that can be enabled/disabled.
 48:     """
 49:     def __init__(self, logger=None):
 50:         """
 51:         Initialize the processing pipeline.
 52:         Args:
 53:             logger: Logger instance
 54:         """
 55:         self.steps: List[ProcessingStep] = []
 56:         self.logger = logger or structlog.get_logger(__name__)
 57:     def add_step(self, step: ProcessingStep) -> None:
 58:         """
 59:         Add a step to the pipeline.
 60:         Args:
 61:             step: The step to add
 62:         """
 63:         self.steps.append(step)
 64:         self.logger.info(f"Added step to pipeline: {step.name}", enabled=step.enabled)
 65:     def add_steps(self, steps: List[ProcessingStep]) -> None:
 66:         """
 67:         Add multiple steps to the pipeline.
 68:         Args:
 69:             steps: List of steps to add
 70:         """
 71:         for step in steps:
 72:             self.add_step(step)
 73:     def register_step(self, name: str, enabled: bool = True, description: str = "") -> Callable:
 74:         """
 75:         Decorator to register a function as a pipeline step.
 76:         Args:
 77:             name: Name of the step
 78:             enabled: Whether this step is enabled by default
 79:             description: Description of what this step does
 80:         Returns:
 81:             Decorator function
 82:         """
 83:         def decorator(func: Callable) -> Callable:
 84:             self.add_step(ProcessingStep(name, func, enabled, description))
 85:             return func
 86:         return decorator
 87:     def enable_step(self, name: str) -> None:
 88:         """
 89:         Enable a step by name.
 90:         Args:
 91:             name: Name of the step to enable
 92:         """
 93:         for step in self.steps:
 94:             if step.name == name:
 95:                 step.enabled = True
 96:                 self.logger.info(f"Enabled step: {name}")
 97:                 return
 98:         self.logger.warning(f"Step not found: {name}")
 99:     def disable_step(self, name: str) -> None:
100:         """
101:         Disable a step by name.
102:         Args:
103:             name: Name of the step to disable
104:         """
105:         for step in self.steps:
106:             if step.name == name:
107:                 step.enabled = False
108:                 self.logger.info(f"Disabled step: {name}")
109:                 return
110:         self.logger.warning(f"Step not found: {name}")
111:     def configure_steps(self, config: Dict[str, bool]) -> None:
112:         """
113:         Configure multiple steps at once.
114:         Args:
115:             config: Dictionary mapping step names to enabled status
116:         """
117:         for name, enabled in config.items():
118:             found = False
119:             for step in self.steps:
120:                 if step.name == name:
121:                     step.enabled = enabled
122:                     found = True
123:                     break
124:             if not found:
125:                 self.logger.warning(f"Step not found: {name}")
126:     def get_step(self, name: str) -> Optional[ProcessingStep]:
127:         """
128:         Get a step by name.
129:         Args:
130:             name: Name of the step to get
131:         Returns:
132:             The step or None if not found
133:         """
134:         for step in self.steps:
135:             if step.name == name:
136:                 return step
137:         return None
138:     def get_enabled_steps(self) -> List[ProcessingStep]:
139:         """
140:         Get all enabled steps.
141:         Returns:
142:             List of enabled steps
143:         """
144:         return [step for step in self.steps if step.enabled]
145:     def get_disabled_steps(self) -> List[ProcessingStep]:
146:         """
147:         Get all disabled steps.
148:         Returns:
149:             List of disabled steps
150:         """
151:         return [step for step in self.steps if not step.enabled]
152:     def execute_pipeline(self, initial_data: Dict[str, Any], **kwargs) -> Dict[str, Any]:
153:         """
154:         Execute all enabled steps in the pipeline.
155:         Args:
156:             initial_data: Initial data to pass to the first step
157:             **kwargs: Additional keyword arguments to pass to steps that accept them
158:         Returns:
159:             Dictionary with the results of all steps
160:         """
161:         result = initial_data.copy()
162:         for step in self.steps:
163:             if not step.enabled:
164:                 self.logger.info(f"Skipping disabled step: {step.name}")
165:                 continue
166:             self.logger.info(f"Executing step: {step.name}")
167:             try:
168:                 # Execute the step with the current result and kwargs
169:                 # The step itself will filter kwargs to only those it accepts
170:                 step_result = step.execute(result, **kwargs)
171:                 # If the step returns None, we continue with the current result
172:                 # If it returns a dict, we update our result with it
173:                 # Otherwise, we store the result with the step name as the key
174:                 if step_result is None:
175:                     pass
176:                 elif isinstance(step_result, dict):
177:                     result.update(step_result)
178:                     # Check for duplicate detection - if found and not forcing reprocess, stop pipeline
179:                     if (step.name == "duplicate_check" and 
180:                         step_result.get('is_duplicate') and 
181:                         not kwargs.get('force_reprocess', False)):
182:                         self.logger.info(f"Duplicate file detected - stopping pipeline",
183:                                        existing_id=step_result.get('existing_clip_id'),
184:                                        existing_file=step_result.get('existing_file_name'))
185:                         result['pipeline_stopped'] = True
186:                         result['stop_reason'] = 'duplicate_detected'
187:                         break
188:                 else:
189:                     result[step.name] = step_result
190:             except Exception as e:
191:                 self.logger.error(f"Error in step {step.name}: {str(e)}")
192:                 result[f"{step.name}_error"] = str(e)
193:         return result
</file>

<file path="video_ingest_tool/processor.py">
   1: """
   2: Core processor for the video ingest tool.
   3: Handles the primary processing logic for video ingestion using the pipeline system.
   4: """
   5: import os
   6: import datetime
   7: import uuid
   8: from typing import Any, Dict, List, Optional, Tuple, Union
   9: from .models import (
  10:     VideoIngestOutput, FileInfo, VideoCodecDetails, VideoResolution, VideoHDRDetails,
  11:     VideoColorDetails, VideoExposureDetails, VideoDetails, CameraFocalLength,
  12:     CameraSettings, CameraLocation, CameraDetails, AnalysisDetails,
  13:     AudioTrack, SubtitleTrack, ComprehensiveAIAnalysis, AIAnalysisSummary,
  14:     VisualAnalysis, AudioAnalysis, ContentAnalysis, ShotType, TechnicalQuality,
  15:     TextAndGraphics, DetectedText, DetectedLogo, KeyframeAnalysis, RecommendedKeyframe,
  16:     Transcript, TranscriptSegment, SpeakerAnalysis, Speaker, SoundEvent, AudioQuality,
  17:     Entities, PersonDetail, Location, ObjectOfInterest, Activity, ContentWarning
  18: )
  19: from .utils import calculate_checksum, calculate_aspect_ratio_str
  20: from .extractors import extract_mediainfo, extract_ffprobe_info
  21: from .extractors_extended import extract_exiftool_info, extract_extended_exif_metadata, extract_audio_tracks
  22: from .extractors_hdr import extract_subtitle_tracks, extract_codec_parameters, extract_hdr_metadata
  23: from .processors import generate_thumbnails, analyze_exposure, detect_focal_length_with_ai
  24: from .config import FOCAL_LENGTH_RANGES, HAS_TRANSFORMERS, Config
  25: from .pipeline import ProcessingPipeline, ProcessingStep
  26: # Try to import VideoProcessor - it may not be available if dependencies are missing
  27: try:
  28:     from .video_processor import VideoProcessor
  29:     HAS_VIDEO_PROCESSOR = True
  30: except ImportError as e:
  31:     HAS_VIDEO_PROCESSOR = False
  32:     VIDEO_PROCESSOR_ERROR = str(e)
  33: # Import the centralized config
  34: try:
  35:     from .video_processor import DEFAULT_COMPRESSION_CONFIG
  36: except ImportError:
  37:     # Fallback if circular import issues
  38:     DEFAULT_COMPRESSION_CONFIG = {'fps': 5, 'video_bitrate': '1000k'}
  39: # Create a global pipeline instance
  40: pipeline = ProcessingPipeline()
  41: @pipeline.register_step(
  42:     name="checksum_generation", 
  43:     enabled=True,
  44:     description="Calculate file checksum for deduplication"
  45: )
  46: def generate_checksum(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
  47:     """
  48:     Generate checksum for a video file.
  49:     Args:
  50:         data: Pipeline data containing file_path
  51:         logger: Optional logger
  52:     Returns:
  53:         Dict with checksum information
  54:     """
  55:     file_path = data.get('file_path')
  56:     if not file_path:
  57:         raise ValueError("Missing file_path in data")
  58:     if logger:
  59:         logger.info("Generating checksum", path=file_path)
  60:     checksum = calculate_checksum(file_path)
  61:     file_size_bytes = os.path.getsize(file_path)
  62:     file_name = os.path.basename(file_path)
  63:     return {
  64:         'checksum': checksum,
  65:         'file_size_bytes': file_size_bytes,
  66:         'file_name': file_name
  67:     }
  68: @pipeline.register_step(
  69:     name="duplicate_check", 
  70:     enabled=True,
  71:     description="Check database for existing files with same checksum"
  72: )
  73: def check_duplicate_step(data: Dict[str, Any], logger=None, force_reprocess: bool = False) -> Dict[str, Any]:
  74:     """
  75:     Check if a file with the same checksum already exists in the database.
  76:     Args:
  77:         data: Pipeline data containing checksum and file info
  78:         logger: Optional logger
  79:         force_reprocess: If True, skip duplicate check and proceed with processing
  80:     Returns:
  81:         Dict with duplicate check results
  82:     """
  83:     if force_reprocess:
  84:         if logger:
  85:             logger.info("Force reprocess enabled - skipping duplicate check")
  86:         return {
  87:             'is_duplicate': False,
  88:             'duplicate_check_skipped': True,
  89:             'reason': 'force_reprocess'
  90:         }
  91:     from .auth import AuthManager
  92:     # Check if database storage is enabled (duplicate check only makes sense with database)
  93:     auth_manager = AuthManager()
  94:     if not auth_manager.get_current_session():
  95:         if logger:
  96:             logger.info("No authentication - skipping duplicate check")
  97:         return {
  98:             'is_duplicate': False,
  99:             'duplicate_check_skipped': True,
 100:             'reason': 'not_authenticated'
 101:         }
 102:     checksum = data.get('checksum')
 103:     if not checksum:
 104:         if logger:
 105:             logger.warning("No checksum available for duplicate check")
 106:         return {
 107:             'is_duplicate': False,
 108:             'duplicate_check_skipped': True,
 109:             'reason': 'no_checksum'
 110:         }
 111:     try:
 112:         client = auth_manager.get_authenticated_client()
 113:         if not client:
 114:             if logger:
 115:                 logger.warning("No authenticated client - skipping duplicate check")
 116:             return {
 117:                 'is_duplicate': False,
 118:                 'duplicate_check_skipped': True,
 119:                 'reason': 'no_client'
 120:             }
 121:         # Query database for existing file with same checksum
 122:         result = client.table('clips').select('id, file_name, file_path, processed_at').eq('file_checksum', checksum).execute()
 123:         if result.data:
 124:             existing_file = result.data[0]
 125:             if logger:
 126:                 logger.info(f"Found duplicate file in database", 
 127:                            existing_id=existing_file['id'],
 128:                            existing_file=existing_file['file_name'],
 129:                            existing_path=existing_file['file_path'],
 130:                            processed_at=existing_file['processed_at'])
 131:             return {
 132:                 'is_duplicate': True,
 133:                 'existing_clip_id': existing_file['id'],
 134:                 'existing_file_name': existing_file['file_name'],
 135:                 'existing_file_path': existing_file['file_path'],
 136:                 'existing_processed_at': existing_file['processed_at']
 137:             }
 138:         else:
 139:             if logger:
 140:                 logger.info("No duplicate found - proceeding with processing")
 141:             return {
 142:                 'is_duplicate': False
 143:             }
 144:     except Exception as e:
 145:         if logger:
 146:             logger.warning(f"Duplicate check failed: {str(e)} - proceeding with processing")
 147:         return {
 148:             'is_duplicate': False,
 149:             'duplicate_check_failed': True,
 150:             'error': str(e)
 151:         }
 152: @pipeline.register_step(
 153:     name="mediainfo_extraction", 
 154:     enabled=True,
 155:     description="Extract metadata using MediaInfo"
 156: )
 157: def extract_mediainfo_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 158:     """
 159:     Extract metadata using MediaInfo.
 160:     Args:
 161:         data: Pipeline data containing file_path
 162:         logger: Optional logger
 163:     Returns:
 164:         Dict with mediainfo data
 165:     """
 166:     file_path = data.get('file_path')
 167:     if not file_path:
 168:         raise ValueError("Missing file_path in data")
 169:     mediainfo_data = extract_mediainfo(file_path, logger)
 170:     return {
 171:         'mediainfo_data': mediainfo_data
 172:     }
 173: @pipeline.register_step(
 174:     name="ffprobe_extraction", 
 175:     enabled=True,
 176:     description="Extract metadata using FFprobe/PyAV"
 177: )
 178: def extract_ffprobe_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 179:     """
 180:     Extract metadata using FFprobe/PyAV.
 181:     Args:
 182:         data: Pipeline data containing file_path
 183:         logger: Optional logger
 184:     Returns:
 185:         Dict with ffprobe data
 186:     """
 187:     file_path = data.get('file_path')
 188:     if not file_path:
 189:         raise ValueError("Missing file_path in data")
 190:     ffprobe_data = extract_ffprobe_info(file_path, logger)
 191:     return {
 192:         'ffprobe_data': ffprobe_data
 193:     }
 194: @pipeline.register_step(
 195:     name="exiftool_extraction", 
 196:     enabled=True,
 197:     description="Extract EXIF metadata"
 198: )
 199: def extract_exiftool_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 200:     """
 201:     Extract metadata using ExifTool.
 202:     Args:
 203:         data: Pipeline data containing file_path
 204:         logger: Optional logger
 205:     Returns:
 206:         Dict with exiftool data
 207:     """
 208:     file_path = data.get('file_path')
 209:     if not file_path:
 210:         raise ValueError("Missing file_path in data")
 211:     exiftool_data = extract_exiftool_info(file_path, logger)
 212:     return {
 213:         'exiftool_data': exiftool_data
 214:     }
 215: @pipeline.register_step(
 216:     name="extended_exif_extraction", 
 217:     enabled=True,
 218:     description="Extract extended EXIF metadata"
 219: )
 220: def extract_extended_exif_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 221:     """
 222:     Extract extended EXIF metadata.
 223:     Args:
 224:         data: Pipeline data containing file_path
 225:         logger: Optional logger
 226:     Returns:
 227:         Dict with extended EXIF data
 228:     """
 229:     file_path = data.get('file_path')
 230:     if not file_path:
 231:         raise ValueError("Missing file_path in data")
 232:     extended_exif_data = extract_extended_exif_metadata(file_path, logger)
 233:     return {
 234:         'extended_exif_data': extended_exif_data
 235:     }
 236: @pipeline.register_step(
 237:     name="codec_extraction", 
 238:     enabled=True,
 239:     description="Extract detailed codec parameters"
 240: )
 241: def extract_codec_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 242:     """
 243:     Extract codec parameters.
 244:     Args:
 245:         data: Pipeline data containing file_path
 246:         logger: Optional logger
 247:     Returns:
 248:         Dict with codec data
 249:     """
 250:     file_path = data.get('file_path')
 251:     if not file_path:
 252:         raise ValueError("Missing file_path in data")
 253:     codec_params = extract_codec_parameters(file_path, logger)
 254:     return {
 255:         'codec_params': codec_params
 256:     }
 257: @pipeline.register_step(
 258:     name="hdr_extraction", 
 259:     enabled=True,
 260:     description="Extract HDR metadata"
 261: )
 262: def extract_hdr_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 263:     """
 264:     Extract HDR metadata.
 265:     Args:
 266:         data: Pipeline data containing file_path
 267:         logger: Optional logger
 268:     Returns:
 269:         Dict with HDR data
 270:     """
 271:     file_path = data.get('file_path')
 272:     if not file_path:
 273:         raise ValueError("Missing file_path in data")
 274:     hdr_data = extract_hdr_metadata(file_path, logger)
 275:     return {
 276:         'hdr_data': hdr_data
 277:     }
 278: @pipeline.register_step(
 279:     name="audio_extraction", 
 280:     enabled=True,
 281:     description="Extract audio track information"
 282: )
 283: def extract_audio_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 284:     """
 285:     Extract audio track information.
 286:     Args:
 287:         data: Pipeline data containing file_path
 288:         logger: Optional logger
 289:     Returns:
 290:         Dict with audio track data
 291:     """
 292:     file_path = data.get('file_path')
 293:     if not file_path:
 294:         raise ValueError("Missing file_path in data")
 295:     audio_tracks = extract_audio_tracks(file_path, logger)
 296:     return {
 297:         'audio_tracks': audio_tracks
 298:     }
 299: @pipeline.register_step(
 300:     name="subtitle_extraction", 
 301:     enabled=True,
 302:     description="Extract subtitle track information"
 303: )
 304: def extract_subtitle_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 305:     """
 306:     Extract subtitle track information.
 307:     Args:
 308:         data: Pipeline data containing file_path
 309:         logger: Optional logger
 310:     Returns:
 311:         Dict with subtitle track data
 312:     """
 313:     file_path = data.get('file_path')
 314:     if not file_path:
 315:         raise ValueError("Missing file_path in data")
 316:     subtitle_tracks = extract_subtitle_tracks(file_path, logger)
 317:     return {
 318:         'subtitle_tracks': subtitle_tracks
 319:     }
 320: @pipeline.register_step(
 321:     name="thumbnail_generation", 
 322:     enabled=True,
 323:     description="Generate thumbnails from video"
 324: )
 325: def generate_thumbnails_step(data: Dict[str, Any], thumbnails_dir=None, logger=None) -> Dict[str, Any]:
 326:     """
 327:     Generate thumbnails for a video file.
 328:     Args:
 329:         data: Pipeline data containing file_path and checksum
 330:         thumbnails_dir: Directory to save thumbnails
 331:         logger: Optional logger
 332:     Returns:
 333:         Dict with thumbnail paths
 334:     """
 335:     file_path = data.get('file_path')
 336:     checksum = data.get('checksum')
 337:     if not file_path or not checksum:
 338:         raise ValueError("Missing file_path or checksum in data")
 339:     if not thumbnails_dir:
 340:         raise ValueError("Missing thumbnails_dir parameter")
 341:     # Create thumbnail directory with filename first, then checksum
 342:     base_name = os.path.splitext(os.path.basename(file_path))[0]
 343:     thumbnail_dir_name = f"{base_name}_{checksum}"
 344:     thumbnail_dir_for_file = os.path.join(thumbnails_dir, thumbnail_dir_name)
 345:     thumbnail_paths = generate_thumbnails(file_path, thumbnail_dir_for_file, logger=logger)
 346:     return {
 347:         'thumbnail_paths': thumbnail_paths
 348:     }
 349: @pipeline.register_step(
 350:     name="exposure_analysis", 
 351:     enabled=True,
 352:     description="Analyze exposure in thumbnails"
 353: )
 354: def analyze_exposure_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 355:     """
 356:     Analyze exposure in thumbnails.
 357:     Args:
 358:         data: Pipeline data containing thumbnail_paths
 359:         logger: Optional logger
 360:     Returns:
 361:         Dict with exposure analysis results
 362:     """
 363:     thumbnail_paths = data.get('thumbnail_paths', [])
 364:     if not thumbnail_paths:
 365:         if logger:
 366:             logger.warning("No thumbnails available for exposure analysis")
 367:         return {
 368:             'exposure_data': {}
 369:         }
 370:     exposure_data = analyze_exposure(thumbnail_paths[0], logger)
 371:     return {
 372:         'exposure_data': exposure_data
 373:     }
 374: @pipeline.register_step(
 375:     name="ai_focal_length", 
 376:     enabled=True,
 377:     description="Detect focal length using AI when EXIF data is not available"
 378: )
 379: def detect_focal_length_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 380:     """
 381:     Detect focal length using AI when EXIF data is not available.
 382:     Args:
 383:         data: Pipeline data containing thumbnail_paths and metadata
 384:         logger: Optional logger
 385:     Returns:
 386:         Dict with focal length data
 387:     """
 388:     # Check if we already have focal length information
 389:     exiftool_data = data.get('exiftool_data', {})
 390:     extended_exif_data = data.get('extended_exif_data', {})
 391:     # Check if we have valid focal length data from EXIF (not None/null)
 392:     has_exif_focal_length = (
 393:         exiftool_data.get('focal_length_mm') is not None or
 394:         exiftool_data.get('focal_length_category') is not None or
 395:         extended_exif_data.get('focal_length_mm') is not None or
 396:         extended_exif_data.get('focal_length_category') is not None
 397:     )
 398:     if has_exif_focal_length:
 399:         if logger:
 400:             logger.info("Valid focal length available from EXIF, skipping AI detection")
 401:         return {
 402:             'focal_length_source': 'EXIF'
 403:         }
 404:     thumbnail_paths = data.get('thumbnail_paths', [])
 405:     if not thumbnail_paths:
 406:         if logger:
 407:             logger.warning("No thumbnails available for focal length detection")
 408:         return {
 409:             'focal_length_source': None  # Source is unknown if no thumbnails and no EXIF
 410:         }
 411:     if logger:
 412:         logger.info("Focal length not found, attempting AI detection.")
 413:     category = detect_focal_length_with_ai(
 414:         thumbnail_paths[0],
 415:         FOCAL_LENGTH_RANGES,
 416:         has_transformers=HAS_TRANSFORMERS,
 417:         logger=logger
 418:     )
 419:     if category:
 420:         if logger:
 421:             logger.info(f"AI detected focal length category: {category}")
 422:         return {
 423:             'focal_length_category': category,    # The AI-detected category
 424:             'focal_length_mm': None,              # AI never provides mm value
 425:             'focal_length_source': 'AI'           # Mark as AI-sourced
 426:         }
 427:     if logger:
 428:         logger.warning("AI detection failed to determine focal length")
 429:     return {
 430:         'focal_length_category': None,
 431:         'focal_length_mm': None,
 432:         'focal_length_source': None
 433:     }
 434: @pipeline.register_step(
 435:     name="ai_video_analysis", 
 436:     enabled=False,  # Disabled by default due to API costs
 437:     description="Comprehensive video analysis using Gemini Flash 2.5 AI"
 438: )
 439: def ai_video_analysis_step(data: Dict[str, Any], thumbnails_dir=None, logger=None, compression_fps: int = DEFAULT_COMPRESSION_CONFIG['fps'], compression_bitrate: str = DEFAULT_COMPRESSION_CONFIG['video_bitrate']) -> Dict[str, Any]:
 440:     """
 441:     Perform comprehensive AI video analysis using Gemini Flash 2.5.
 442:     Args:
 443:         data: Pipeline data containing file_path, checksum, and other metadata
 444:         thumbnails_dir: Directory where thumbnails are stored
 445:         logger: Optional logger
 446:         compression_fps: Frame rate for video compression
 447:         compression_bitrate: Bitrate for video compression
 448:     Returns:
 449:         Dict with AI analysis results
 450:     """
 451:     if not HAS_VIDEO_PROCESSOR:
 452:         if logger:
 453:             logger.warning(f"VideoProcessor not available: {VIDEO_PROCESSOR_ERROR}")
 454:         return {
 455:             'ai_analysis_data': {},
 456:             'ai_analysis_file_path': None
 457:         }
 458:     file_path = data.get('file_path')
 459:     checksum = data.get('checksum')
 460:     if not file_path:
 461:         if logger:
 462:             logger.error("No file_path provided for AI analysis")
 463:         return {
 464:             'ai_analysis_data': {},
 465:             'ai_analysis_file_path': None
 466:         }
 467:     try:
 468:         if logger:
 469:             logger.info(f"Starting comprehensive AI analysis for: {os.path.basename(file_path)}")
 470:         # Initialize VideoProcessor with compression configuration
 471:         config = Config()
 472:         # Create compression config with custom parameters
 473:         compression_config = {
 474:             'fps': compression_fps,
 475:             'video_bitrate': compression_bitrate
 476:         }
 477:         video_processor = VideoProcessor(config, compression_config=compression_config)
 478:         # Determine output directory for compressed files
 479:         # Use the parent directory of thumbnails_dir as the run directory
 480:         run_dir = None
 481:         if thumbnails_dir:
 482:             run_dir = os.path.dirname(thumbnails_dir)  # thumbnails_dir is run_dir/thumbnails
 483:         # Process the video (this will compress and analyze)
 484:         result = video_processor.process(file_path, run_dir)
 485:         if not result.get('success'):
 486:             if logger:
 487:                 logger.error(f"AI analysis failed: {result.get('error', 'Unknown error')}")
 488:             return {
 489:                 'ai_analysis_data': {},
 490:                 'ai_analysis_file_path': None
 491:             }
 492:         # Get the analysis results
 493:         analysis_json = result.get('analysis_json', {})
 494:         # Create AI-specific JSON file with proper naming
 495:         if analysis_json and file_path:
 496:             try:
 497:                 import json
 498:                 # Create AI analysis directory in run structure (same level as thumbnails)
 499:                 if run_dir:
 500:                     ai_analysis_dir = os.path.join(run_dir, "ai_analysis")
 501:                     os.makedirs(ai_analysis_dir, exist_ok=True)
 502:                     input_basename = os.path.basename(file_path)
 503:                     ai_filename = f"{os.path.splitext(input_basename)[0]}_AI_analysis.json"
 504:                     ai_analysis_path = os.path.join(ai_analysis_dir, ai_filename)
 505:                     # Save the complete AI analysis to AI-specific file
 506:                     with open(ai_analysis_path, 'w') as f:
 507:                         json.dump(analysis_json, f, indent=2)
 508:                     if logger:
 509:                         logger.info(f"AI analysis saved to: {ai_analysis_path}")
 510:                 else:
 511:                     # No run directory available - skip saving separate AI file
 512:                     ai_analysis_path = None
 513:                     if logger:
 514:                         logger.warning("No run directory available - AI analysis not saved to separate file")
 515:                 # Create summary for main JSON (lightweight)
 516:                 ai_summary = _create_ai_summary(analysis_json)
 517:                 return {
 518:                     'ai_analysis_summary': ai_summary,  # Lightweight summary for main JSON
 519:                     'ai_analysis_file_path': ai_analysis_path,  # Path to full AI analysis
 520:                     'full_ai_analysis_data': analysis_json,  # Full analysis data for model creation
 521:                     'compressed_video_path': result.get('compressed_path')
 522:                 }
 523:             except Exception as e:
 524:                 if logger:
 525:                     logger.error(f"Failed to save AI analysis files: {str(e)}")
 526:         if logger:
 527:             logger.info(f"AI analysis completed successfully")
 528:         return {
 529:             'ai_analysis_summary': {},
 530:             'ai_analysis_file_path': None,
 531:             'full_ai_analysis_data': {},
 532:             'compressed_video_path': result.get('compressed_path')
 533:         }
 534:     except Exception as e:
 535:         if logger:
 536:             logger.error(f"AI analysis failed with exception: {str(e)}")
 537:         return {
 538:             'ai_analysis_summary': {},
 539:             'ai_analysis_file_path': None,
 540:             'full_ai_analysis_data': {},
 541:             'error': str(e)
 542:         }
 543: def _create_ai_summary(analysis_json: Dict[str, Any]) -> Dict[str, Any]:
 544:     """
 545:     Create a lightweight summary of AI analysis for inclusion in main JSON.
 546:     Args:
 547:         analysis_json: Complete AI analysis data
 548:     Returns:
 549:         Dict with summary information
 550:     """
 551:     try:
 552:         summary = {}
 553:         # Extract key summary information
 554:         if 'summary' in analysis_json:
 555:             summary_data = analysis_json['summary']
 556:             summary['content_category'] = summary_data.get('content_category')
 557:             summary['overall_summary'] = summary_data.get('overall')
 558:             summary['key_activities_count'] = len(summary_data.get('key_activities', []))
 559:         # Extract key metrics from visual analysis
 560:         if 'visual_analysis' in analysis_json:
 561:             visual = analysis_json['visual_analysis']
 562:             summary['shot_types_detected'] = len(visual.get('shot_types', []))
 563:             if 'technical_quality' in visual:
 564:                 tech_quality = visual['technical_quality']
 565:                 summary['usability_rating'] = tech_quality.get('usability_rating')
 566:                 summary['focus_quality'] = tech_quality.get('overall_focus_quality')
 567:             if 'text_and_graphics' in visual:
 568:                 text_graphics = visual['text_and_graphics']
 569:                 summary['text_elements_detected'] = len(text_graphics.get('detected_text', []))
 570:                 summary['logos_icons_detected'] = len(text_graphics.get('detected_logos_icons', []))
 571:         # Extract key metrics from audio analysis
 572:         if 'audio_analysis' in analysis_json:
 573:             audio = analysis_json['audio_analysis']
 574:             if 'speaker_analysis' in audio:
 575:                 speaker_analysis = audio['speaker_analysis']
 576:                 summary['speaker_count'] = speaker_analysis.get('speaker_count', 0)
 577:             if 'sound_events' in audio:
 578:                 summary['sound_events_detected'] = len(audio['sound_events'])
 579:             if 'audio_quality' in audio:
 580:                 audio_quality = audio['audio_quality']
 581:                 summary['audio_clarity'] = audio_quality.get('clarity')
 582:                 summary['dialogue_intelligibility'] = audio_quality.get('dialogue_intelligibility')
 583:             # Add transcript preview (first 100 chars)
 584:             if 'transcript' in audio and 'full_text' in audio['transcript']:
 585:                 full_text = audio['transcript']['full_text']
 586:                 if full_text:
 587:                     preview = full_text[:100] + "..." if len(full_text) > 100 else full_text
 588:                     summary['transcript_preview'] = preview
 589:         # Extract key metrics from content analysis
 590:         if 'content_analysis' in analysis_json:
 591:             content = analysis_json['content_analysis']
 592:             if 'entities' in content:
 593:                 entities = content['entities']
 594:                 summary['people_count'] = entities.get('people_count', 0)
 595:                 summary['locations_detected'] = len(entities.get('locations', []))
 596:                 summary['objects_of_interest'] = len(entities.get('objects_of_interest', []))
 597:             if 'activity_summary' in content:
 598:                 activities = content['activity_summary']
 599:                 summary['activities_detected'] = len(activities)
 600:                 high_importance_activities = [a for a in activities if a.get('importance') == 'High']
 601:                 summary['high_importance_activities'] = len(high_importance_activities)
 602:             if 'content_warnings' in content:
 603:                 summary['content_warnings_count'] = len(content['content_warnings'])
 604:         # Add analysis metadata
 605:         summary['analysis_timestamp'] = datetime.datetime.now().isoformat()
 606:         summary['has_comprehensive_analysis'] = True
 607:         return summary
 608:     except Exception as e:
 609:         return {
 610:             'analysis_timestamp': datetime.datetime.now().isoformat(),
 611:             'has_comprehensive_analysis': False,
 612:             'error': f"Failed to create summary: {str(e)}"
 613:         }
 614: @pipeline.register_step(
 615:     name="metadata_consolidation", 
 616:     enabled=True,
 617:     description="Consolidate metadata from all sources"
 618: )
 619: def consolidate_metadata_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 620:     """
 621:     Consolidate metadata from all sources.
 622:     Args:
 623:         data: Pipeline data containing all extracted metadata
 624:         logger: Optional logger
 625:     Returns:
 626:         Dict with consolidated metadata
 627:     """
 628:     mediainfo_data = data.get('mediainfo_data', {})
 629:     ffprobe_data = data.get('ffprobe_data', {})
 630:     exiftool_data = data.get('exiftool_data', {})
 631:     extended_exif_data = data.get('extended_exif_data', {})
 632:     codec_params = data.get('codec_params', {})
 633:     hdr_data = data.get('hdr_data', {})
 634:     # Get focal length info from AI or EXIF
 635:     focal_length_source = data.get('focal_length_source')
 636:     ai_focal_length_info = {
 637:         'category': data.get('focal_length_category'),
 638:         'mm': data.get('focal_length_mm')
 639:     }
 640:     # Initialize the master metadata dictionary
 641:     master_metadata = {}
 642:     # Prioritize sources for technical video properties
 643:     tech_keys = ['codec', 'width', 'height', 'frame_rate', 'bit_rate_kbps', 'bit_depth', 'color_space', 'container', 'duration_seconds', 'profile', 'level', 'chroma_subsampling', 'pixel_format', 'bitrate_mode', 'scan_type', 'field_order', 'cabac', 'ref_frames', 'gop_size']
 644:     for key in tech_keys:
 645:         master_metadata[key] = mediainfo_data.get(key, ffprobe_data.get(key, exiftool_data.get(key, codec_params.get(key))))
 646:     # Prioritize sources for camera/lens info
 647:     camera_keys = ['camera_make', 'camera_model', 'focal_length_mm', 'focal_length_category', 'lens_model', 'iso', 'shutter_speed', 'f_stop', 'exposure_mode', 'white_balance', 'gps_latitude', 'gps_longitude', 'gps_altitude', 'location_name', 'camera_serial_number']
 648:     for key in camera_keys:
 649:         # Prioritize extended_exif_data then exiftool_data for camera specific info
 650:         master_metadata[key] = extended_exif_data.get(key, exiftool_data.get(key, mediainfo_data.get(key, ffprobe_data.get(key))))
 651:     # Prioritize sources for dates
 652:     master_metadata['created_at'] = exiftool_data.get('created_at', mediainfo_data.get('created_at', ffprobe_data.get('created_at')))
 653:     # Merge remaining from specific extractions if not already set or to overwrite with more specific data
 654:     for key, value in codec_params.items():
 655:         if master_metadata.get(key) is None or key in ['profile', 'level', 'pixel_format', 'chroma_subsampling', 'bitrate_mode', 'scan_type', 'field_order', 'cabac', 'ref_frames', 'gop_size']:
 656:             if value is not None: master_metadata[key] = value
 657:     for key, value in hdr_data.items():
 658:         if master_metadata.get(key) is None or key in ['hdr_format', 'master_display', 'max_cll', 'max_fall', 'color_primaries', 'transfer_characteristics', 'matrix_coefficients', 'color_range']:
 659:             if value is not None: master_metadata[key] = value
 660:     for key, value in extended_exif_data.items(): # Ensure all extended_exif_data is considered
 661:         if master_metadata.get(key) is None: # Add if not already set
 662:              if value is not None: master_metadata[key] = value
 663:     # Handle focal length data from different sources
 664:     if focal_length_source == 'AI':
 665:         # Use AI detected values, overriding any EXIF data
 666:         master_metadata['focal_length_source'] = 'AI'
 667:         master_metadata['focal_length_category'] = ai_focal_length_info['category']
 668:         master_metadata['focal_length_mm'] = None  # AI only provides category
 669:         if logger:
 670:             logger.info("Using AI-detected focal length",
 671:                        source='AI',
 672:                        category=ai_focal_length_info['category'])
 673:     elif focal_length_source == 'EXIF':
 674:         # Keep EXIF values from earlier camera_keys import
 675:         master_metadata['focal_length_source'] = 'EXIF'
 676:         if logger:
 677:             logger.info("Using EXIF focal length",
 678:                        source='EXIF',
 679:                        mm=master_metadata.get('focal_length_mm'),
 680:                        category=master_metadata.get('focal_length_category'))
 681:     else:
 682:         # No focal length data available
 683:         master_metadata['focal_length_source'] = None
 684:         master_metadata['focal_length_category'] = None
 685:         master_metadata['focal_length_mm'] = None
 686:         if logger:
 687:             logger.info("No focal length information available")
 688:     return {
 689:         'master_metadata': master_metadata
 690:     }
 691: @pipeline.register_step(
 692:     name="model_creation", 
 693:     enabled=True,
 694:     description="Create Pydantic model from processed data"
 695: )
 696: def create_model_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
 697:     """
 698:     Create Pydantic model from processed data.
 699:     Args:
 700:         data: Pipeline data containing all processed information
 701:         logger: Optional logger
 702:     Returns:
 703:         Dict with the output model
 704:     """
 705:     file_path = data.get('file_path')
 706:     file_name = data.get('file_name')
 707:     checksum = data.get('checksum')
 708:     file_size_bytes = data.get('file_size_bytes')
 709:     processed_at_time = datetime.datetime.now()
 710:     master_metadata = data.get('master_metadata', {})
 711:     thumbnail_paths = data.get('thumbnail_paths', [])
 712:     exposure_data = data.get('exposure_data', {})
 713:     audio_tracks = data.get('audio_tracks', [])
 714:     subtitle_tracks = data.get('subtitle_tracks', [])
 715:     ai_analysis_summary = data.get('ai_analysis_summary', {})
 716:     ai_analysis_file_path = data.get('ai_analysis_file_path')
 717:     # Create complete AI analysis object for main JSON
 718:     ai_analysis_obj = None
 719:     if ai_analysis_summary:
 720:         try:
 721:             # Get the full AI analysis data from the step
 722:             full_ai_analysis = data.get('full_ai_analysis_data', {})
 723:             # Create the complete analysis objects if data is available
 724:             visual_analysis_obj = None
 725:             audio_analysis_obj = None
 726:             content_analysis_obj = None
 727:             if full_ai_analysis.get('visual_analysis'):
 728:                 visual_data = full_ai_analysis['visual_analysis']
 729:                 # Create shot types
 730:                 shot_types = []
 731:                 for shot in visual_data.get('shot_types', []):
 732:                     shot_types.append(ShotType(
 733:                         timestamp=shot.get('timestamp', '00:00:000'),
 734:                         shot_type=shot.get('shot_type', ''),
 735:                         description=shot.get('description', ''),
 736:                         confidence=shot.get('confidence')
 737:                     ))
 738:                 # Create technical quality
 739:                 tech_quality_obj = None
 740:                 if visual_data.get('technical_quality'):
 741:                     tq = visual_data['technical_quality']
 742:                     tech_quality_obj = TechnicalQuality(
 743:                         overall_focus_quality=tq.get('overall_focus_quality'),
 744:                         stability_assessment=tq.get('stability_assessment'),
 745:                         detected_artifacts=tq.get('detected_artifacts', []),
 746:                         usability_rating=tq.get('usability_rating')
 747:                     )
 748:                 # Create text and graphics
 749:                 text_graphics_obj = None
 750:                 if visual_data.get('text_and_graphics'):
 751:                     tg = visual_data['text_and_graphics']
 752:                     detected_text = []
 753:                     for text in tg.get('detected_text', []):
 754:                         detected_text.append(DetectedText(
 755:                             timestamp=text.get('timestamp', '00:00:000'),
 756:                             text_content=text.get('text_content'),
 757:                             text_type=text.get('text_type'),
 758:                             readability=text.get('readability')
 759:                         ))
 760:                     detected_logos = []
 761:                     for logo in tg.get('detected_logos_icons', []):
 762:                         detected_logos.append(DetectedLogo(
 763:                             timestamp=logo.get('timestamp', '00:00:000'),
 764:                             element_type=logo.get('element_type', ''),
 765:                             size=logo.get('size')
 766:                         ))
 767:                     text_graphics_obj = TextAndGraphics(
 768:                         detected_text=detected_text,
 769:                         detected_logos_icons=detected_logos
 770:                     )
 771:                 # Create keyframe analysis
 772:                 keyframe_analysis_obj = None
 773:                 if visual_data.get('keyframe_analysis'):
 774:                     ka = visual_data['keyframe_analysis']
 775:                     recommended_keyframes = []
 776:                     for kf in ka.get('recommended_keyframes', []):
 777:                         recommended_keyframes.append(RecommendedKeyframe(
 778:                             timestamp=kf.get('timestamp', '00:00:000'),
 779:                             reason=kf.get('reason', ''),
 780:                             visual_quality=kf.get('visual_quality', '')
 781:                         ))
 782:                     keyframe_analysis_obj = KeyframeAnalysis(
 783:                         recommended_keyframes=recommended_keyframes
 784:                     )
 785:                 visual_analysis_obj = VisualAnalysis(
 786:                     shot_types=shot_types,
 787:                     technical_quality=tech_quality_obj,
 788:                     text_and_graphics=text_graphics_obj,
 789:                     keyframe_analysis=keyframe_analysis_obj
 790:                 )
 791:             if full_ai_analysis.get('audio_analysis'):
 792:                 audio_data = full_ai_analysis['audio_analysis']
 793:                 # Create transcript
 794:                 transcript_obj = None
 795:                 if audio_data.get('transcript'):
 796:                     t = audio_data['transcript']
 797:                     segments = []
 798:                     for seg in t.get('segments', []):
 799:                         segments.append(TranscriptSegment(
 800:                             timestamp=seg.get('timestamp', '00:00:000'),
 801:                             speaker=seg.get('speaker'),
 802:                             text=seg.get('text', '')
 803:                         ))
 804:                     transcript_obj = Transcript(
 805:                         full_text=t.get('full_text'),
 806:                         segments=segments
 807:                     )
 808:                 # Create speaker analysis
 809:                 speaker_analysis_obj = None
 810:                 if audio_data.get('speaker_analysis'):
 811:                     sa = audio_data['speaker_analysis']
 812:                     speakers = []
 813:                     for speaker in sa.get('speakers', []):
 814:                         speakers.append(Speaker(
 815:                             speaker_id=speaker.get('speaker_id', ''),
 816:                             speaking_time_seconds=speaker.get('speaking_time_seconds', 0.0),
 817:                             segments_count=speaker.get('segments_count')
 818:                         ))
 819:                     speaker_analysis_obj = SpeakerAnalysis(
 820:                         speaker_count=sa.get('speaker_count', 0),
 821:                         speakers=speakers
 822:                     )
 823:                 # Create sound events
 824:                 sound_events = []
 825:                 for event in audio_data.get('sound_events', []):
 826:                     sound_events.append(SoundEvent(
 827:                         timestamp=event.get('timestamp', '00:00:000'),
 828:                         event_type=event.get('event_type', ''),
 829:                         description=event.get('description'),
 830:                         duration_seconds=event.get('duration_seconds'),
 831:                         prominence=event.get('prominence')
 832:                     ))
 833:                 # Create audio quality
 834:                 audio_quality_obj = None
 835:                 if audio_data.get('audio_quality'):
 836:                     aq = audio_data['audio_quality']
 837:                     audio_quality_obj = AudioQuality(
 838:                         clarity=aq.get('clarity'),
 839:                         background_noise_level=aq.get('background_noise_level'),
 840:                         dialogue_intelligibility=aq.get('dialogue_intelligibility')
 841:                     )
 842:                 audio_analysis_obj = AudioAnalysis(
 843:                     transcript=transcript_obj,
 844:                     speaker_analysis=speaker_analysis_obj,
 845:                     sound_events=sound_events,
 846:                     audio_quality=audio_quality_obj
 847:                 )
 848:             if full_ai_analysis.get('content_analysis'):
 849:                 content_data = full_ai_analysis['content_analysis']
 850:                 # Create entities
 851:                 entities_obj = None
 852:                 if content_data.get('entities'):
 853:                     e = content_data['entities']
 854:                     people_details = []
 855:                     for person in e.get('people_details', []):
 856:                         people_details.append(PersonDetail(
 857:                             description=person.get('description'),
 858:                             role=person.get('role'),
 859:                             visibility_duration=person.get('visibility_duration')
 860:                         ))
 861:                     locations = []
 862:                     for location in e.get('locations', []):
 863:                         locations.append(Location(
 864:                             name=location.get('name', ''),
 865:                             type=location.get('type', ''),
 866:                             description=location.get('description')
 867:                         ))
 868:                     objects_of_interest = []
 869:                     for obj in e.get('objects_of_interest', []):
 870:                         objects_of_interest.append(ObjectOfInterest(
 871:                             object=obj.get('object', ''),
 872:                             significance=obj.get('significance', ''),
 873:                             timestamp=obj.get('timestamp')
 874:                         ))
 875:                     entities_obj = Entities(
 876:                         people_count=e.get('people_count', 0),
 877:                         people_details=people_details,
 878:                         locations=locations,
 879:                         objects_of_interest=objects_of_interest
 880:                     )
 881:                 # Create activity summary
 882:                 activity_summary = []
 883:                 for activity in content_data.get('activity_summary', []):
 884:                     activity_summary.append(Activity(
 885:                         activity=activity.get('activity', ''),
 886:                         timestamp=activity.get('timestamp'),
 887:                         duration=activity.get('duration'),
 888:                         importance=activity.get('importance', '')
 889:                     ))
 890:                 # Create content warnings
 891:                 content_warnings = []
 892:                 for warning in content_data.get('content_warnings', []):
 893:                     content_warnings.append(ContentWarning(
 894:                         type=warning.get('type', ''),
 895:                         description=warning.get('description')
 896:                     ))
 897:                 content_analysis_obj = ContentAnalysis(
 898:                     entities=entities_obj,
 899:                     activity_summary=activity_summary,
 900:                     content_warnings=content_warnings
 901:                 )
 902:             # Create AI analysis summary
 903:             summary_obj = AIAnalysisSummary(
 904:                 overall=ai_analysis_summary.get('overall_summary'),
 905:                 key_activities=full_ai_analysis.get('summary', {}).get('key_activities', []),
 906:                 content_category=ai_analysis_summary.get('content_category')
 907:             ) if ai_analysis_summary.get('overall_summary') or ai_analysis_summary.get('content_category') else None
 908:             ai_analysis_obj = ComprehensiveAIAnalysis(
 909:                 summary=summary_obj,
 910:                 visual_analysis=visual_analysis_obj,
 911:                 audio_analysis=audio_analysis_obj,
 912:                 content_analysis=content_analysis_obj,
 913:                 analysis_file_path=ai_analysis_file_path
 914:             )
 915:         except Exception as e:
 916:             if logger:
 917:                 logger.warning(f"Failed to create complete AI analysis: {str(e)}")
 918:                 logger.debug(f"AI analysis data available: {list(data.keys())}")
 919:             ai_analysis_obj = None
 920:     # Create the Pydantic models
 921:     file_info_obj = FileInfo(
 922:         file_path=file_path,
 923:         file_name=file_name,
 924:         file_checksum=checksum,
 925:         file_size_bytes=file_size_bytes,
 926:         created_at=master_metadata.get('created_at'),
 927:         processed_at=processed_at_time
 928:     )
 929:     video_codec_details_obj = VideoCodecDetails(
 930:         name=master_metadata.get('codec'),
 931:         profile=master_metadata.get('profile'),
 932:         level=master_metadata.get('level'),
 933:         bitrate_kbps=master_metadata.get('bit_rate_kbps'),
 934:         bit_depth=master_metadata.get('bit_depth'),
 935:         chroma_subsampling=master_metadata.get('chroma_subsampling'),
 936:         pixel_format=master_metadata.get('pixel_format'),
 937:         bitrate_mode=master_metadata.get('bitrate_mode'),
 938:         cabac=master_metadata.get('cabac'),
 939:         ref_frames=master_metadata.get('ref_frames'),
 940:         gop_size=master_metadata.get('gop_size'),
 941:         scan_type=master_metadata.get('scan_type'),
 942:         field_order=master_metadata.get('field_order')
 943:     )
 944:     video_resolution_obj = VideoResolution(
 945:         width=master_metadata.get('width'),
 946:         height=master_metadata.get('height'),
 947:         aspect_ratio=calculate_aspect_ratio_str(master_metadata.get('width'), master_metadata.get('height'))
 948:     )
 949:     video_hdr_details_obj = VideoHDRDetails(
 950:         is_hdr=bool(master_metadata.get('hdr_format')),
 951:         format=master_metadata.get('hdr_format'),
 952:         master_display=master_metadata.get('master_display'),
 953:         max_cll=master_metadata.get('max_cll'),
 954:         max_fall=master_metadata.get('max_fall')
 955:     )
 956:     video_color_details_obj = VideoColorDetails(
 957:         color_space=master_metadata.get('color_space'),
 958:         color_primaries=master_metadata.get('color_primaries'),
 959:         transfer_characteristics=master_metadata.get('transfer_characteristics'),
 960:         matrix_coefficients=master_metadata.get('matrix_coefficients'),
 961:         color_range=master_metadata.get('color_range'),
 962:         hdr=video_hdr_details_obj
 963:     )
 964:     video_exposure_details_obj = VideoExposureDetails(
 965:         warning=exposure_data.get('exposure_warning'),
 966:         stops=exposure_data.get('exposure_stops'),
 967:         overexposed_percentage=exposure_data.get('overexposed_percentage'),
 968:         underexposed_percentage=exposure_data.get('underexposed_percentage')
 969:     )
 970:     video_details_obj = VideoDetails(
 971:         duration_seconds=master_metadata.get('duration_seconds'),
 972:         codec=video_codec_details_obj,
 973:         container=master_metadata.get('container'),
 974:         resolution=video_resolution_obj,
 975:         frame_rate=master_metadata.get('frame_rate'),
 976:         color=video_color_details_obj,
 977:         exposure=video_exposure_details_obj
 978:     )
 979:     audio_track_models = [AudioTrack(**track) for track in audio_tracks]
 980:     subtitle_track_models = [SubtitleTrack(**track) for track in subtitle_tracks]
 981:     camera_focal_length_obj = CameraFocalLength(
 982:         value_mm=master_metadata.get('focal_length_mm'),
 983:         category=master_metadata.get('focal_length_category'),
 984:         source=master_metadata.get('focal_length_source')  # Will be either 'EXIF', 'AI', or None
 985:     )
 986:     if logger:
 987:         logger.info("Creating focal length object",
 988:                    source=master_metadata.get('focal_length_source'),
 989:                    category=master_metadata.get('focal_length_category'),
 990:                    value_mm=master_metadata.get('focal_length_mm'))
 991:     camera_settings_obj = CameraSettings(
 992:         iso=master_metadata.get('iso'),
 993:         shutter_speed=master_metadata.get('shutter_speed'),
 994:         f_stop=master_metadata.get('f_stop'),
 995:         exposure_mode=master_metadata.get('exposure_mode'),
 996:         white_balance=master_metadata.get('white_balance')
 997:     )
 998:     camera_location_obj = CameraLocation(
 999:         gps_latitude=master_metadata.get('gps_latitude'),
1000:         gps_longitude=master_metadata.get('gps_longitude'),
1001:         gps_altitude=master_metadata.get('gps_altitude'),
1002:         location_name=master_metadata.get('location_name')
1003:     )
1004:     camera_details_obj = CameraDetails(
1005:         make=master_metadata.get('camera_make'),
1006:         model=master_metadata.get('camera_model'),
1007:         lens_model=master_metadata.get('lens_model'),
1008:         focal_length=camera_focal_length_obj,
1009:         settings=camera_settings_obj,
1010:         location=camera_location_obj
1011:     )
1012:     # Extract content tags and summary from AI analysis if available
1013:     content_tags = []
1014:     content_summary = None
1015:     if ai_analysis_summary:
1016:         # Use AI analysis to populate content tags and summary
1017:         if ai_analysis_summary.get('content_category'):
1018:             content_tags.append(ai_analysis_summary['content_category'])
1019:         # Add key metrics as tags
1020:         if ai_analysis_summary.get('speaker_count', 0) > 0:
1021:             content_tags.append(f"speakers:{ai_analysis_summary['speaker_count']}")
1022:         if ai_analysis_summary.get('usability_rating'):
1023:             content_tags.append(f"quality:{ai_analysis_summary['usability_rating'].lower()}")
1024:         # Add actual shot types instead of count
1025:         full_ai_analysis = data.get('full_ai_analysis_data', {})
1026:         if full_ai_analysis.get('visual_analysis', {}).get('shot_types'):
1027:             shot_types = full_ai_analysis['visual_analysis']['shot_types']
1028:             unique_shot_types = set()
1029:             for shot in shot_types:
1030:                 shot_type = shot.get('shot_type', '').strip()
1031:                 if shot_type:
1032:                     unique_shot_types.add(shot_type)
1033:             # Add each unique shot type as a tag
1034:             for shot_type in sorted(unique_shot_types):
1035:                 content_tags.append(shot_type)
1036:         # Use overall summary as content summary
1037:         content_summary = ai_analysis_summary.get('overall_summary')
1038:     analysis_details_obj = AnalysisDetails(
1039:         scene_changes=[],  # Placeholder for future implementation
1040:         content_tags=content_tags,  # Populated from AI analysis
1041:         content_summary=content_summary,  # Populated from AI analysis
1042:         ai_analysis=ai_analysis_obj  # Minimal AI analysis with file path
1043:     )
1044:     output = VideoIngestOutput(
1045:         id=str(uuid.uuid4()),
1046:         file_info=file_info_obj,
1047:         video=video_details_obj,
1048:         audio_tracks=audio_track_models,
1049:         subtitle_tracks=subtitle_track_models,
1050:         camera=camera_details_obj,
1051:         thumbnails=thumbnail_paths,
1052:         analysis=analysis_details_obj
1053:     )
1054:     return {
1055:         'output': output
1056:     }
1057: @pipeline.register_step(
1058:     name="database_storage", 
1059:     enabled=False,  # Disabled by default
1060:     description="Store video metadata and analysis in Supabase database"
1061: )
1062: def database_storage_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
1063:     """
1064:     Store video data in Supabase database.
1065:     Args:
1066:         data: Pipeline data containing the output model
1067:         logger: Optional logger
1068:     Returns:
1069:         Dict with database storage results
1070:     """
1071:     from .auth import AuthManager
1072:     from .database_storage import store_video_in_database
1073:     # Check authentication
1074:     auth_manager = AuthManager()
1075:     if not auth_manager.get_current_session():
1076:         if logger:
1077:             logger.warning("Skipping database storage - not authenticated")
1078:         return {
1079:             'database_storage_skipped': True,
1080:             'reason': 'not_authenticated'
1081:         }
1082:     output = data.get('output')
1083:     if not output:
1084:         if logger:
1085:             logger.error("No output model found for database storage")
1086:         return {
1087:             'database_storage_failed': True,
1088:             'reason': 'no_output_model'
1089:         }
1090:     try:
1091:         result = store_video_in_database(output, logger)
1092:         if logger:
1093:             logger.info(f"Successfully stored video in database: {result.get('clip_id')}")
1094:         return result
1095:     except Exception as e:
1096:         if logger:
1097:             logger.error(f"Database storage failed: {str(e)}")
1098:         return {
1099:             'database_storage_failed': True,
1100:             'error': str(e)
1101:         }
1102: @pipeline.register_step(
1103:     name="generate_embeddings", 
1104:     enabled=False,  # Disabled by default
1105:     description="Generate vector embeddings for semantic search"
1106: )
1107: def generate_embeddings_step(data: Dict[str, Any], logger=None) -> Dict[str, Any]:
1108:     """
1109:     Generate and store vector embeddings for semantic search.
1110:     Args:
1111:         data: Pipeline data containing the output model and clip_id
1112:         logger: Optional logger
1113:     Returns:
1114:         Dict with embedding generation results
1115:     """
1116:     from .auth import AuthManager
1117:     from .embeddings import prepare_embedding_content, generate_embeddings, store_embeddings
1118:     # Check authentication
1119:     auth_manager = AuthManager()
1120:     if not auth_manager.get_current_session():
1121:         if logger:
1122:             logger.warning("Skipping embedding generation - not authenticated")
1123:         return {
1124:             'embeddings_skipped': True,
1125:             'reason': 'not_authenticated'
1126:         }
1127:     # Get clip_id from database storage results
1128:     clip_id = data.get('clip_id')
1129:     if not clip_id:
1130:         if logger:
1131:             logger.error("No clip_id found for embedding generation")
1132:         return {
1133:             'embeddings_failed': True,
1134:             'reason': 'no_clip_id'
1135:         }
1136:     # Get output model
1137:     output = data.get('output')
1138:     if not output:
1139:         if logger:
1140:             logger.error("No output model found for embedding generation")
1141:         return {
1142:             'embeddings_failed': True,
1143:             'reason': 'no_output_model'
1144:         }
1145:     try:
1146:         # Prepare embedding content using the existing function
1147:         summary_content, keyword_content, metadata = prepare_embedding_content(output)
1148:         if logger:
1149:             logger.info(f"Prepared embedding content - Summary: {metadata['summary_tokens']} tokens, Keywords: {metadata['keyword_tokens']} tokens")
1150:         # Generate embeddings
1151:         summary_embedding, keyword_embedding = generate_embeddings(
1152:             summary_content, keyword_content, logger
1153:         )
1154:         # Store embeddings in database
1155:         original_content = f"Summary: {summary_content}\nKeywords: {keyword_content}"
1156:         store_embeddings(
1157:             clip_id=clip_id,
1158:             summary_embedding=summary_embedding,
1159:             keyword_embedding=keyword_embedding,
1160:             summary_content=summary_content,
1161:             keyword_content=keyword_content,
1162:             original_content=original_content,
1163:             metadata=metadata,
1164:             logger=logger
1165:         )
1166:         if logger:
1167:             logger.info(f"Successfully generated and stored embeddings for clip: {clip_id}")
1168:         return {
1169:             'embeddings_generated': True,
1170:             'clip_id': clip_id,
1171:             'summary_tokens': metadata['summary_tokens'],
1172:             'keyword_tokens': metadata['keyword_tokens'],
1173:             'truncation_applied': metadata['summary_truncation'] != 'none' or metadata['keyword_truncation'] != 'none'
1174:         }
1175:     except Exception as e:
1176:         if logger:
1177:             logger.error(f"Embedding generation failed: {str(e)}")
1178:         return {
1179:             'embeddings_failed': True,
1180:             'error': str(e)
1181:         }
1182: def process_video_file(file_path: str, thumbnails_dir: str, logger=None, config: Dict[str, bool] = None, compression_fps: int = DEFAULT_COMPRESSION_CONFIG['fps'], compression_bitrate: str = DEFAULT_COMPRESSION_CONFIG['video_bitrate'], force_reprocess: bool = False) -> VideoIngestOutput:
1183:     """
1184:     Process a video file using the pipeline.
1185:     Args:
1186:         file_path: Path to the video file
1187:         thumbnails_dir: Directory to save thumbnails
1188:         logger: Optional logger
1189:         config: Dictionary of step configurations (enabled/disabled)
1190:         compression_fps: Frame rate for video compression (default: 5)
1191:         compression_bitrate: Bitrate for video compression (default: 500k)
1192:         force_reprocess: Force reprocessing even if file exists in database
1193:     Returns:
1194:         VideoIngestOutput: Processed video data object
1195:     """
1196:     if logger:
1197:         logger.info("Processing video file", path=file_path)
1198:     # Configure pipeline if config is provided
1199:     if config:
1200:         pipeline.configure_steps(config)
1201:     # Initial data
1202:     initial_data = {
1203:         'file_path': file_path,
1204:         'processed_at': datetime.datetime.now()
1205:     }
1206:     # Execute pipeline
1207:     result = pipeline.execute_pipeline(
1208:         initial_data, 
1209:         thumbnails_dir=thumbnails_dir,
1210:         logger=logger,
1211:         compression_fps=compression_fps,
1212:         compression_bitrate=compression_bitrate,
1213:         force_reprocess=force_reprocess
1214:     )
1215:     # Check if pipeline was stopped due to duplicate detection
1216:     if result.get('pipeline_stopped') and result.get('stop_reason') == 'duplicate_detected':
1217:         if logger:
1218:             logger.info("Skipping file - duplicate detected", 
1219:                        path=file_path,
1220:                        existing_id=result.get('existing_clip_id'),
1221:                        existing_file=result.get('existing_file_name'))
1222:         # Return a special marker indicating this was skipped
1223:         return {
1224:             'skipped': True,
1225:             'reason': 'duplicate_detected',
1226:             'existing_clip_id': result.get('existing_clip_id'),
1227:             'existing_file_name': result.get('existing_file_name'),
1228:             'existing_file_path': result.get('existing_file_path'),
1229:             'existing_processed_at': result.get('existing_processed_at')
1230:         }
1231:     # Return the output model
1232:     output = result.get('output')
1233:     if not output:
1234:         raise ValueError("Pipeline did not produce an output model")
1235:     if logger:
1236:         logger.info("Video processing complete", path=file_path, id=output.id)
1237:     return output
1238: # Optional: Function to get default pipeline configuration
1239: def get_default_pipeline_config() -> Dict[str, bool]:
1240:     """
1241:     Get the default pipeline configuration.
1242:     Returns:
1243:         Dict[str, bool]: Dictionary mapping step names to enabled status
1244:     """
1245:     return {step.name: step.enabled for step in pipeline.steps}
1246: # Optional: Function to get available pipeline steps
1247: def get_available_pipeline_steps() -> List[Dict[str, Any]]:
1248:     """
1249:     Get available pipeline steps with descriptions.
1250:     Returns:
1251:         List[Dict[str, Any]]: List of dictionaries with step information
1252:     """
1253:     return [
1254:         {
1255:             'name': step.name,
1256:             'enabled': step.enabled,
1257:             'description': step.description
1258:         }
1259:         for step in pipeline.steps
1260:     ]
</file>

<file path="video_ingest_tool/processors.py">
  1: """
  2: Processors for the video ingest tool.
  3: Contains functions for thumbnail generation, exposure analysis, and AI detection.
  4: """
  5: import os
  6: import cv2
  7: import math
  8: import numpy as np
  9: import av
 10: from PIL import Image
 11: import torch
 12: from typing import Any, Dict, List, Optional, Tuple, Union
 13: def generate_thumbnails(file_path: str, output_dir: str, count: int = 5, logger=None) -> List[str]:
 14:     """
 15:     Generate thumbnails from video file using PyAV.
 16:     Args:
 17:         file_path: Path to the video file
 18:         output_dir: Directory to save thumbnails
 19:         count: Number of thumbnails to generate
 20:         logger: Logger instance
 21:     Returns:
 22:         List[str]: Paths to generated thumbnails
 23:     """
 24:     if logger:
 25:         logger.info("Generating thumbnails", path=file_path, count=count)
 26:     thumbnail_paths = []
 27:     try:
 28:         os.makedirs(output_dir, exist_ok=True)
 29:         with av.open(file_path) as container:
 30:             duration = float(container.duration / 1000000) if container.duration else 0
 31:             if duration <= 0:
 32:                 if logger:
 33:                     logger.error("Could not determine video duration", path=file_path)
 34:                 return []
 35:             positions = [duration * i / (count + 1) for i in range(1, count + 1)]
 36:             if not container.streams.video:
 37:                 if logger:
 38:                     logger.error("No video stream found", path=file_path)
 39:                 return []
 40:             stream = container.streams.video[0]
 41:             for i, position in enumerate(positions):
 42:                 output_path = os.path.join(output_dir, f"{os.path.basename(file_path)}_{i}.jpg")
 43:                 container.seek(int(position * 1000000), stream=stream)
 44:                 for frame in container.decode(video=0):
 45:                     img = frame.to_image()
 46:                     width, height = img.size
 47:                     new_width = 640
 48:                     new_height = int(height * new_width / width)
 49:                     img = img.resize((new_width, new_height), Image.LANCZOS)
 50:                     img.save(output_path, quality=95)
 51:                     thumbnail_paths.append(output_path)
 52:                     if logger:
 53:                         logger.info("Generated thumbnail", path=output_path, position=position)
 54:                     break
 55:         if logger:
 56:             logger.info("Thumbnail generation complete", path=file_path, count=len(thumbnail_paths))
 57:         return thumbnail_paths
 58:     except Exception as e:
 59:         if logger:
 60:             logger.error("Thumbnail generation failed", path=file_path, error=str(e))
 61:         return []
 62: def analyze_exposure(thumbnail_path: str, logger=None) -> Dict[str, Any]:
 63:     """
 64:     Analyze exposure in an image.
 65:     Args:
 66:         thumbnail_path: Path to the thumbnail image
 67:         logger: Logger instance
 68:     Returns:
 69:         Dict: Exposure analysis results including warning flag and exposure deviation in stops
 70:     """
 71:     if logger:
 72:         logger.info("Analyzing exposure", path=thumbnail_path)
 73:     try:
 74:         image = cv2.imread(thumbnail_path)
 75:         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
 76:         hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
 77:         hist = hist.flatten() / (gray.shape[0] * gray.shape[1])
 78:         overexposed = sum(hist[240:])
 79:         underexposed = sum(hist[:16])
 80:         # Calculate exposure warning flag
 81:         exposure_warning = overexposed > 0.05 or underexposed > 0.05
 82:         # Estimate exposure deviation in stops
 83:         exposure_stops = 0.0
 84:         if overexposed > underexposed and overexposed > 0.05:
 85:             # Rough approximation of stops overexposed
 86:             exposure_stops = math.log2(overexposed * 20)
 87:         elif underexposed > 0.05:
 88:             # Rough approximation of stops underexposed (negative value)
 89:             exposure_stops = -math.log2(underexposed * 20)
 90:         result = {
 91:             'exposure_warning': exposure_warning,
 92:             'exposure_stops': exposure_stops,
 93:             'overexposed_percentage': float(overexposed * 100),
 94:             'underexposed_percentage': float(underexposed * 100)
 95:         }
 96:         if logger:
 97:             logger.info("Exposure analysis complete", path=thumbnail_path, result=result)
 98:         return result
 99:     except Exception as e:
100:         if logger:
101:             logger.error("Exposure analysis failed", path=thumbnail_path, error=str(e))
102:         return {
103:             'exposure_warning': False,
104:             'exposure_stops': 0.0,
105:             'overexposed_percentage': 0.0,
106:             'underexposed_percentage': 0.0
107:         }
108: def detect_focal_length_with_ai(image_path: str, focal_length_ranges: dict, has_transformers: bool = False, logger=None) -> Optional[str]:
109:     """
110:     Use AI to detect the focal length category from an image when EXIF data is not available.
111:     Args:
112:         image_path: Path to the image file
113:         focal_length_ranges: Dictionary of focal length ranges
114:         has_transformers: Whether transformers library is available
115:         logger: Logger instance
116:     Returns:
117:         Optional[str]: Focal length category (e.g., 'ULTRA-WIDE', 'WIDE', etc.)
118:     """
119:     if not has_transformers:
120:         if logger:
121:             logger.warning("AI-based focal length detection requested but transformers library is not available",
122:                         path=image_path)
123:         return None
124:     try:
125:         if logger:
126:             logger.info("Using AI to detect focal length", path=image_path)
127:         # Import here to avoid errors if module is not available
128:         from transformers import pipeline
129:         # Device selection logic - prioritize MPS, then CUDA, then CPU
130:         if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
131:             device = "mps"
132:         elif torch.cuda.is_available():
133:             device = "cuda"
134:         else:
135:             device = "cpu"
136:         if logger:
137:             logger.info(f"Using device for AI model: {device}")
138:         # Create a pipeline using the Hugging Face model
139:         pipe = pipeline("image-classification", model="tonyassi/camera-lens-focal-length", device=device)
140:         # Load the image
141:         pil_image = Image.open(image_path)
142:         # Run the model to estimate focal length category
143:         prediction_result = pipe(pil_image)
144:         # Extract the top prediction
145:         if prediction_result and len(prediction_result) > 0:
146:             top_prediction = prediction_result[0]
147:             category = top_prediction["label"]
148:             confidence = top_prediction["score"]
149:             if logger:
150:                 logger.info(f"AI detected focal length category: {category} (confidence: {confidence:.4f})",
151:                           path=image_path, category=category, confidence=confidence)
152:             return category
153:         else:
154:             if logger:
155:                 logger.warning("AI model did not return predictions for focal length", path=image_path)
156:             return None
157:     except Exception as e:
158:         if logger:
159:             logger.error("Error using AI to detect focal length", path=image_path, error=str(e))
160:         return None
</file>

<file path="video_ingest_tool/search.py">
  1: """
  2: Search utilities for video catalog using hybrid search.
  3: Provides functions for semantic search, full-text search, and hybrid search
  4: combining both approaches using Reciprocal Rank Fusion (RRF).
  5: """
  6: import os
  7: from typing import List, Dict, Any, Optional, Tuple, Literal
  8: import structlog
  9: from .auth import AuthManager
 10: from .embeddings import generate_embeddings
 11: logger = structlog.get_logger(__name__)
 12: def prepare_search_embeddings(query: str) -> Tuple[str, str]:
 13:     """
 14:     Prepare search query for embedding generation.
 15:     Args:
 16:         query: Search query text
 17:     Returns:
 18:         Tuple of (summary_content, keyword_content)
 19:     """
 20:     # For search queries, we use the query as both summary and keyword content
 21:     # This ensures we get meaningful embeddings for comparison
 22:     summary_content = f"Video content about: {query}"
 23:     keyword_content = query
 24:     return summary_content, keyword_content
 25: SearchType = Literal["semantic", "fulltext", "hybrid", "transcripts", "similar"]
 26: class VideoSearcher:
 27:     """
 28:     Video search utility class for performing various types of searches.
 29:     """
 30:     def __init__(self):
 31:         self.auth_manager = AuthManager()
 32:     def _get_authenticated_client(self):
 33:         """Get authenticated Supabase client."""
 34:         client = self.auth_manager.get_authenticated_client()
 35:         if not client:
 36:             raise ValueError("Authentication required for search operations")
 37:         return client
 38:     def _get_current_user_id(self):
 39:         """Get current authenticated user ID."""
 40:         session = self.auth_manager.get_current_session()
 41:         if not session:
 42:             raise ValueError("Authentication required")
 43:         return session.get('user_id')
 44:     def search(
 45:         self,
 46:         query: str,
 47:         search_type: SearchType = "hybrid",
 48:         match_count: int = 10,
 49:         filters: Optional[Dict[str, Any]] = None,
 50:         weights: Optional[Dict[str, float]] = None
 51:     ) -> List[Dict[str, Any]]:
 52:         """
 53:         Perform search across video catalog.
 54:         Args:
 55:             query: Search query text
 56:             search_type: Type of search to perform
 57:             match_count: Number of results to return
 58:             filters: Optional filters (camera_make, content_category, etc.)
 59:             weights: Optional search weights for hybrid search
 60:         Returns:
 61:             List of matching video clips with metadata
 62:         """
 63:         client = self._get_authenticated_client()
 64:         user_id = self._get_current_user_id()
 65:         if search_type == "semantic":
 66:             return self._semantic_search(client, user_id, query, match_count, weights)
 67:         elif search_type == "fulltext":
 68:             return self._fulltext_search(client, user_id, query, match_count)
 69:         elif search_type == "hybrid":
 70:             return self._hybrid_search(client, user_id, query, match_count, weights)
 71:         elif search_type == "transcripts":
 72:             return self._transcript_search(client, user_id, query, match_count)
 73:         else:
 74:             raise ValueError(f"Unsupported search type: {search_type}")
 75:     def find_similar(
 76:         self,
 77:         clip_id: str,
 78:         match_count: int = 5,
 79:         similarity_threshold: float = 0.5
 80:     ) -> List[Dict[str, Any]]:
 81:         """
 82:         Find clips similar to a given clip.
 83:         Args:
 84:             clip_id: ID of the source clip
 85:             match_count: Number of similar clips to return
 86:             similarity_threshold: Minimum similarity score
 87:         Returns:
 88:             List of similar clips
 89:         """
 90:         client = self._get_authenticated_client()
 91:         user_id = self._get_current_user_id()
 92:         try:
 93:             result = client.rpc('find_similar_clips', {
 94:                 'source_clip_id': clip_id,
 95:                 'user_id_filter': user_id,
 96:                 'match_count': match_count,
 97:                 'similarity_threshold': similarity_threshold
 98:             }).execute()
 99:             return result.data if result.data else []
100:         except Exception as e:
101:             logger.error(f"Similar search failed: {str(e)}")
102:             raise
103:     def _semantic_search(
104:         self,
105:         client,
106:         user_id: str,
107:         query: str,
108:         match_count: int,
109:         weights: Optional[Dict[str, float]] = None
110:     ) -> List[Dict[str, Any]]:
111:         """Perform semantic search using vector embeddings."""
112:         try:
113:             # Prepare content for embedding
114:             summary_content, keyword_content = prepare_search_embeddings(query)
115:             # Generate embeddings
116:             summary_embedding, keyword_embedding = generate_embeddings(
117:                 summary_content, keyword_content, logger
118:             )
119:             # Set default weights (only semantic search parameters)
120:             search_params = {
121:                 'summary_weight': weights.get('summary_weight', 1.0) if weights else 1.0,
122:                 'keyword_weight': weights.get('keyword_weight', 0.8) if weights else 0.8,
123:                 'similarity_threshold': weights.get('similarity_threshold', 0.0) if weights else 0.0
124:             }
125:             # Execute semantic search
126:             result = client.rpc('semantic_search_clips', {
127:                 'query_summary_embedding': summary_embedding,
128:                 'query_keyword_embedding': keyword_embedding,
129:                 'user_id_filter': user_id,
130:                 'match_count': match_count,
131:                 **search_params
132:             }).execute()
133:             return result.data if result.data else []
134:         except Exception as e:
135:             logger.error(f"Semantic search failed: {str(e)}")
136:             raise
137:     def _fulltext_search(
138:         self,
139:         client,
140:         user_id: str,
141:         query: str,
142:         match_count: int
143:     ) -> List[Dict[str, Any]]:
144:         """Perform full-text search."""
145:         try:
146:             result = client.rpc('fulltext_search_clips', {
147:                 'query_text': query,
148:                 'user_id_filter': user_id,
149:                 'match_count': match_count
150:             }).execute()
151:             return result.data if result.data else []
152:         except Exception as e:
153:             logger.error(f"Full-text search failed: {str(e)}")
154:             raise
155:     def _hybrid_search(
156:         self,
157:         client,
158:         user_id: str,
159:         query: str,
160:         match_count: int,
161:         weights: Optional[Dict[str, float]] = None
162:     ) -> List[Dict[str, Any]]:
163:         """Perform hybrid search combining full-text and semantic search."""
164:         try:
165:             # Prepare content for embedding
166:             summary_content, keyword_content = prepare_search_embeddings(query)
167:             # Generate embeddings
168:             summary_embedding, keyword_embedding = generate_embeddings(
169:                 summary_content, keyword_content, logger
170:             )
171:             # Set default weights for RRF
172:             search_params = {
173:                 'fulltext_weight': weights.get('fulltext_weight', 1.0) if weights else 1.0,
174:                 'summary_weight': weights.get('summary_weight', 1.0) if weights else 1.0,
175:                 'keyword_weight': weights.get('keyword_weight', 0.8) if weights else 0.8,
176:                 'rrf_k': weights.get('rrf_k', 50) if weights else 50
177:             }
178:             # Execute hybrid search
179:             result = client.rpc('hybrid_search_clips', {
180:                 'query_text': query,
181:                 'query_summary_embedding': summary_embedding,
182:                 'query_keyword_embedding': keyword_embedding,
183:                 'user_id_filter': user_id,
184:                 'match_count': match_count,
185:                 **search_params
186:             }).execute()
187:             return result.data if result.data else []
188:         except Exception as e:
189:             logger.error(f"Hybrid search failed: {str(e)}")
190:             raise
191:     def _transcript_search(
192:         self,
193:         client,
194:         user_id: str,
195:         query: str,
196:         match_count: int
197:     ) -> List[Dict[str, Any]]:
198:         """Perform search specifically on transcripts."""
199:         try:
200:             result = client.rpc('search_transcripts', {
201:                 'query_text': query,
202:                 'user_id_filter': user_id,
203:                 'match_count': match_count,
204:                 'min_content_length': 50
205:             }).execute()
206:             return result.data if result.data else []
207:         except Exception as e:
208:             logger.error(f"Transcript search failed: {str(e)}")
209:             raise
210:     def get_user_stats(self) -> Dict[str, Any]:
211:         """Get user statistics for the video catalog."""
212:         client = self._get_authenticated_client()
213:         try:
214:             result = client.rpc('get_user_stats').execute()
215:             return result.data[0] if result.data else {}
216:         except Exception as e:
217:             logger.error(f"Failed to get user stats: {str(e)}")
218:             return {}
219: def format_search_results(
220:     results: List[Dict[str, Any]], 
221:     search_type: SearchType,
222:     show_scores: bool = True
223: ) -> List[Dict[str, Any]]:
224:     """
225:     Format search results for display.
226:     Args:
227:         results: Raw search results
228:         search_type: Type of search performed
229:         show_scores: Whether to include similarity/ranking scores
230:     Returns:
231:         Formatted results for display
232:     """
233:     formatted_results = []
234:     for result in results:
235:         formatted = {
236:             'id': result.get('id'),
237:             'file_name': result.get('file_name'),
238:             'local_path': result.get('local_path'),
239:             'content_summary': result.get('content_summary'),
240:             'content_tags': result.get('content_tags', []),
241:             'duration_seconds': result.get('duration_seconds'),
242:             'camera_make': result.get('camera_make'),
243:             'camera_model': result.get('camera_model'),
244:             'content_category': result.get('content_category'),
245:             'processed_at': result.get('processed_at')
246:         }
247:         # Add search-specific fields
248:         if search_type == "semantic" and show_scores:
249:             formatted.update({
250:                 'summary_similarity': result.get('summary_similarity'),
251:                 'keyword_similarity': result.get('keyword_similarity'),
252:                 'combined_similarity': result.get('combined_similarity')
253:             })
254:         elif search_type == "hybrid" and show_scores:
255:             formatted.update({
256:                 'similarity_score': result.get('similarity_score'),
257:                 'search_rank': result.get('search_rank'),
258:                 'match_type': result.get('match_type')
259:             })
260:         elif search_type == "fulltext" and show_scores:
261:             formatted.update({
262:                 'fts_rank': result.get('fts_rank')
263:             })
264:         elif search_type == "transcripts":
265:             formatted.update({
266:                 'clip_id': result.get('clip_id'),
267:                 'full_text': result.get('full_text'),
268:                 'transcript_preview': result.get('transcript_preview'),
269:                 'fts_rank': result.get('fts_rank') if show_scores else None
270:             })
271:         elif search_type == "similar" and show_scores:
272:             formatted.update({
273:                 'similarity_score': result.get('similarity_score')
274:             })
275:         formatted_results.append(formatted)
276:     return formatted_results
277: def format_duration(seconds: float) -> str:
278:     """Format duration in seconds to human-readable format."""
279:     if not seconds:
280:         return "Unknown"
281:     hours = int(seconds // 3600)
282:     minutes = int((seconds % 3600) // 60)
283:     secs = int(seconds % 60)
284:     if hours > 0:
285:         return f"{hours}h {minutes}m {secs}s"
286:     elif minutes > 0:
287:         return f"{minutes}m {secs}s"
288:     else:
289:         return f"{secs}s"
290: def format_file_size(bytes_size: int) -> str:
291:     """Format file size in bytes to human-readable format."""
292:     if not bytes_size:
293:         return "Unknown"
294:     for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
295:         if bytes_size < 1024.0:
296:             return f"{bytes_size:.1f} {unit}"
297:         bytes_size /= 1024.0
298:     return f"{bytes_size:.1f} PB"
</file>

<file path="video_ingest_tool/supabase_config.py">
 1: """
 2: Supabase configuration and client management for AI Ingesting Tool.
 3: """
 4: import os
 5: from typing import Optional
 6: from supabase import create_client, Client
 7: from supabase.client import ClientOptions
 8: import structlog
 9: from dotenv import load_dotenv
10: # Load environment variables from .env file
11: load_dotenv()
12: logger = structlog.get_logger(__name__)
13: # Load environment variables
14: SUPABASE_URL = os.getenv("SUPABASE_URL")
15: SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY")
16: SUPABASE_SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
17: def get_supabase_client(use_service_role: bool = False) -> Client:
18:     """
19:     Get Supabase client instance.
20:     Args:
21:         use_service_role: Use service role key instead of anon key
22:     Returns:
23:         Configured Supabase client
24:     Raises:
25:         ValueError: If required environment variables are not set
26:     """
27:     if not SUPABASE_URL:
28:         raise ValueError("SUPABASE_URL environment variable not set")
29:     key = SUPABASE_SERVICE_ROLE_KEY if use_service_role else SUPABASE_ANON_KEY
30:     if not key:
31:         key_type = "SUPABASE_SERVICE_ROLE_KEY" if use_service_role else "SUPABASE_ANON_KEY"
32:         raise ValueError(f"{key_type} environment variable not set")
33:     # Configure client options (based on official docs)
34:     options = ClientOptions(
35:         auto_refresh_token=True,
36:         persist_session=True
37:     )
38:     client = create_client(SUPABASE_URL, key, options)
39:     return client
40: def verify_connection() -> bool:
41:     """
42:     Verify connection to Supabase.
43:     Returns:
44:         True if connection successful
45:     """
46:     try:
47:         client = get_supabase_client()
48:         # Test connection with a simple query to check if we can reach the database
49:         result = client.table('user_profiles').select('count').execute()
50:         logger.info("Supabase connection verified")
51:         return True
52:     except Exception as e:
53:         logger.error(f"Supabase connection failed: {str(e)}")
54:         return False
55: def get_database_status() -> dict:
56:     """
57:     Get database status and basic information.
58:     Returns:
59:         Dictionary with database status information
60:     """
61:     try:
62:         client = get_supabase_client()
63:         # Check if core tables exist
64:         tables_to_check = ['user_profiles', 'clips', 'segments', 'analysis', 'vectors', 'transcripts']
65:         table_status = {}
66:         for table in tables_to_check:
67:             try:
68:                 result = client.table(table).select('count').execute()
69:                 table_status[table] = 'exists'
70:             except Exception:
71:                 table_status[table] = 'missing'
72:         return {
73:             'connection': 'success',
74:             'tables': table_status,
75:             'url': SUPABASE_URL
76:         }
77:     except Exception as e:
78:         return {
79:             'connection': 'failed',
80:             'error': str(e),
81:             'url': SUPABASE_URL
82:         }
</file>

<file path="video_ingest_tool/utils.py">
  1: """
  2: Utility functions for the video ingest tool.
  3: Contains helper functions for checksum calculation, parsing dates, etc.
  4: """
  5: import os
  6: import math
  7: import hashlib
  8: import datetime
  9: from typing import Optional, Union
 10: from dateutil import parser as dateutil_parser
 11: def calculate_checksum(file_path: str, block_size: int = 65536) -> str:
 12:     """
 13:     Calculate MD5 checksum of a file.
 14:     Args:
 15:         file_path: Path to the file
 16:         block_size: Block size for reading the file
 17:     Returns:
 18:         str: Hex digest of MD5 checksum
 19:     """
 20:     hash_md5 = hashlib.md5()
 21:     with open(file_path, "rb") as f:
 22:         for chunk in iter(lambda: f.read(block_size), b""):
 23:             hash_md5.update(chunk)
 24:     return hash_md5.hexdigest()
 25: def parse_datetime_string(date_str: Optional[str]) -> Optional[datetime.datetime]:
 26:     """Parse a date string into a datetime object, handling various formats and UTC."""
 27:     if not date_str:
 28:         return None
 29:     try:
 30:         # Clean up string first
 31:         cleaned_date_str = date_str
 32:         # Handle common date formats with timezone info
 33:         if 'UTC' in cleaned_date_str:
 34:             # Try to handle formats like "2026-04-18 04:54:32 UTC" or "2026-04-18 04-54-32 UTC"
 35:             cleaned_date_str = cleaned_date_str.replace(' UTC', 'Z')
 36:             # Replace all hyphens in time part with colons
 37:             if ' ' in cleaned_date_str:
 38:                 date_part, time_part = cleaned_date_str.split(' ', 1)
 39:                 time_part = time_part.replace('-', ':')
 40:                 cleaned_date_str = f"{date_part} {time_part}"
 41:         # Handle formats with colons in date part (2022:01:01 12:30:00)
 42:         elif cleaned_date_str.count(':') > 2:
 43:             parts = cleaned_date_str.split(' ', 1)
 44:             date_part = parts[0].replace(':', '-')
 45:             if len(parts) > 1:
 46:                 time_part = parts[1]
 47:                 cleaned_date_str = f"{date_part} {time_part}"
 48:             else:
 49:                 cleaned_date_str = date_part
 50:         # Parse the cleaned string
 51:         dt = dateutil_parser.parse(cleaned_date_str)
 52:         return dt
 53:     except (ValueError, TypeError):
 54:         return None
 55: def calculate_aspect_ratio_str(width: Optional[int], height: Optional[int]) -> Optional[str]:
 56:     """Calculate aspect ratio as a string (e.g., '16:9')."""
 57:     if not width or not height or width <= 0 or height <= 0:
 58:         return None
 59:     common_divisor = math.gcd(width, height)
 60:     return f"{width // common_divisor}:{height // common_divisor}"
 61: def map_exposure_mode(mode_val: Optional[Union[str, int]]) -> Optional[str]:
 62:     """Map EXIF ExposureMode numerical value to a human-readable string."""
 63:     if mode_val is None:
 64:         return None
 65:     try:
 66:         val = int(str(mode_val).strip())
 67:         if val == 0:
 68:             return "AUTO_EXPOSURE"
 69:         elif val == 1:
 70:             return "MANUAL_EXPOSURE"
 71:         elif val == 2:
 72:             return "AUTO_BRACKET"
 73:         else:
 74:             return str(mode_val) # Return original value if unknown
 75:     except ValueError:
 76:         return str(mode_val) # Return original string if not an int
 77: def map_white_balance(wb_val: Optional[Union[str, int]]) -> Optional[str]:
 78:     """Map EXIF WhiteBalance numerical value to a human-readable string."""
 79:     if wb_val is None:
 80:         return None
 81:     try:
 82:         val = int(str(wb_val).strip())
 83:         if val == 0:
 84:             return "AUTO_WHITE_BALANCE"
 85:         elif val == 1:
 86:             return "MANUAL_WHITE_BALANCE"
 87:         else:
 88:             # Many more values exist for white balance, but these are the most common.
 89:             return str(wb_val) # Return original value if unknown or unmapped
 90:     except ValueError:
 91:         return str(wb_val) # Return original string if not an int
 92: def is_video_file(file_path: str, has_polyfile: bool = False) -> bool:
 93:     """
 94:     Check if a file is a video file based on MIME type.
 95:     Args:
 96:         file_path: Path to the file to check
 97:         has_polyfile: Whether polyfile module is available
 98:     Returns:
 99:         bool: True if the file is a video, False otherwise
100:     """
101:     import mimetypes
102:     try:
103:         # Use polyfile for file type detection if available
104:         if has_polyfile:
105:             from polyfile.magic import MagicMatcher
106:             with open(file_path, 'rb') as f:
107:                 # Read a small chunk, as PolyFile can work with partial data
108:                 file_bytes = f.read(2048) # Read first 2KB for type detection
109:                 for match in MagicMatcher.DEFAULT_INSTANCE.match(file_bytes):
110:                     for mime_type in match.mimetypes:
111:                         if mime_type.startswith('video/'):
112:                             return True
113:         # Fallback to mimetypes if polyfile is not available or doesn't find a video
114:         mime_type, _ = mimetypes.guess_type(file_path)
115:         if mime_type and mime_type.startswith('video/'):
116:             return True
117:         # Additional fallback: check extensions
118:         file_ext = os.path.splitext(file_path.lower())[1]
119:         video_extensions = ['.mp4', '.mov', '.avi', '.wmv', '.flv', '.mkv', '.webm', '.m4v', '.mpg', '.mpeg']
120:         if file_ext in video_extensions:
121:             return True
122:         return False
123:     except Exception:
124:         # Ultimate fallback to mimetypes if everything else errors out
125:         try:
126:             mime_type, _ = mimetypes.guess_type(file_path)
127:             return bool(mime_type and mime_type.startswith('video/'))
128:         except:
129:             return False
130: def categorize_focal_length(focal_length: Optional[Union[str, int, float]], ranges: dict) -> Optional[str]:
131:     """
132:     Categorize a focal length value into a standard category.
133:     Args:
134:         focal_length: The focal length value (can be string, int, or float)
135:         ranges: Dictionary of focal length ranges by category
136:     Returns:
137:         str: The focal length category (ULTRA-WIDE, WIDE, MEDIUM, LONG-LENS, TELEPHOTO) or None if not determinable
138:     """
139:     if focal_length is None:
140:         return None
141:     try:
142:         # Convert focal length to float if it's a string or other type
143:         if isinstance(focal_length, str):
144:             # Clean up string - remove 'mm' suffix and any spaces
145:             focal_length = focal_length.lower().replace('mm', '').strip()
146:         focal_mm = float(focal_length)
147:         # Determine category based on range
148:         for category, (min_val, max_val) in ranges.items():
149:             if min_val <= focal_mm <= max_val:
150:                 return category
151:         # Handle extreme values
152:         if focal_mm < 8:
153:             return "ULTRA-WIDE"
154:         elif focal_mm > 800:
155:             return "TELEPHOTO"
156:         return None
157:     except (ValueError, TypeError):
158:         return None
</file>

<file path="video_ingest_tool/video_processor.py">
  1: """
  2: Video processing module for compressing and analyzing videos using Gemini Flash 2.5.
  3: """
  4: import os
  5: import sys
  6: import json
  7: import logging
  8: import subprocess
  9: from typing import Dict, Any, Optional
 10: from pathlib import Path
 11: from dotenv import load_dotenv
 12: from google import genai
 13: from google.genai import types
 14: # Import the Config class directly
 15: from video_ingest_tool.config import Config
 16: # Load environment variables from .env file
 17: # Look for .env file in the project root directory
 18: project_root = Path(__file__).parent.parent
 19: env_path = project_root / '.env'
 20: load_dotenv(dotenv_path=env_path)
 21: # Default compression configuration - single source of truth
 22: DEFAULT_COMPRESSION_CONFIG = {
 23:     'max_dimension': 1280,  # Scale longest dimension to this size
 24:     'fps': 5,
 25:     'video_bitrate': '1000k',
 26:     'audio_bitrate': '32k',
 27:     'audio_channels': 1,
 28:     'use_hardware_accel': True,
 29:     'codec_priority': ['hevc_videotoolbox', 'h264_videotoolbox', 'libx265', 'libx264'],
 30:     'crf_value': '25',
 31: }
 32: class VideoCompressor:
 33:     """Handles video compression using ffmpeg with hardware acceleration when available."""
 34:     def __init__(self, config: Optional[Dict[str, Any]] = None):
 35:         self.config = {
 36:             **DEFAULT_COMPRESSION_CONFIG,
 37:             **(config or {})
 38:         }
 39:         self.logger = logging.getLogger(self.__class__.__name__)
 40:     def _check_videotoolbox_availability(self) -> Dict[str, bool]:
 41:         """
 42:         Check if VideoToolbox hardware acceleration is available on macOS.
 43:         Returns:
 44:             Dict[str, bool]: Dictionary with availability of h264 and hevc encoders
 45:         """
 46:         result = {
 47:             'h264_videotoolbox': False,
 48:             'hevc_videotoolbox': False
 49:         }
 50:         if sys.platform != 'darwin':
 51:             return result
 52:         try:
 53:             # Check if VideoToolbox encoders are available in ffmpeg
 54:             proc = subprocess.run(
 55:                 ["ffmpeg", "-hide_banner", "-encoders"], 
 56:                 check=True, capture_output=True, text=True
 57:             )
 58:             result['h264_videotoolbox'] = 'h264_videotoolbox' in proc.stdout
 59:             result['hevc_videotoolbox'] = 'hevc_videotoolbox' in proc.stdout
 60:             if result['h264_videotoolbox']:
 61:                 self.logger.info("H.264 VideoToolbox encoder is available")
 62:             if result['hevc_videotoolbox']:
 63:                 self.logger.info("HEVC VideoToolbox encoder is available")
 64:             return result
 65:         except Exception as e:
 66:             self.logger.warning(f"Error checking for VideoToolbox: {str(e)}")
 67:             return result
 68:     def _select_best_codec(self) -> str:
 69:         """
 70:         Select the best available codec based on priorities and system capabilities.
 71:         Returns:
 72:             str: The best available codec to use
 73:         """
 74:         available_codecs = {
 75:             'libx264': True,  # Assume libx264 is always available
 76:             'libx265': False,  # Will be checked below
 77:             'h264_videotoolbox': False,
 78:             'hevc_videotoolbox': False
 79:         }
 80:         # Check if h265/HEVC is available
 81:         try:
 82:             hevc_check = subprocess.run(
 83:                 ["ffmpeg", "-hide_banner", "-encoders"], 
 84:                 check=True, capture_output=True, text=True
 85:             )
 86:             available_codecs['libx265'] = 'libx265' in hevc_check.stdout
 87:         except Exception:
 88:             pass
 89:         # If hardware acceleration is enabled and we're on macOS, check for VideoToolbox
 90:         if self.config['use_hardware_accel'] and sys.platform == 'darwin':
 91:             vt_availability = self._check_videotoolbox_availability()
 92:             available_codecs['h264_videotoolbox'] = vt_availability['h264_videotoolbox']
 93:             available_codecs['hevc_videotoolbox'] = vt_availability['hevc_videotoolbox']
 94:         # Select the best codec based on priority list
 95:         for codec in self.config['codec_priority']:
 96:             if codec in available_codecs and available_codecs[codec]:
 97:                 self.logger.info(f"Selected codec: {codec}")
 98:                 return codec
 99:         # Default to libx264 as fallback
100:         self.logger.info("Falling back to libx264 codec")
101:         return 'libx264'
102:     def _get_video_resolution(self, input_path: str) -> tuple:
103:         """
104:         Get the resolution of the input video.
105:         Args:
106:             input_path: Path to input video file
107:         Returns:
108:             tuple: (width, height) or (None, None) if detection fails
109:         """
110:         try:
111:             # Use ffprobe to get video resolution
112:             cmd = [
113:                 "ffprobe", "-v", "quiet", "-print_format", "json", 
114:                 "-show_streams", "-select_streams", "v:0", input_path
115:             ]
116:             result = subprocess.run(cmd, capture_output=True, text=True, check=True)
117:             import json
118:             probe_data = json.loads(result.stdout)
119:             if probe_data.get('streams'):
120:                 video_stream = probe_data['streams'][0]
121:                 width = video_stream.get('width')
122:                 height = video_stream.get('height')
123:                 if width and height:
124:                     self.logger.info(f"Detected video resolution: {width}x{height}")
125:                     return (int(width), int(height))
126:             self.logger.warning("Could not detect video resolution")
127:             return (None, None)
128:         except Exception as e:
129:             self.logger.warning(f"Failed to detect video resolution: {str(e)}")
130:             return (None, None)
131:     def compress(self, input_path: str, output_dir: str = None) -> str:
132:         """
133:         Compress video using ffmpeg with the best available codec.
134:         Args:
135:             input_path: Path to input video file
136:             output_dir: Directory to save compressed file (defaults to compressed/ next to input)
137:         Returns:
138:             str: Path to compressed output video
139:         Raises:
140:             RuntimeError: If compression fails
141:         """
142:         try:
143:             # Create output directory - use provided directory or create compressed/ next to input
144:             if output_dir:
145:                 compressed_dir = os.path.join(output_dir, "compressed")
146:             else:
147:                 input_dir = os.path.dirname(input_path)
148:                 compressed_dir = os.path.join(input_dir, "compressed")
149:             os.makedirs(compressed_dir, exist_ok=True)
150:             # Use just the filename for the output, not the full path
151:             input_basename = os.path.basename(input_path)
152:             output_basename = f"{os.path.splitext(input_basename)[0]}_compressed.mp4"
153:             output_path = os.path.join(compressed_dir, output_basename)
154:             self.logger.info(f"Compressing {input_path} to {output_path}")
155:             # Check if input file exists
156:             if not os.path.exists(input_path):
157:                 raise FileNotFoundError(f"Input file not found: {input_path}")
158:             # Check for ffmpeg
159:             try:
160:                 subprocess.run(["which", "ffmpeg"], check=True, capture_output=True)
161:             except subprocess.CalledProcessError:
162:                 raise RuntimeError("ffmpeg not found in PATH. Please install ffmpeg.")
163:             # Detect input video resolution
164:             input_width, input_height = self._get_video_resolution(input_path)
165:             # Determine if we need to scale down
166:             needs_scaling = False
167:             scale_filter = None
168:             max_dimension = self.config['max_dimension']
169:             if input_width and input_height:
170:                 # Find the longest dimension
171:                 longest_dimension = max(input_width, input_height)
172:                 # Check if longest dimension exceeds our target
173:                 if longest_dimension > max_dimension:
174:                     needs_scaling = True
175:                     # Calculate scaling to fit longest dimension
176:                     scale_factor = max_dimension / longest_dimension
177:                     target_width = int(input_width * scale_factor)
178:                     target_height = int(input_height * scale_factor)
179:                     # Make sure dimensions are even (required for many codecs)
180:                     target_width = target_width if target_width % 2 == 0 else target_width - 1
181:                     target_height = target_height if target_height % 2 == 0 else target_height - 1
182:                     scale_filter = f"scale={target_width}:{target_height}"
183:                     self.logger.info(f"Scaling down from {input_width}x{input_height} to {target_width}x{target_height} (longest dimension: {longest_dimension} → {max_dimension})")
184:                 else:
185:                     self.logger.info(f"Resolution {input_width}x{input_height} fits within {max_dimension}px, compressing without scaling")
186:             else:
187:                 # If we can't detect resolution, use default scaling as fallback
188:                 needs_scaling = True
189:                 scale_filter = f"scale={max_dimension}:{max_dimension}"
190:                 self.logger.warning(f"Could not detect resolution, using default scaling to {max_dimension}px")
191:             # Select the best available codec
192:             video_codec = self._select_best_codec()
193:             # Base ffmpeg command
194:             cmd = [
195:                 "ffmpeg", "-i", input_path,
196:                 "-c:v", video_codec
197:             ]
198:             # Add codec-specific parameters
199:             if video_codec == 'libx264' or video_codec == 'libx265':
200:                 # For software encoding, use CRF (Constant Rate Factor) for quality-based encoding
201:                 cmd.extend(["-crf", str(self.config['crf_value'])])  # Use configurable CRF
202:             elif 'videotoolbox' in video_codec:
203:                 # For hardware encoding, use bitrate-based encoding
204:                 cmd.extend(["-b:v", self.config['video_bitrate']])
205:                 # Add specific VideoToolbox parameters for better quality
206:                 cmd.extend(["-allow_sw", "1"])  # Allow software encoding as fallback
207:                 # Add ProRes options for better quality with VideoToolbox
208:                 if video_codec == 'hevc_videotoolbox':
209:                     cmd.extend(["-profile:v", "main"])
210:             # Add scaling filter if needed
211:             if needs_scaling:
212:                 cmd.extend(["-vf", scale_filter])
213:             # Add other common parameters
214:             cmd.extend([
215:                 "-r", str(self.config['fps']),     # Frame rate
216:                 "-c:a", "aac", "-b:a", self.config['audio_bitrate'],  # Audio codec and bitrate
217:                 "-ac", str(self.config['audio_channels']),  # Audio channels
218:                 "-y",                              # Overwrite output
219:                 output_path
220:             ])
221:             self.logger.info(f"Running ffmpeg command: {' '.join(cmd)}")
222:             # Execute ffmpeg command
223:             result = subprocess.run(cmd, check=True, capture_output=True, text=True)
224:             # Check if output file was created and has a reasonable size
225:             if os.path.exists(output_path):
226:                 input_size = os.path.getsize(input_path)
227:                 output_size = os.path.getsize(output_path)
228:                 compression_ratio = input_size / output_size if output_size > 0 else 0
229:                 self.logger.info(f"Compression successful!")
230:                 self.logger.info(f"Input size: {input_size/1024/1024:.2f} MB")
231:                 self.logger.info(f"Output size: {output_size/1024/1024:.2f} MB")
232:                 self.logger.info(f"Compression ratio: {compression_ratio:.2f}x")
233:             else:
234:                 self.logger.warning(f"Output file not found: {output_path}")
235:             return output_path
236:         except subprocess.CalledProcessError as e:
237:             self.logger.error(f"ffmpeg error: {e.stderr}")
238:             raise RuntimeError(f"Compression failed: ffmpeg error: {e.stderr}")
239:         except Exception as e:
240:             self.logger.error(f"Compression failed: {str(e)}", exc_info=True)
241:             raise RuntimeError(f"Compression failed: {str(e)}")
242: class VideoAnalyzer:
243:     """Handles comprehensive video analysis using Gemini Flash 2.5."""
244:     def __init__(self, api_key: str, fps: int = 1):
245:         self.client = genai.Client(api_key=api_key)
246:         self.api_key = api_key
247:         self.fps = fps
248:         self.logger = logging.getLogger(self.__class__.__name__)
249:     def _get_comprehensive_analysis_schema(self) -> Dict[str, Any]:
250:         """
251:         Define comprehensive analysis schema for video processing.
252:         Returns:
253:             Dict[str, Any]: JSON schema for structured AI analysis
254:         """
255:         return {
256:             "type": "OBJECT",
257:             "properties": {
258:                 "summary": {
259:                     "type": "OBJECT",
260:                     "properties": {
261:                         "overall": {"type": "STRING"},
262:                         "key_activities": {
263:                             "type": "ARRAY",
264:                             "items": {"type": "STRING"},
265:                             "description": "List of main activities or actions occurring in the video"
266:                         },
267:                         "content_category": {
268:                             "type": "STRING",
269:                             "description": "Primary category of content (e.g., Interview, Tutorial, Event, Nature, etc.)"
270:                         }
271:                     },
272:                     "required": ["overall", "key_activities", "content_category"]
273:                 },
274:                 "visual_analysis": {
275:                     "type": "OBJECT",
276:                     "properties": {
277:                         "shot_types": {
278:                             "type": "ARRAY",
279:                             "items": {
280:                                 "type": "OBJECT",
281:                                 "properties": {
282:                                     "timestamp": {"type": "STRING"},
283:                                     "duration_seconds": {"type": "NUMBER"},
284:                                     "shot_type": {
285:                                         "type": "STRING",
286:                                         "enum": [
287:                                             "Drone Shot", "Scenic Wide / Exterior", "Interview Setup",
288:                                             "Talking Head", "Close-Up", "Extreme Close-Up / Detail Shot",
289:                                             "Wide Shot (General Context)", "POV (Point of View) Shot",
290:                                             "Tracking / Follow Shot", "Static / Locked-Down Shot",
291:                                             "Handheld Shot", "Slow Motion Visuals", "Time-Lapse Visuals",
292:                                             "Screen Recording / Screencast", "Graphic / Animation",
293:                                             "Dutch Angle / Canted Shot", "Rack Focus",
294:                                             "Over-the-Shoulder Shot (OTS)", "Low Angle Shot",
295:                                             "High Angle Shot", "Other"
296:                                         ]
297:                                     },
298:                                     "description": {"type": "STRING"},
299:                                     "confidence": {"type": "NUMBER", "minimum": 0, "maximum": 1}
300:                                 },
301:                                 "required": ["timestamp", "shot_type", "description"]
302:                             }
303:                         },
304:                         "technical_quality": {
305:                             "type": "OBJECT",
306:                             "properties": {
307:                                 "overall_focus_quality": {
308:                                     "type": "STRING",
309:                                     "enum": ["Excellent", "Good", "Fair", "Poor", "Very Poor"]
310:                                 },
311:                                 "stability_assessment": {
312:                                     "type": "STRING",
313:                                     "enum": ["Very Stable", "Stable", "Moderately Shaky", "Very Shaky", "Unusable"]
314:                                 },
315:                                 "detected_artifacts": {
316:                                     "type": "ARRAY",
317:                                     "items": {
318:                                         "type": "OBJECT",
319:                                         "properties": {
320:                                             "type": {
321:                                                 "type": "STRING",
322:                                                 "enum": ["Blockiness", "Banding", "Dead Pixels", "Sensor Dust", "Compression Artifacts", "Motion Blur", "Other"]
323:                                             },
324:                                             "severity": {
325:                                                 "type": "STRING",
326:                                                 "enum": ["Minor", "Moderate", "Severe"]
327:                                             },
328:                                             "description": {"type": "STRING"}
329:                                         },
330:                                         "required": ["type", "severity", "description"]
331:                                     }
332:                                 },
333:                                 "usability_rating": {
334:                                     "type": "STRING",
335:                                     "enum": ["Excellent", "Good", "Acceptable", "Poor", "Unusable"]
336:                                 }
337:                             },
338:                             "required": ["overall_focus_quality", "stability_assessment", "usability_rating"]
339:                         },
340:                         "text_and_graphics": {
341:                             "type": "OBJECT",
342:                             "properties": {
343:                                 "detected_text": {
344:                                     "type": "ARRAY",
345:                                     "items": {
346:                                         "type": "OBJECT",
347:                                         "properties": {
348:                                             "timestamp": {"type": "STRING"},
349:                                             "text_content": {"type": "STRING"},
350:                                             "text_type": {
351:                                                 "type": "STRING",
352:                                                 "enum": ["Title/Heading", "Subtitle", "Caption", "UI Text", "Signage", "Other", "Unclear/Blurry"]
353:                                             },
354:                                             "readability": {
355:                                                 "type": "STRING",
356:                                                 "enum": ["Clear", "Partially Clear", "Blurry", "Unreadable"]
357:                                             }
358:                                         },
359:                                         "required": ["timestamp", "text_type", "readability"]
360:                                     }
361:                                 },
362:                                 "detected_logos_icons": {
363:                                     "type": "ARRAY",
364:                                     "items": {
365:                                         "type": "OBJECT",
366:                                         "properties": {
367:                                             "timestamp": {"type": "STRING"},
368:                                             "description": {
369:                                                 "type": "STRING",
370:                                                 "description": "Generic description without brand identification (e.g., 'Red circular icon', 'Blue rectangular logo')"
371:                                             },
372:                                             "element_type": {
373:                                                 "type": "STRING",
374:                                                 "enum": ["Logo", "Icon", "Graphic Element", "UI Element"]
375:                                             },
376:                                             "size": {
377:                                                 "type": "STRING",
378:                                                 "enum": ["Small", "Medium", "Large", "Prominent"]
379:                                             }
380:                                         },
381:                                         "required": ["timestamp", "description", "element_type"]
382:                                     }
383:                                 }
384:                             }
385:                         },
386:                         "keyframe_analysis": {
387:                             "type": "OBJECT",
388:                             "properties": {
389:                                 "recommended_keyframes": {
390:                                     "type": "ARRAY",
391:                                     "items": {
392:                                         "type": "OBJECT",
393:                                         "properties": {
394:                                             "timestamp": {"type": "STRING"},
395:                                             "reason": {
396:                                                 "type": "STRING",
397:                                                 "description": "Why this frame is recommended (e.g., 'Clear face visible', 'Good composition', 'Key action moment')"
398:                                             },
399:                                             "visual_quality": {
400:                                                 "type": "STRING",
401:                                                 "enum": ["Excellent", "Good", "Fair", "Poor"]
402:                                             }
403:                                         },
404:                                         "required": ["timestamp", "reason", "visual_quality"]
405:                                     }
406:                                 }
407:                             }
408:                         }
409:                     },
410:                     "required": ["shot_types", "technical_quality"]
411:                 },
412:                 "audio_analysis": {
413:                     "type": "OBJECT",
414:                     "properties": {
415:                         "transcript": {
416:                             "type": "OBJECT",
417:                             "properties": {
418:                                 "full_text": {"type": "STRING"},
419:                                 "segments": {
420:                                     "type": "ARRAY",
421:                                     "items": {
422:                                         "type": "OBJECT",
423:                                         "properties": {
424:                                             "timestamp": {"type": "STRING"},
425:                                             "speaker": {"type": "STRING"},
426:                                             "text": {"type": "STRING"},
427:                                             "confidence": {"type": "NUMBER", "minimum": 0, "maximum": 1}
428:                                         },
429:                                         "required": ["timestamp", "text"]
430:                                     }
431:                                 }
432:                             }
433:                         },
434:                         "speaker_analysis": {
435:                             "type": "OBJECT",
436:                             "properties": {
437:                                 "speaker_count": {"type": "INTEGER"},
438:                                 "speakers": {
439:                                     "type": "ARRAY",
440:                                     "items": {
441:                                         "type": "OBJECT",
442:                                         "properties": {
443:                                             "speaker_id": {"type": "STRING"},
444:                                             "speaking_time_seconds": {"type": "NUMBER"},
445:                                             "segments_count": {"type": "INTEGER"}
446:                                         },
447:                                         "required": ["speaker_id", "speaking_time_seconds"]
448:                                     }
449:                                 }
450:                             },
451:                             "required": ["speaker_count"]
452:                         },
453:                         "sound_events": {
454:                             "type": "ARRAY",
455:                             "items": {
456:                                 "type": "OBJECT",
457:                                 "properties": {
458:                                     "timestamp": {"type": "STRING"},
459:                                     "event_type": {
460:                                         "type": "STRING",
461:                                         "enum": [
462:                                             "Applause", "Laughter", "Music", "Door Slam", "Phone Ringing",
463:                                             "Footsteps", "Crowd Noise", "Vehicle Sounds", "Nature Sounds",
464:                                             "Mechanical Sounds", "Electronic Beeps", "Wind", "Rain",
465:                                             "Background Music", "Other"
466:                                         ]
467:                                     },
468:                                     "description": {"type": "STRING"},
469:                                     "duration_seconds": {"type": "NUMBER"},
470:                                     "prominence": {
471:                                         "type": "STRING",
472:                                         "enum": ["Background", "Moderate", "Prominent", "Dominant"]
473:                                     }
474:                                 },
475:                                 "required": ["timestamp", "event_type", "description"]
476:                             }
477:                         },
478:                         "audio_quality": {
479:                             "type": "OBJECT",
480:                             "properties": {
481:                                 "clarity": {
482:                                     "type": "STRING",
483:                                     "enum": ["Excellent", "Good", "Fair", "Poor", "Very Poor"]
484:                                 },
485:                                 "background_noise_level": {
486:                                     "type": "STRING",
487:                                     "enum": ["Minimal", "Low", "Moderate", "High", "Excessive"]
488:                                 },
489:                                 "dialogue_intelligibility": {
490:                                     "type": "STRING",
491:                                     "enum": ["Very Clear", "Clear", "Mostly Clear", "Difficult", "Unintelligible"]
492:                                 }
493:                             }
494:                         }
495:                     }
496:                 },
497:                 "content_analysis": {
498:                     "type": "OBJECT",
499:                     "properties": {
500:                         "entities": {
501:                             "type": "OBJECT",
502:                             "properties": {
503:                                 "people_count": {"type": "INTEGER"},
504:                                 "people_details": {
505:                                     "type": "ARRAY",
506:                                     "items": {
507:                                         "type": "OBJECT",
508:                                         "properties": {
509:                                             "description": {"type": "STRING"},
510:                                             "role": {"type": "STRING", "description": "Role in the video (e.g., 'Speaker', 'Interviewer', 'Subject', 'Background')"},
511:                                             "visibility_duration": {"type": "STRING", "description": "How long they appear (e.g., 'Throughout', 'Brief', 'Intermittent')"}
512:                                         },
513:                                         "required": ["description"]
514:                                     }
515:                                 },
516:                                 "locations": {
517:                                     "type": "ARRAY",
518:                                     "items": {
519:                                         "type": "OBJECT",
520:                                         "properties": {
521:                                             "name": {"type": "STRING"},
522:                                             "type": {
523:                                                 "type": "STRING",
524:                                                 "enum": ["Indoor", "Outdoor", "Studio", "Office", "Home", "Public Space", "Natural Setting", "Other"]
525:                                             },
526:                                             "description": {"type": "STRING"}
527:                                         },
528:                                         "required": ["name", "type"]
529:                                     }
530:                                 },
531:                                 "objects_of_interest": {
532:                                     "type": "ARRAY",
533:                                     "items": {
534:                                         "type": "OBJECT",
535:                                         "properties": {
536:                                             "object": {"type": "STRING"},
537:                                             "significance": {"type": "STRING", "description": "Why this object is notable"},
538:                                             "timestamp": {"type": "STRING"}
539:                                         },
540:                                         "required": ["object", "significance"]
541:                                     }
542:                                 }
543:                             },
544:                             "required": ["people_count"]
545:                         },
546:                         "activity_summary": {
547:                             "type": "ARRAY",
548:                             "items": {
549:                                 "type": "OBJECT",
550:                                 "properties": {
551:                                     "activity": {"type": "STRING"},
552:                                     "timestamp": {"type": "STRING"},
553:                                     "duration": {"type": "STRING"},
554:                                     "importance": {
555:                                         "type": "STRING",
556:                                         "enum": ["High", "Medium", "Low"]
557:                                     }
558:                                 },
559:                                 "required": ["activity", "timestamp", "importance"]
560:                             }
561:                         },
562:                         "content_warnings": {
563:                             "type": "ARRAY",
564:                             "items": {
565:                                 "type": "OBJECT",
566:                                 "properties": {
567:                                     "type": {
568:                                         "type": "STRING",
569:                                         "enum": ["Violence", "Strong Language", "Adult Content", "Flashing Lights", "Loud Sounds", "Other"]
570:                                     },
571:                                     "description": {"type": "STRING"},
572:                                     "timestamp": {"type": "STRING"}
573:                                 },
574:                                 "required": ["type", "description"]
575:                             }
576:                         }
577:                     }
578:                 }
579:             },
580:             "required": ["summary", "visual_analysis", "audio_analysis", "content_analysis"]
581:         }
582:     def _create_analysis_prompt(self) -> str:
583:         """
584:         Create comprehensive analysis prompt for Gemini Flash 2.5.
585:         Returns:
586:             str: Detailed prompt for video analysis
587:         """
588:         return """
589:         Analyze this video comprehensively for video editing purposes. Provide detailed analysis in the following areas:
590:         1. SUMMARY & OVERVIEW:
591:         - Overall content summary
592:         - Key activities and actions occurring
593:         - Primary content category
594:         2. VISUAL ANALYSIS:
595:         - Shot type classification for each significant segment using these categories:
596:           * Drone Shot, Scenic Wide/Exterior, Interview Setup, Talking Head
597:           * Close-Up, Extreme Close-Up/Detail Shot, Wide Shot, POV Shot
598:           * Tracking/Follow Shot, Static/Locked-Down Shot, Handheld Shot
599:           * Slow Motion, Time-Lapse, Screen Recording, Graphic/Animation
600:           * Dutch Angle, Rack Focus, Over-the-Shoulder, Low/High Angle
601:         - Technical quality assessment (focus, stability, artifacts)
602:         - Text and logo/icon detection with readability assessment
603:         - Recommended keyframes for thumbnails based on visual quality and content
604:         3. AUDIO ANALYSIS:
605:         - Full speech transcription with speaker identification
606:         - Speaker diarization (distinguish different speakers)
607:         - Non-speech sound event detection (applause, music, ambient sounds, etc.)
608:         - Audio quality assessment
609:         4. CONTENT ANALYSIS:
610:         - People count and role identification
611:         - Location and setting identification
612:         - Significant objects or elements
613:         - Activity timeline with importance ratings
614:         - Content warnings if applicable
615:         Focus on information that would be valuable for video editors to quickly understand and organize footage.
616:         """
617:     def analyze_video(self, video_path: str) -> Dict[str, Any]:
618:         """
619:         Perform comprehensive video analysis using Gemini Flash 2.5.
620:         Args:
621:             video_path: Path to video file to analyze
622:         Returns:
623:             Dict[str, Any]: Structured analysis results
624:         """
625:         try:
626:             self.logger.info(f"Starting comprehensive AI analysis of: {video_path}")
627:             # Load video bytes
628:             with open(video_path, 'rb') as f:
629:                 video_bytes = f.read()
630:             # Create video part for Gemini API
631:             video_blob = types.Blob(
632:                 data=video_bytes,
633:                 mime_type="video/mp4"
634:             )
635:             video_part = types.Part(
636:                 inline_data=video_blob,
637:                 video_metadata=types.VideoMetadata(
638:                     fps=self.fps  # Use actual FPS setting from compression config
639:                 )
640:             )
641:             # Get comprehensive analysis schema
642:             schema = self._get_comprehensive_analysis_schema()
643:             # Create analysis prompt
644:             prompt = self._create_analysis_prompt()
645:             self.logger.info("Sending video to Gemini Flash 2.5 for analysis...")
646:             # Request comprehensive analysis
647:             response = self.client.models.generate_content(
648:                 model="gemini-2.5-flash-preview-05-20",
649:                 contents=[prompt, video_part],
650:                 config=types.GenerateContentConfig(
651:                     response_mime_type="application/json",
652:                     response_schema=schema,
653:                     mediaResolution=types.MediaResolution.MEDIA_RESOLUTION_LOW
654:                 )
655:             )
656:             self.logger.info("AI analysis completed successfully")
657:             return response.text
658:         except Exception as e:
659:             self.logger.error(f"AI analysis failed: {str(e)}")
660:             raise
661: class VideoProcessor(object): # Changed inheritance
662:     """Pipeline processor for video analysis."""
663:     def __init__(self, config: Config, compression_config: Dict[str, Any] = None):
664:         self.config = config
665:         self.logger = logging.getLogger(self.__class__.__name__)
666:         # Store compression configuration for VideoCompressor
667:         self.compression_config = compression_config or {}
668:         # Get API key from environment variables (now loaded from .env file)
669:         self.api_key = os.getenv('GEMINI_API_KEY')
670:         if not self.api_key:
671:             raise ValueError("GEMINI_API_KEY environment variable not found. Check your .env file in the project root.")
672:         self.logger.info("VideoProcessor initialized successfully")
673:     def _display_analysis_summary(self, analysis_json: Dict[str, Any]) -> None:
674:         """
675:         Display a comprehensive summary of the analysis results.
676:         Args:
677:             analysis_json: The complete analysis results dictionary
678:         """
679:         self.logger.info("\n" + "="*80)
680:         self.logger.info("COMPREHENSIVE AI ANALYSIS SUMMARY")
681:         self.logger.info("="*80)
682:         # Summary section
683:         summary = analysis_json.get('summary', {})
684:         self.logger.info(f"\n📝 OVERALL SUMMARY:")
685:         self.logger.info(f"   Content: {summary.get('overall', 'No summary available')}")
686:         self.logger.info(f"   Category: {summary.get('content_category', 'Unknown')}")
687:         key_activities = summary.get('key_activities', [])
688:         if key_activities:
689:             self.logger.info(f"   Key Activities: {', '.join(key_activities)}")
690:         # Visual Analysis section
691:         visual = analysis_json.get('visual_analysis', {})
692:         self.logger.info(f"\n🎬 VISUAL ANALYSIS:")
693:         shot_types = visual.get('shot_types', [])
694:         if shot_types:
695:             self.logger.info(f"   Shot Types Detected: {len(shot_types)} different shots")
696:             for shot in shot_types[:3]:  # Show first 3 shots
697:                 self.logger.info(f"     • {shot['timestamp']}: {shot['shot_type']} - {shot['description']}")
698:             if len(shot_types) > 3:
699:                 self.logger.info(f"     ... and {len(shot_types) - 3} more shots")
700:         # Technical quality
701:         tech_quality = visual.get('technical_quality', {})
702:         if tech_quality:
703:             self.logger.info(f"   Technical Quality:")
704:             self.logger.info(f"     • Focus: {tech_quality.get('overall_focus_quality', 'Unknown')}")
705:             self.logger.info(f"     • Stability: {tech_quality.get('stability_assessment', 'Unknown')}")
706:             self.logger.info(f"     • Usability: {tech_quality.get('usability_rating', 'Unknown')}")
707:             artifacts = tech_quality.get('detected_artifacts', [])
708:             if artifacts:
709:                 self.logger.info(f"     • Artifacts Detected: {len(artifacts)} issues found")
710:                 for artifact in artifacts:
711:                     self.logger.info(f"       - {artifact['type']}: {artifact['severity']} - {artifact['description']}")
712:         # Text and graphics
713:         text_graphics = visual.get('text_and_graphics', {})
714:         detected_text = text_graphics.get('detected_text', [])
715:         detected_logos = text_graphics.get('detected_logos_icons', [])
716:         if detected_text or detected_logos:
717:             self.logger.info(f"   Text & Graphics:")
718:             if detected_text:
719:                 self.logger.info(f"     • Text Elements: {len(detected_text)} detected")
720:                 for text in detected_text[:2]:  # Show first 2
721:                     self.logger.info(f"       - {text['timestamp']}: {text['text_type']} ({text['readability']})")
722:                     if text.get('text_content'):
723:                         self.logger.info(f"         Content: \"{text['text_content']}\"")
724:             if detected_logos:
725:                 self.logger.info(f"     • Logos/Icons: {len(detected_logos)} detected")
726:                 for logo in detected_logos[:2]:  # Show first 2
727:                     self.logger.info(f"       - {logo['timestamp']}: {logo['element_type']} - {logo['description']}")
728:         # Audio Analysis section
729:         audio = analysis_json.get('audio_analysis', {})
730:         self.logger.info(f"\n🔊 AUDIO ANALYSIS:")
731:         # Transcript
732:         transcript = audio.get('transcript', {})
733:         if transcript and transcript.get('full_text'):
734:             full_text = transcript['full_text']
735:             preview_text = full_text[:100] + "..." if len(full_text) > 100 else full_text
736:             self.logger.info(f"   Transcript Preview: \"{preview_text}\"")
737:         # Speakers
738:         speaker_analysis = audio.get('speaker_analysis', {})
739:         speaker_count = speaker_analysis.get('speaker_count', 0)
740:         if speaker_count > 0:
741:             self.logger.info(f"   Speakers: {speaker_count} detected")
742:             speakers = speaker_analysis.get('speakers', [])
743:             for speaker in speakers:
744:                 self.logger.info(f"     • {speaker['speaker_id']}: {speaker['speaking_time_seconds']:.1f}s speaking time")
745:         # Sound events
746:         sound_events = audio.get('sound_events', [])
747:         if sound_events:
748:             self.logger.info(f"   Sound Events: {len(sound_events)} detected")
749:             for event in sound_events[:3]:  # Show first 3
750:                 self.logger.info(f"     • {event['timestamp']}: {event['event_type']} - {event['description']}")
751:         # Audio quality
752:         audio_quality = audio.get('audio_quality', {})
753:         if audio_quality:
754:             self.logger.info(f"   Audio Quality:")
755:             self.logger.info(f"     • Clarity: {audio_quality.get('clarity', 'Unknown')}")
756:             self.logger.info(f"     • Background Noise: {audio_quality.get('background_noise_level', 'Unknown')}")
757:             self.logger.info(f"     • Dialogue Intelligibility: {audio_quality.get('dialogue_intelligibility', 'Unknown')}")
758:         # Content Analysis section
759:         content = analysis_json.get('content_analysis', {})
760:         self.logger.info(f"\n👥 CONTENT ANALYSIS:")
761:         # Entities
762:         entities = content.get('entities', {})
763:         people_count = entities.get('people_count', 0)
764:         if people_count > 0:
765:             self.logger.info(f"   People: {people_count} detected")
766:             people_details = entities.get('people_details', [])
767:             for person in people_details[:3]:  # Show first 3
768:                 self.logger.info(f"     • {person['description']} ({person.get('role', 'Unknown role')})")
769:         locations = entities.get('locations', [])
770:         if locations:
771:             self.logger.info(f"   Locations:")
772:             for location in locations:
773:                 self.logger.info(f"     • {location['name']} ({location['type']}): {location.get('description', '')}")
774:         objects = entities.get('objects_of_interest', [])
775:         if objects:
776:             self.logger.info(f"   Objects of Interest: {len(objects)} detected")
777:             for obj in objects[:3]:  # Show first 3
778:                 self.logger.info(f"     • {obj['object']}: {obj['significance']}")
779:         # Activity summary
780:         activities = content.get('activity_summary', [])
781:         if activities:
782:             self.logger.info(f"   Key Activities:")
783:             for activity in activities[:5]:  # Show first 5
784:                 importance_emoji = "🔴" if activity['importance'] == "High" else "🟡" if activity['importance'] == "Medium" else "🟢"
785:                 self.logger.info(f"     {importance_emoji} {activity['timestamp']}: {activity['activity']} ({activity.get('duration', 'Unknown duration')})")
786:         # Content warnings
787:         warnings = content.get('content_warnings', [])
788:         if warnings:
789:             self.logger.info(f"\n⚠️  CONTENT WARNINGS:")
790:             for warning in warnings:
791:                 self.logger.info(f"   • {warning['type']}: {warning['description']} (at {warning['timestamp']})")
792:         # Keyframe recommendations
793:         keyframes = visual.get('keyframe_analysis', {}).get('recommended_keyframes', [])
794:         if keyframes:
795:             self.logger.info(f"\n🖼️  RECOMMENDED KEYFRAMES:")
796:             for keyframe in keyframes[:3]:  # Show first 3
797:                 self.logger.info(f"   • {keyframe['timestamp']}: {keyframe['reason']} (Quality: {keyframe['visual_quality']})")
798:         self.logger.info("\n" + "="*80)
799:         self.logger.info("END OF ANALYSIS SUMMARY")
800:         self.logger.info("="*80)
801:     def process(self, file_path: str, output_dir: str = None) -> Dict[str, Any]:
802:         """
803:         Process a video file through compression and AI analysis.
804:         Args:
805:             file_path: Path to video file
806:             output_dir: Directory to save processed files (optional)
807:         Returns:
808:             Dict[str, Any]: Processing results
809:         """
810:         try:
811:             # Verify file exists
812:             if not os.path.exists(file_path):
813:                 self.logger.error(f"File not found: {file_path}")
814:                 return {
815:                     'success': False,
816:                     'error': f"File not found: {file_path}"
817:                 }
818:             self.logger.info(f"Starting compression for: {file_path}")
819:             # Compress video
820:             compressor = VideoCompressor(self.compression_config)
821:             compressed_path = compressor.compress(file_path, output_dir)
822:             self.logger.info(f"Compression successful. Output: {compressed_path}")
823:             self.logger.info(f"Starting AI analysis...")
824:             # Analyze video
825:             # Get FPS from compression config, with fallback to default
826:             fps_for_analysis = self.compression_config.get('fps', 5)
827:             analyzer = VideoAnalyzer(api_key=self.api_key, fps=fps_for_analysis)
828:             analysis_results = analyzer.analyze_video(compressed_path)
829:             # Parse analysis results
830:             analysis_json = json.loads(analysis_results)
831:             self.logger.info(f"AI analysis completed successfully")
832:             # Display comprehensive analysis summary
833:             try:
834:                 self._display_analysis_summary(analysis_json)
835:             except Exception as e:
836:                 self.logger.error(f"Error displaying analysis summary: {str(e)}")
837:             return {
838:                 'success': True,
839:                 'compressed_path': compressed_path,
840:                 'analysis_json': analysis_json
841:             }
842:         except Exception as e:
843:             self.logger.error(f"Video processing failed: {str(e)}", exc_info=True)
844:             return {
845:                 'success': False,
846:                 'error': str(e)
847:             }
</file>

<file path=".env.example">
 1: # Supabase Configuration
 2: SUPABASE_URL=
 3: SUPABASE_ANON_KEY=
 4: SUPABASE_SERVICE_ROLE_KEY=
 5: 
 6: # AI Service Configuration
 7: GEMINI_API_KEY=
 8: 
 9: # Optional: Vector Embedding Configuration (for future use)
10: DEEPINFRA_API_KEY=
11: EMBEDDING_MODEL=BAAI/bge-m3
12: EMBEDDING_DIMENSIONS=1024
13: EMBEDDING_MAX_TOKENS=3500
</file>

<file path=".gitattributes">
1: # Auto detect text files and perform LF normalization
2: * text=auto
</file>

<file path=".gitignore">
 1: .env
 2: # Python
 3: __pycache__/
 4: *.py[cod]
 5: *$py.class
 6: *.so
 7: .Python
 8: build/
 9: develop-eggs/
10: dist/
11: downloads/
12: eggs/
13: .eggs/
14: lib/
15: lib64/
16: parts/
17: sdist/
18: var/
19: wheels/
20: *.egg-info/
21: .installed.cfg
22: *.egg
23: 
24: # Virtual Environment
25: venv/
26: ENV/
27: env/
28: .env/
29: .venv/
30: 
31: # Jupyter Notebook
32: .ipynb_checkpoints
33: 
34: # Generated files
35: json_output/
36: logs/
37: output/
38: 
39: # IDE files
40: .idea/
41: .vscode/
42: *.swp
43: *.swo
44: 
45: # OS files
46: .DS_Store
47: .DS_Store?
48: ._*
49: .Spotlight-V100
50: .Trashes
51: ehthumbs.db
52: Thumbs.db
53: /test
54: /output_depthpro
55: /output_depthpro_hf
56: /output_huggingface
57: /output_poselib
58: /test_poselib_output
59: /thumbnails_output
60: P1000011.MOV
61: *.mp4
</file>

<file path=".mcp.json">
 1: {
 2:   "mcpServers": {
 3:     "repomix": {
 4:       "command": "npx",
 5:       "args": ["-y", "repomix", "--mcp"]
 6:     },
 7:     "context7": {
 8:       "command": "npx",
 9:       "args": ["-y", "@upstash/context7-mcp@latest"]
10:     },
11:     "dart": {
12:       "command": "npx",
13:       "args": ["-y", "dart-mcp-server"],
14:       "env": {
15:         "DART_TOKEN": "dsa_4782353e6356c556f2b7607e1b5bdf10e376a88e0c39b903f9dd6247455e5be0"
16:       }
17:     },
18:     "desktop-commander": {
19:       "command": "npx",
20:       "args": ["-y", "@wonderwhy-er/desktop-commander"]
21:     }
22:   }
23: }
</file>

<file path="CLAUDE.md">
  1: # CLAUDE.md
  2: 
  3: This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.
  4: 
  5: ## Current Development Context
  6: - **Active Branch**: `supabase` (implementing database integration)
  7: - **Main Branch**: `main` (stable release)
  8: - **Current Focus**: Supabase database integration and authentication
  9: - **Status**: Core processing pipeline complete, database integration in progress
 10: 
 11: ## Project Overview
 12: 
 13: This is an AI-Powered Video Ingest & Catalog Tool - a comprehensive CLI application for automated video content analysis, categorization, and metadata extraction. The tool processes video files through a configurable pipeline to extract technical metadata, generate thumbnails, perform AI-powered content analysis, and output structured JSON data.
 14: 
 15: ## Architecture
 16: 
 17: ### Core Components
 18: 
 19: **Main Entry Point**: `video_ingest_tool/__main__.py` - imports and runs the CLI
 20: **CLI Interface**: `video_ingest_tool/cli.py` - Typer-based command-line interface with two main commands:
 21: - `ingest` - Main processing command with extensive configuration options
 22: - `list_steps` - Shows available pipeline steps
 23: 
 24: **Processing Pipeline**: `video_ingest_tool/pipeline.py` - Modular pipeline system with:
 25: - `ProcessingPipeline` class for managing configurable steps
 26: - `ProcessingStep` class for individual processing operations
 27: - Decorator-based step registration system
 28: - Runtime enable/disable configuration
 29: 
 30: **Data Models**: `video_ingest_tool/models.py` - Comprehensive Pydantic models including:
 31: - `VideoIngestOutput` - Main output model containing all extracted data
 32: - Technical metadata models (video, audio, subtitle tracks)
 33: - AI analysis models (visual, audio, content analysis)
 34: - 25+ detailed model classes for structured data
 35: 
 36: ### Key Processing Modules
 37: 
 38: - `discovery.py` - Directory scanning and file discovery
 39: - `processor.py` - Main processing orchestration and pipeline execution
 40: - `extractors.py`, `extractors_extended.py`, `extractors_hdr.py` - Metadata extraction using multiple tools
 41: - `video_processor.py` - AI video analysis and compression
 42: - `output.py` - JSON output and file organization
 43: - `config.py` - Logging setup and configuration
 44: - `utils.py` - Utility functions
 45: 
 46: ## Common Development Commands
 47: 
 48: ### Running the Tool
 49: ```bash
 50: # Basic usage - process videos in a directory
 51: python -m video_ingest_tool ingest /path/to/videos/
 52: 
 53: # With configuration options
 54: python -m video_ingest_tool ingest /path/to/videos/ --recursive --limit=5 --output-dir=output
 55: 
 56: # Configure pipeline steps
 57: python -m video_ingest_tool ingest /path/to/videos/ --disable=hdr_extraction,ai_focal_length
 58: python -m video_ingest_tool ingest /path/to/videos/ --enable=ai_video_analysis --fps=5 --bitrate=1000k
 59: 
 60: # List available pipeline steps
 61: python -m video_ingest_tool list_steps
 62: ```
 63: 
 64: ### Development Setup
 65: ```bash
 66: # Install dependencies
 67: pip install -r requirements.txt
 68: 
 69: # External dependencies required:
 70: # - FFmpeg (must be in PATH)
 71: # - ExifTool (must be in PATH)
 72: 
 73: # Set up environment variables (copy and modify)
 74: cp .env.example .env
 75: # Edit .env with your API keys
 76: ```
 77: 
 78: ### Authentication Commands (Supabase Integration)
 79: ```bash
 80: # Login to your account
 81: python -m video_ingest_tool auth login
 82: 
 83: # Check authentication status
 84: python -m video_ingest_tool auth status
 85: 
 86: # View user profile and statistics
 87: python -m video_ingest_tool profile show
 88: python -m video_ingest_tool profile stats
 89: 
 90: # Logout
 91: python -m video_ingest_tool auth logout
 92: ```
 93: 
 94: ### Database Integration Commands
 95: ```bash
 96: # Process videos and store in database
 97: python -m video_ingest_tool ingest /path/to/videos/ --store-database
 98: 
 99: # Generate vector embeddings for semantic search
100: python -m video_ingest_tool ingest /path/to/videos/ --store-database --generate-embeddings
101: 
102: # Test database connection
103: python -m video_ingest_tool test-db
104: ```
105: 
106: ## Important Implementation Details
107: 
108: ### Pipeline System
109: The tool uses a modular pipeline architecture where each processing step can be individually enabled/disabled:
110: - Steps are registered using decorators in `processor.py`
111: - Configuration via CLI parameters (`--enable`, `--disable`) or JSON config files
112: - Each step receives data dictionary and returns updates to merge back
113: 
114: ### AI Integration
115: - **Gemini Flash 2.5** integration for comprehensive video analysis (disabled by default due to API costs)
116: - Hardware-accelerated video compression for AI processing
117: - Structured AI analysis output including transcription, visual analysis, content analysis
118: - Separate detailed AI analysis JSON files
119: 
120: ### Output Structure
121: Creates timestamped run directories with organized output:
122: ```
123: run_YYYYMMDD_HHMMSS/
124: ├── json/          # Individual video JSON files
125: ├── thumbnails/    # Video thumbnails by checksum
126: ├── ai_analysis/   # Detailed AI analysis files
127: ├── compressed/    # Compressed videos for AI analysis
128: └── logs/          # Processing logs
129: ```
130: 
131: ### Key Features
132: - **Non-destructive processing** - never modifies original files
133: - **Checksum-based deduplication** - avoids reprocessing identical files
134: - **Comprehensive metadata extraction** - technical, HDR, audio, subtitle metadata
135: - **Intelligent thumbnail generation** - evenly distributed keyframes
136: - **Configurable pipeline** - enable/disable specific processing steps
137: - **Rich CLI interface** - progress bars, tables, colored output
138: - **Structured data models** - 25+ Pydantic models for type safety
139: 
140: ## Dependencies and Requirements
141: 
142: ### Python Dependencies
143: The project has extensive dependencies listed in `plan.md` section 8.1, including:
144: - PyAV, pymediainfo, PyExifTool for metadata extraction
145: - OpenCV, Pillow for image processing
146: - Typer, Rich for CLI interface
147: - Pydantic for data validation
148: - Google Generative AI for AI analysis
149: - Transformers, PyTorch for AI focal length detection
150: 
151: ### External Dependencies
152: - **FFmpeg** - must be installed and available in PATH
153: - **ExifTool** - must be installed and available in PATH
154: 
155: ## Database Integration (In Progress)
156: The codebase is transitioning to Supabase PostgreSQL integration:
157: 
158: ### Current Status
159: - **Database Schema**: Complete schema designed in `SUPABASE_IMPLEMENTATION.md`
160: - **Authentication System**: Implemented in `auth.py` with JWT tokens
161: - **Vector Embeddings**: Ready for BAAI/bge-m3 via DeepInfra API
162: - **CLI Commands**: Auth and profile management commands available
163: 
164: ### Key Files for Database Work
165: - `supabase_config.py` - Supabase client configuration
166: - `auth.py` - Authentication management
167: - `SUPABASE_IMPLEMENTATION.md` - Complete implementation guide
168: - `DATABASE_SETUP_INSTRUCTIONS.md` - Setup instructions
169: - `SUPABASE_FIX.sql` - Database fixes and migrations
170: 
171: ### Environment Variables Required
172: ```bash
173: SUPABASE_URL=https://your-project.supabase.co
174: SUPABASE_ANON_KEY=your_anon_key
175: SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
176: DEEPINFRA_API_KEY=your_deepinfra_key
177: GEMINI_API_KEY=your_gemini_key
178: ```
179: 
180: ## Testing and Validation
181: - `test_supabase.py` - Database connection testing
182: - Use existing comprehensive data models for test fixtures
183: - Test files should follow the existing project structure
184: 
185: ## Development Best Practices
186: - Always check authentication before database operations
187: - Use the pipeline system for new processing steps
188: - Store absolute local paths for CLI file access
189: - Follow the existing Pydantic model patterns
190: 
191: ## Common Development Tasks
192: 
193: ### Adding a New Pipeline Step
194: 1. Open `video_ingest_tool/processor.py`
195: 2. Use the `@pipeline.register_step()` decorator
196: 3. Set enabled=False by default for experimental features
197: 4. Include comprehensive error handling and logging
198: 
199: ### Database Operations
200: 1. Always check authentication with `auth_manager.get_current_session()`
201: 2. Use authenticated client: `auth_manager.get_authenticated_client()`
202: 3. Follow the database schema in `SUPABASE_IMPLEMENTATION.md`
203: 4. Test connections with `test_supabase.py`
204: 
205: ### Running Tests
206: ```bash
207: # Test database connection
208: python test_supabase.py
209: 
210: # Test video processing on sample files
211: python -m video_ingest_tool ingest /path/to/test/videos/ --limit=1
212: 
213: # Test specific pipeline steps
214: python -m video_ingest_tool ingest /path/to/test/videos/ --enable=checksum_generation --disable=all_others
215: ```
216: 
217: ## Troubleshooting
218: 
219: ### Common Issues
220: - **FFmpeg not found**: Ensure FFmpeg is installed and in PATH
221: - **ExifTool not found**: Install ExifTool and verify PATH
222: - **Authentication failed**: Check `.env` file and Supabase credentials
223: - **Database connection failed**: Verify Supabase URL and keys
224: - **Pipeline step failed**: Check logs in `logs/` directory
225: 
226: ### Debug Mode
227: - Enable verbose logging by checking `config.py`
228: - View pipeline step execution in real-time
229: - Check individual JSON outputs in run directories
</file>

<file path="complete_policy_reset.sql">
 1: -- =====================================================
 2: -- COMPLETE RLS POLICY RESET AND FIX
 3: -- =====================================================
 4: -- Disable RLS temporarily to fix the recursion issue
 5: ALTER TABLE public.user_profiles DISABLE ROW LEVEL SECURITY;
 6: ALTER TABLE public.clips DISABLE ROW LEVEL SECURITY;
 7: ALTER TABLE public.transcripts DISABLE ROW LEVEL SECURITY;
 8: ALTER TABLE public.analysis DISABLE ROW LEVEL SECURITY;
 9: -- Drop ALL existing policies to start fresh
10: DROP POLICY IF EXISTS "Users can view own profile" ON public.user_profiles;
11: DROP POLICY IF EXISTS "Users can update own profile" ON public.user_profiles;
12: DROP POLICY IF EXISTS "Users can insert own profile" ON public.user_profiles;
13: DROP POLICY IF EXISTS "Enable read access for own profile" ON public.user_profiles;
14: DROP POLICY IF EXISTS "Enable insert for own profile" ON public.user_profiles;
15: DROP POLICY IF EXISTS "Enable update for own profile" ON public.user_profiles;
16: DROP POLICY IF EXISTS "Users can manage own clips" ON public.clips;
17: DROP POLICY IF EXISTS "Enable all operations for authenticated users on clips" ON public.clips;
18: DROP POLICY IF EXISTS "Users can manage own transcripts" ON public.transcripts;
19: DROP POLICY IF EXISTS "Enable all operations for authenticated users on transcripts" ON public.transcripts;
20: DROP POLICY IF EXISTS "Users can manage own analysis" ON public.analysis;
21: DROP POLICY IF EXISTS "Enable all operations for authenticated users on analysis" ON public.analysis;
22: -- Re-enable RLS
23: ALTER TABLE public.user_profiles ENABLE ROW LEVEL SECURITY;
24: ALTER TABLE public.clips ENABLE ROW LEVEL SECURITY;
25: ALTER TABLE public.transcripts ENABLE ROW LEVEL SECURITY;
26: ALTER TABLE public.analysis ENABLE ROW LEVEL SECURITY;
27: -- Create simple, non-recursive policies for user_profiles
28: CREATE POLICY "user_profiles_select" ON public.user_profiles
29:     FOR SELECT USING (id = auth.uid());
30: CREATE POLICY "user_profiles_insert" ON public.user_profiles
31:     FOR INSERT WITH CHECK (id = auth.uid());
32: CREATE POLICY "user_profiles_update" ON public.user_profiles
33:     FOR UPDATE USING (id = auth.uid());
34: -- Create simple policies for clips (allow all operations for authenticated users)
35: CREATE POLICY "clips_all" ON public.clips
36:     FOR ALL USING (auth.uid() IS NOT NULL);
37: -- Create simple policies for transcripts (allow all operations for authenticated users)
38: CREATE POLICY "transcripts_all" ON public.transcripts
39:     FOR ALL USING (auth.uid() IS NOT NULL);
40: -- Create simple policies for analysis (allow all operations for authenticated users)
41: CREATE POLICY "analysis_all" ON public.analysis
42:     FOR ALL USING (auth.uid() IS NOT NULL);
43: -- Verify policies are in place
44: SELECT schemaname, tablename, policyname, cmd, qual 
45: FROM pg_policies 
46: WHERE schemaname = 'public' 
47: ORDER BY tablename, policyname;
48: -- Test that we can query user_profiles without recursion
49: SELECT count(*) FROM public.user_profiles;
</file>

<file path="DATABASE_SETUP_INSTRUCTIONS.md">
 1: # Database Setup Instructions
 2: 
 3: ## Complete AI Ingesting Tool Database Schema
 4: 
 5: Your `database_setup.sql` file now contains the **complete schema** based on your SUPABASE_IMPLEMENTATION.md:
 6: 
 7: ### ✅ What's Included:
 8: 
 9: 1. **User Profiles** (with auto-creation trigger) - ✅ This fixes your profile creation issue
10: 2. **Clips Table** - Main video metadata storage
11: 3. **Segments Table** - Segment-level analysis
12: 4. **Analysis Table** - AI analysis results 
13: 5. **Vectors Table** - Vector embeddings for search
14: 6. **Transcripts Table** - Video transcriptions
15: 7. **Performance Indexes** - Optimized for search and queries
16: 8. **Row Level Security Policies** - Complete security setup
17: 9. **Helper Functions** - Utility functions for the CLI
18: 
19: ### 🚀 How to Apply:
20: 
21: 1. **Go to your Supabase Dashboard**
22:    - Visit: https://supabase.com/dashboard
23:    - Select your project: https://dwnujuxvakiqsqnimkby.supabase.co
24: 
25: 2. **Open SQL Editor**
26:    - Click "SQL Editor" in the left sidebar
27:    - Click "New Query"
28: 
29: 3. **Copy and Paste**
30:    - Copy the ENTIRE contents of `database_setup.sql`  
31:    - Paste into the SQL Editor
32: 
33: 4. **Run the Script**
34:    - Click "Run" or press Ctrl+Enter
35:    - This will create all tables, indexes, policies, and functions
36: 
37: ### 🧪 Test After Setup:
38: 
39: ```bash
40: # Test the connection (should show all tables as "Exists")
41: conda activate video-ingest
42: cd /Users/developer/Development/GitHub/AIIngestingTool
43: python test_supabase.py
44: 
45: # Test authentication
46: python -m video_ingest_tool auth signup
47: python -m video_ingest_tool auth login
48: python -m video_ingest_tool auth status
49: ```
50: 
51: ### 🔑 Key Features:
52: 
53: - **Auto Profile Creation**: Users automatically get profiles when they sign up
54: - **Row Level Security**: Users can only see their own data
55: - **Admin Support**: Admin users can see all data
56: - **Vector Search Ready**: Supports BAAI/bge-m3 embeddings
57: - **Full Text Search**: Built-in search across all content
58: - **Segment Support**: Ready for future segment-level analysis
59: 
60: This is the complete database schema that will support all your AI Ingesting Tool features!
</file>

<file path="database_setup.sql">
  1: -- =====================================================
  2: -- AI INGESTING TOOL - COMPLETE DATABASE SETUP
  3: -- Version: PRODUCTION READY - All fixes incorporated
  4: -- =====================================================
  5: -- Enable required extensions
  6: CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
  7: CREATE EXTENSION IF NOT EXISTS "vector";
  8: -- =====================================================
  9: -- 1. USER PROFILES TABLE & TRIGGER (WORKING VERSION)
 10: -- =====================================================
 11: CREATE TABLE IF NOT EXISTS user_profiles (
 12:   id UUID PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,
 13:   profile_type TEXT CHECK (profile_type IN ('admin', 'user')) DEFAULT 'user',
 14:   display_name TEXT,
 15:   created_at TIMESTAMPTZ DEFAULT NOW(),
 16:   updated_at TIMESTAMPTZ DEFAULT NOW()
 17: );
 18: ALTER TABLE user_profiles ENABLE ROW LEVEL SECURITY;
 19: -- Bulletproof user profile creation trigger (TESTED & WORKING)
 20: CREATE OR REPLACE FUNCTION handle_new_user()
 21: RETURNS TRIGGER 
 22: LANGUAGE plpgsql 
 23: SECURITY DEFINER
 24: AS $$
 25: BEGIN
 26:   INSERT INTO public.user_profiles (id, profile_type, display_name)
 27:   VALUES (
 28:     NEW.id,
 29:     'user',
 30:     COALESCE(
 31:       NEW.raw_user_meta_data->>'display_name',
 32:       NEW.raw_user_meta_data->>'full_name', 
 33:       NEW.raw_user_meta_data->>'name',
 34:       split_part(NEW.email, '@', 1),
 35:       'User'
 36:     )
 37:   );
 38:   RETURN NEW;
 39: EXCEPTION
 40:   WHEN OTHERS THEN
 41:     RAISE WARNING 'Failed to create user profile for user %: %', NEW.id, SQLERRM;
 42:     RETURN NEW;
 43: END;
 44: $$;
 45: DROP TRIGGER IF EXISTS on_auth_user_created ON auth.users;
 46: CREATE TRIGGER on_auth_user_created
 47:   AFTER INSERT ON auth.users
 48:   FOR EACH ROW EXECUTE FUNCTION handle_new_user();
 49: -- =====================================================
 50: -- 2. CLIPS TABLE - Main video storage
 51: -- =====================================================
 52: CREATE TABLE IF NOT EXISTS clips (
 53:   id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
 54:   user_id UUID REFERENCES auth.users(id) NOT NULL,
 55:   -- File information
 56:   file_path TEXT NOT NULL,
 57:   local_path TEXT NOT NULL,
 58:   file_name TEXT NOT NULL,
 59:   file_checksum TEXT UNIQUE NOT NULL,
 60:   file_size_bytes BIGINT NOT NULL,
 61:   duration_seconds NUMERIC,
 62:   created_at TIMESTAMPTZ DEFAULT NOW(),
 63:   processed_at TIMESTAMPTZ DEFAULT NOW(),
 64:   -- Technical metadata
 65:   width INTEGER,
 66:   height INTEGER,
 67:   frame_rate NUMERIC,
 68:   codec TEXT,
 69:   camera_make TEXT,
 70:   camera_model TEXT,
 71:   container TEXT,
 72:   -- AI analysis summaries
 73:   content_category TEXT,
 74:   content_summary TEXT,
 75:   content_tags TEXT[],
 76:   -- Transcript data
 77:   full_transcript TEXT,
 78:   transcript_preview TEXT,
 79:   -- Search columns (populated by triggers)
 80:   searchable_content TEXT,
 81:   fts tsvector,
 82:   -- Complex metadata as JSONB
 83:   technical_metadata JSONB,
 84:   camera_details JSONB,
 85:   audio_tracks JSONB,
 86:   subtitle_tracks JSONB,
 87:   thumbnails TEXT[]
 88: );
 89: ALTER TABLE clips ENABLE ROW LEVEL SECURITY;
 90: -- Search content trigger for clips
 91: CREATE OR REPLACE FUNCTION update_clips_search_content()
 92: RETURNS TRIGGER AS $$
 93: BEGIN
 94:   NEW.searchable_content := COALESCE(NEW.file_name, '') || ' ' ||
 95:                            COALESCE(NEW.content_summary, '') || ' ' ||
 96:                            COALESCE(NEW.transcript_preview, '') || ' ' ||
 97:                            COALESCE(array_to_string(NEW.content_tags, ' '), '') || ' ' ||
 98:                            COALESCE(NEW.content_category, '');
 99:   NEW.fts := to_tsvector('english', NEW.searchable_content);
100:   RETURN NEW;
101: END;
102: $$ LANGUAGE plpgsql;
103: CREATE TRIGGER clips_search_content_trigger
104:   BEFORE INSERT OR UPDATE ON clips
105:   FOR EACH ROW EXECUTE FUNCTION update_clips_search_content();
106: -- =====================================================
107: -- 3. SEGMENTS TABLE - For future segment-level analysis
108: -- =====================================================
109: CREATE TABLE IF NOT EXISTS segments (
110:   id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
111:   clip_id UUID REFERENCES clips(id) ON DELETE CASCADE,
112:   user_id UUID REFERENCES auth.users(id) NOT NULL,
113:   segment_index INTEGER NOT NULL,
114:   start_time_seconds NUMERIC NOT NULL,
115:   end_time_seconds NUMERIC NOT NULL,
116:   duration_seconds NUMERIC,
117:   segment_type TEXT DEFAULT 'auto',
118:   speaker_id TEXT,
119:   segment_description TEXT,
120:   keyframe_timestamp NUMERIC,
121:   segment_content TEXT,
122:   fts tsvector,
123:   created_at TIMESTAMPTZ DEFAULT NOW(),
124:   UNIQUE(clip_id, segment_index),
125:   CONSTRAINT check_segment_times CHECK (start_time_seconds < end_time_seconds),
126:   CONSTRAINT check_segment_index CHECK (segment_index >= 0)
127: );
128: ALTER TABLE segments ENABLE ROW LEVEL SECURITY;
129: -- Segment calculated fields trigger
130: CREATE OR REPLACE FUNCTION update_segments_calculated_fields()
131: RETURNS TRIGGER AS $$
132: BEGIN
133:   NEW.duration_seconds := NEW.end_time_seconds - NEW.start_time_seconds;
134:   NEW.fts := to_tsvector('english', COALESCE(NEW.segment_content, ''));
135:   RETURN NEW;
136: END;
137: $$ LANGUAGE plpgsql;
138: CREATE TRIGGER segments_calculated_fields_trigger
139:   BEFORE INSERT OR UPDATE ON segments
140:   FOR EACH ROW EXECUTE FUNCTION update_segments_calculated_fields();
141: -- =====================================================
142: -- 4. ANALYSIS TABLE - AI analysis results
143: -- =====================================================
144: CREATE TABLE IF NOT EXISTS analysis (
145:   id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
146:   clip_id UUID REFERENCES clips(id) ON DELETE CASCADE,
147:   segment_id UUID REFERENCES segments(id) ON DELETE CASCADE,
148:   user_id UUID REFERENCES auth.users(id) NOT NULL,
149:   analysis_type TEXT NOT NULL,
150:   analysis_scope TEXT NOT NULL CHECK (analysis_scope IN ('full_clip', 'segment')),
151:   ai_model TEXT DEFAULT 'gemini-flash-2.5',
152:   content_category TEXT,
153:   usability_rating TEXT,
154:   speaker_count INTEGER,
155:   visual_analysis JSONB,
156:   audio_analysis JSONB,
157:   content_analysis JSONB,
158:   analysis_summary JSONB,
159:   analysis_file_path TEXT,
160:   created_at TIMESTAMPTZ DEFAULT NOW(),
161:   CONSTRAINT check_analysis_scope CHECK (
162:     (analysis_scope = 'full_clip' AND segment_id IS NULL) OR
163:     (analysis_scope = 'segment' AND segment_id IS NOT NULL)
164:   )
165: );
166: ALTER TABLE analysis ENABLE ROW LEVEL SECURITY;
167: -- =====================================================
168: -- 5. VECTORS TABLE - For semantic search
169: -- =====================================================
170: CREATE TABLE IF NOT EXISTS vectors (
171:   id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
172:   clip_id UUID REFERENCES clips(id) ON DELETE CASCADE,
173:   segment_id UUID REFERENCES segments(id) ON DELETE CASCADE,
174:   user_id UUID REFERENCES auth.users(id) NOT NULL,
175:   embedding_type TEXT NOT NULL CHECK (embedding_type IN ('full_clip', 'segment', 'keyframe')),
176:   embedding_source TEXT NOT NULL,
177:   summary_vector vector(1024),
178:   keyword_vector vector(1024),
179:   embedded_content TEXT NOT NULL,
180:   original_content TEXT,
181:   token_count INTEGER,
182:   original_token_count INTEGER,
183:   truncation_method TEXT,
184:   created_at TIMESTAMPTZ DEFAULT NOW(),
185:   CONSTRAINT check_vector_scope CHECK (
186:     (embedding_type = 'full_clip' AND segment_id IS NULL) OR
187:     (embedding_type IN ('segment', 'keyframe') AND segment_id IS NOT NULL)
188:   )
189: );
190: ALTER TABLE vectors ENABLE ROW LEVEL SECURITY;
191: -- =====================================================
192: -- 6. TRANSCRIPTS TABLE
193: -- =====================================================
194: CREATE TABLE IF NOT EXISTS transcripts (
195:   clip_id UUID REFERENCES clips(id) ON DELETE CASCADE,
196:   user_id UUID REFERENCES auth.users(id) NOT NULL,
197:   full_text TEXT NOT NULL,
198:   segments JSONB NOT NULL,
199:   speakers JSONB,
200:   non_speech_events JSONB,
201:   fts tsvector,
202:   created_at TIMESTAMPTZ DEFAULT NOW(),
203:   PRIMARY KEY (clip_id)
204: );
205: ALTER TABLE transcripts ENABLE ROW LEVEL SECURITY;
206: -- Transcript search trigger
207: CREATE OR REPLACE FUNCTION update_transcript_search()
208: RETURNS TRIGGER AS $$
209: BEGIN
210:   NEW.fts := to_tsvector('english', NEW.full_text);
211:   RETURN NEW;
212: END;
213: $$ LANGUAGE plpgsql;
214: CREATE TRIGGER transcripts_search_trigger
215:   BEFORE INSERT OR UPDATE ON transcripts
216:   FOR EACH ROW EXECUTE FUNCTION update_transcript_search();
217: -- =====================================================
218: -- 7. PERFORMANCE INDEXES
219: -- =====================================================
220: CREATE INDEX IF NOT EXISTS idx_clips_fts ON clips USING gin(fts);
221: CREATE INDEX IF NOT EXISTS idx_segments_fts ON segments USING gin(fts);
222: CREATE INDEX IF NOT EXISTS idx_transcripts_fts ON transcripts USING gin(fts);
223: CREATE INDEX IF NOT EXISTS idx_vectors_summary ON vectors USING hnsw (summary_vector vector_ip_ops);
224: CREATE INDEX IF NOT EXISTS idx_vectors_keyword ON vectors USING hnsw (keyword_vector vector_ip_ops);
225: CREATE INDEX IF NOT EXISTS idx_segments_clip_order ON segments(clip_id, segment_index);
226: CREATE INDEX IF NOT EXISTS idx_segments_time_range ON segments(clip_id, start_time_seconds, end_time_seconds);
227: CREATE INDEX IF NOT EXISTS idx_clips_user_category ON clips(user_id, content_category);
228: CREATE INDEX IF NOT EXISTS idx_clips_tags ON clips USING gin(content_tags);
229: CREATE INDEX IF NOT EXISTS idx_clips_camera ON clips(camera_make, camera_model) WHERE camera_make IS NOT NULL;
230: CREATE INDEX IF NOT EXISTS idx_clips_duration ON clips(duration_seconds) WHERE duration_seconds IS NOT NULL;
231: CREATE INDEX IF NOT EXISTS idx_analysis_clip_type ON analysis(clip_id, analysis_type);
232: CREATE INDEX IF NOT EXISTS idx_analysis_segment ON analysis(segment_id) WHERE segment_id IS NOT NULL;
233: CREATE INDEX IF NOT EXISTS idx_vectors_clip_source ON vectors(clip_id, embedding_source);
234: CREATE INDEX IF NOT EXISTS idx_vectors_segment ON vectors(segment_id) WHERE segment_id IS NOT NULL;
235: -- =====================================================
236: -- 8. ROW LEVEL SECURITY POLICIES
237: -- =====================================================
238: -- User Profile Policies
239: DROP POLICY IF EXISTS "Users can view own profile" ON user_profiles;
240: CREATE POLICY "Users can view own profile" ON user_profiles
241:   FOR SELECT TO authenticated USING (id = auth.uid());
242: DROP POLICY IF EXISTS "Users can update own profile" ON user_profiles;
243: CREATE POLICY "Users can update own profile" ON user_profiles
244:   FOR UPDATE TO authenticated 
245:   USING (id = auth.uid())
246:   WITH CHECK (id = auth.uid() AND profile_type = (SELECT profile_type FROM user_profiles WHERE id = auth.uid()));
247: DROP POLICY IF EXISTS "Admins can view all profiles" ON user_profiles;
248: CREATE POLICY "Admins can view all profiles" ON user_profiles
249:   FOR SELECT TO authenticated
250:   USING (EXISTS (SELECT 1 FROM user_profiles WHERE id = auth.uid() AND profile_type = 'admin'));
251: -- Clips Policies
252: CREATE POLICY "Users can view own clips" ON clips FOR SELECT TO authenticated USING (user_id = auth.uid());
253: CREATE POLICY "Users can insert own clips" ON clips FOR INSERT TO authenticated WITH CHECK (user_id = auth.uid());
254: CREATE POLICY "Users can update own clips" ON clips FOR UPDATE TO authenticated USING (user_id = auth.uid()) WITH CHECK (user_id = auth.uid());
255: CREATE POLICY "Users can delete own clips" ON clips FOR DELETE TO authenticated USING (user_id = auth.uid());
256: CREATE POLICY "Admins can view all clips" ON clips FOR SELECT TO authenticated USING (EXISTS (SELECT 1 FROM user_profiles WHERE id = auth.uid() AND profile_type = 'admin'));
257: -- Segments Policies
258: CREATE POLICY "Users can view own segments" ON segments FOR SELECT TO authenticated USING (user_id = auth.uid());
259: CREATE POLICY "Users can insert own segments" ON segments FOR INSERT TO authenticated WITH CHECK (user_id = auth.uid() AND EXISTS (SELECT 1 FROM clips WHERE id = clip_id AND user_id = auth.uid()));
260: CREATE POLICY "Users can update own segments" ON segments FOR UPDATE TO authenticated USING (user_id = auth.uid()) WITH CHECK (user_id = auth.uid());
261: CREATE POLICY "Users can delete own segments" ON segments FOR DELETE TO authenticated USING (user_id = auth.uid());
262: -- Analysis Policies
263: CREATE POLICY "Users can view own analysis" ON analysis FOR SELECT TO authenticated USING (user_id = auth.uid());
264: CREATE POLICY "Users can insert own analysis" ON analysis FOR INSERT TO authenticated WITH CHECK (user_id = auth.uid() AND EXISTS (SELECT 1 FROM clips WHERE id = clip_id AND user_id = auth.uid()));
265: -- Vectors Policies
266: CREATE POLICY "Users can view own vectors" ON vectors FOR SELECT TO authenticated USING (user_id = auth.uid());
267: CREATE POLICY "Users can insert own vectors" ON vectors FOR INSERT TO authenticated WITH CHECK (user_id = auth.uid() AND EXISTS (SELECT 1 FROM clips WHERE id = clip_id AND user_id = auth.uid()));
268: -- Transcripts Policies
269: CREATE POLICY "Users can view own transcripts" ON transcripts FOR SELECT TO authenticated USING (user_id = auth.uid());
270: CREATE POLICY "Users can insert own transcripts" ON transcripts FOR INSERT TO authenticated WITH CHECK (user_id = auth.uid() AND EXISTS (SELECT 1 FROM clips WHERE id = clip_id AND user_id = auth.uid()));
271: -- =====================================================
272: -- 9. HELPER FUNCTIONS (FIXED - no parameter conflicts)
273: -- =====================================================
274: -- Drop existing functions to avoid conflicts
275: DROP FUNCTION IF EXISTS is_admin(UUID);
276: DROP FUNCTION IF EXISTS get_user_profile(UUID);
277: DROP FUNCTION IF EXISTS get_user_stats(UUID);
278: -- Check if user is admin
279: CREATE FUNCTION is_admin(check_user_id UUID DEFAULT auth.uid())
280: RETURNS BOOLEAN
281: LANGUAGE SQL
282: SECURITY DEFINER
283: AS $$
284:   SELECT EXISTS (
285:     SELECT 1 FROM user_profiles 
286:     WHERE id = check_user_id AND profile_type = 'admin'
287:   );
288: $$;
289: -- Get user profile info
290: CREATE FUNCTION get_user_profile(profile_user_id UUID DEFAULT auth.uid())
291: RETURNS TABLE (
292:   id UUID,
293:   profile_type TEXT,
294:   display_name TEXT,
295:   created_at TIMESTAMPTZ
296: )
297: LANGUAGE SQL
298: SECURITY DEFINER
299: AS $$
300:   SELECT up.id, up.profile_type, up.display_name, up.created_at
301:   FROM user_profiles up
302:   WHERE up.id = profile_user_id;
303: $$;
304: -- User stats function (FIXED - no ambiguous column names)
305: CREATE FUNCTION get_user_stats(stats_user_id UUID DEFAULT auth.uid())
306: RETURNS TABLE (
307:   total_clips INTEGER,
308:   total_duration_hours NUMERIC,
309:   total_storage_gb NUMERIC,
310:   clips_with_transcripts INTEGER,
311:   clips_with_ai_analysis INTEGER
312: )
313: LANGUAGE SQL
314: SECURITY DEFINER
315: AS $$
316:   SELECT 
317:     COUNT(*)::INTEGER as total_clips,
318:     ROUND(SUM(COALESCE(duration_seconds, 0)) / 3600.0, 2) as total_duration_hours,
319:     ROUND(SUM(COALESCE(file_size_bytes, 0)) / (1024.0^3), 2) as total_storage_gb,
320:     COUNT(t.clip_id)::INTEGER as clips_with_transcripts,
321:     COUNT(DISTINCT a.clip_id)::INTEGER as clips_with_ai_analysis
322:   FROM clips c
323:   LEFT JOIN transcripts t ON c.id = t.clip_id
324:   LEFT JOIN analysis a ON c.id = a.clip_id
325:   WHERE c.user_id = stats_user_id;
326: $$;
327: -- =====================================================
328: -- 10. SETUP COMPLETE MESSAGE
329: -- =====================================================
330: DO $$
331: BEGIN
332:   RAISE NOTICE '🎉 AI Ingesting Tool Database Setup Complete!';
333:   RAISE NOTICE '📊 Tables: user_profiles, clips, segments, analysis, vectors, transcripts';
334:   RAISE NOTICE '🔧 Triggers: Auto user profiles, search content updates';
335:   RAISE NOTICE '🔒 Security: RLS policies with user isolation + admin override';
336:   RAISE NOTICE '🚀 Ready for AI Ingesting Tool integration!';
337: END;
338: $$;
</file>

<file path="disable_rls_for_testing.sql">
 1: -- =====================================================
 2: -- DISABLE RLS COMPLETELY FOR TESTING
 3: -- =====================================================
 4: -- Disable RLS entirely on all tables for testing
 5: ALTER TABLE public.user_profiles DISABLE ROW LEVEL SECURITY;
 6: ALTER TABLE public.clips DISABLE ROW LEVEL SECURITY;  
 7: ALTER TABLE public.transcripts DISABLE ROW LEVEL SECURITY;
 8: ALTER TABLE public.analysis DISABLE ROW LEVEL SECURITY;
 9: -- Drop all policies entirely
10: DROP POLICY IF EXISTS "user_profiles_select" ON public.user_profiles;
11: DROP POLICY IF EXISTS "user_profiles_insert" ON public.user_profiles;
12: DROP POLICY IF EXISTS "user_profiles_update" ON public.user_profiles;
13: DROP POLICY IF EXISTS "clips_all" ON public.clips;
14: DROP POLICY IF EXISTS "transcripts_all" ON public.transcripts;
15: DROP POLICY IF EXISTS "analysis_all" ON public.analysis;
16: -- Test insert into clips table directly
17: INSERT INTO public.clips (
18:     id,
19:     user_id, 
20:     filename,
21:     original_path,
22:     file_size,
23:     duration,
24:     checksum,
25:     created_at
26: ) VALUES (
27:     gen_random_uuid(),
28:     auth.uid(),
29:     'test_video.mov',
30:     '/test/path.mov', 
31:     1000000,
32:     17.0,
33:     'test_checksum_123',
34:     now()
35: ) RETURNING id;
</file>

<file path="fix_rls_policies.sql">
 1: -- =====================================================
 2: -- FIX RLS INFINITE RECURSION ERROR
 3: -- =====================================================
 4: -- Drop existing problematic policies
 5: DROP POLICY IF EXISTS "Users can view own profile" ON public.user_profiles;
 6: DROP POLICY IF EXISTS "Users can update own profile" ON public.user_profiles;
 7: DROP POLICY IF EXISTS "Users can insert own profile" ON public.user_profiles;
 8: -- Create simple, non-recursive RLS policies for user_profiles
 9: CREATE POLICY "Enable read access for own profile" ON public.user_profiles
10:     FOR SELECT USING (auth.uid() = id);
11: CREATE POLICY "Enable insert for own profile" ON public.user_profiles
12:     FOR INSERT WITH CHECK (auth.uid() = id);
13: CREATE POLICY "Enable update for own profile" ON public.user_profiles
14:     FOR UPDATE USING (auth.uid() = id);
15: -- Drop and recreate policies for clips table to be simpler
16: DROP POLICY IF EXISTS "Users can manage own clips" ON public.clips;
17: CREATE POLICY "Enable all operations for authenticated users on clips" ON public.clips
18:     FOR ALL USING (auth.uid() IS NOT NULL);
19: -- Drop and recreate policies for transcripts table to be simpler  
20: DROP POLICY IF EXISTS "Users can manage own transcripts" ON public.transcripts;
21: CREATE POLICY "Enable all operations for authenticated users on transcripts" ON public.transcripts
22:     FOR ALL USING (auth.uid() IS NOT NULL);
23: -- Drop and recreate policies for analysis table to be simpler
24: DROP POLICY IF EXISTS "Users can manage own analysis" ON public.analysis;
25: CREATE POLICY "Enable all operations for authenticated users on analysis" ON public.analysis
26:     FOR ALL USING (auth.uid() IS NOT NULL);
27: -- Verify the policies are working
28: SELECT schemaname, tablename, policyname, cmd, qual 
29: FROM pg_policies 
30: WHERE schemaname = 'public' 
31: ORDER BY tablename, policyname;
</file>

<file path="fix_user_profiles.sql">
 1: -- =====================================================
 2: -- MANUAL USER PROFILE FIX
 3: -- Run this after the main database_setup.sql
 4: -- =====================================================
 5: -- First, let's make sure the trigger function is bulletproof
 6: CREATE OR REPLACE FUNCTION handle_new_user()
 7: RETURNS TRIGGER 
 8: LANGUAGE plpgsql 
 9: SECURITY DEFINER
10: AS $$
11: BEGIN
12:   -- Insert user profile with maximum error handling
13:   INSERT INTO public.user_profiles (id, profile_type, display_name)
14:   VALUES (
15:     NEW.id,
16:     'user', -- Always default to 'user' 
17:     COALESCE(
18:       NEW.raw_user_meta_data->>'display_name',
19:       NEW.raw_user_meta_data->>'full_name', 
20:       NEW.raw_user_meta_data->>'name',
21:       split_part(NEW.email, '@', 1),
22:       'User'
23:     )
24:   );
25:   RETURN NEW;
26: EXCEPTION
27:   WHEN OTHERS THEN
28:     -- Log the error but don't block user creation
29:     RAISE WARNING 'Failed to create user profile for user %: %', NEW.id, SQLERRM;
30:     RETURN NEW;
31: END;
32: $$;
33: -- Recreate the trigger to make sure it's active
34: DROP TRIGGER IF EXISTS on_auth_user_created ON auth.users;
35: CREATE TRIGGER on_auth_user_created
36:   AFTER INSERT ON auth.users
37:   FOR EACH ROW EXECUTE FUNCTION handle_new_user();
38: -- Create profiles for any existing users who don't have them
39: INSERT INTO public.user_profiles (id, profile_type, display_name, created_at)
40: SELECT 
41:   au.id,
42:   'user' as profile_type,
43:   COALESCE(
44:     au.raw_user_meta_data->>'display_name',
45:     au.raw_user_meta_data->>'full_name', 
46:     au.raw_user_meta_data->>'name',
47:     split_part(au.email, '@', 1),
48:     'User'
49:   ) as display_name,
50:   au.created_at
51: FROM auth.users au
52: WHERE NOT EXISTS (
53:   SELECT 1 FROM public.user_profiles up 
54:   WHERE up.id = au.id
55: )
56: ON CONFLICT (id) DO NOTHING;
57: -- Check how many users we have vs profiles
58: DO $$
59: DECLARE
60:   user_count INTEGER;
61:   profile_count INTEGER;
62: BEGIN
63:   SELECT COUNT(*) INTO user_count FROM auth.users;
64:   SELECT COUNT(*) INTO profile_count FROM public.user_profiles;
65:   RAISE NOTICE 'Users in auth.users: %', user_count;
66:   RAISE NOTICE 'Profiles in user_profiles: %', profile_count;
67:   IF user_count = profile_count THEN
68:     RAISE NOTICE 'SUCCESS: All users have profiles!';
69:   ELSE
70:     RAISE NOTICE 'WARNING: % users missing profiles', (user_count - profile_count);
71:   END IF;
72: END;
73: $$;
</file>

<file path="fix_vectors_rls.sql">
 1: -- =====================================================
 2: -- FIX VECTORS TABLE RLS POLICIES
 3: -- =====================================================
 4: -- Drop existing policies if they exist
 5: DROP POLICY IF EXISTS "Users can view own vectors" ON vectors;
 6: DROP POLICY IF EXISTS "Users can insert own vectors" ON vectors;
 7: DROP POLICY IF EXISTS "Users can update own vectors" ON vectors;
 8: DROP POLICY IF EXISTS "Users can delete own vectors" ON vectors;
 9: -- Create comprehensive RLS policies for vectors table
10: CREATE POLICY "Users can view own vectors" ON vectors 
11:   FOR SELECT TO authenticated 
12:   USING (user_id = auth.uid());
13: CREATE POLICY "Users can insert own vectors" ON vectors 
14:   FOR INSERT TO authenticated 
15:   WITH CHECK (
16:     user_id = auth.uid() AND 
17:     EXISTS (SELECT 1 FROM clips WHERE id = clip_id AND user_id = auth.uid())
18:   );
19: CREATE POLICY "Users can update own vectors" ON vectors 
20:   FOR UPDATE TO authenticated 
21:   USING (user_id = auth.uid()) 
22:   WITH CHECK (user_id = auth.uid());
23: CREATE POLICY "Users can delete own vectors" ON vectors 
24:   FOR DELETE TO authenticated 
25:   USING (user_id = auth.uid());
26: -- Verify the policies are in place
27: SELECT schemaname, tablename, policyname, cmd, permissive, roles, qual 
28: FROM pg_policies 
29: WHERE tablename = 'vectors' 
30: ORDER BY policyname;
</file>

<file path="requirements.txt">
 1: # Existing dependencies (from your current system)
 2: av>=14.4.0
 3: pymediainfo>=6.0.0
 4: PyExifTool>=0.5.0
 5: opencv-python>=4.8.0
 6: typer[all]>=0.9.0
 7: rich>=13.4.0
 8: pydantic>=2.4.0
 9: structlog>=23.1.0
10: numpy>=1.24.0
11: pillow>=10.0.0
12: polyfile>=0.5.5
13: hachoir==3.3.0
14: python-dateutil>=2.8.2
15: transformers>=4.28.0
16: torch>=2.0.0
17: google-generativeai>=0.3.0
18: python-dotenv>=1.0.0
19: 
20: # New Supabase dependencies
21: supabase>=2.3.0
22: tiktoken>=0.5.0
23: openai>=1.0.0
24: structlog
</file>

<file path="simple_disable_rls.sql">
 1: -- =====================================================
 2: -- SIMPLE RLS DISABLE FOR TESTING
 3: -- =====================================================
 4: -- Disable RLS entirely on all tables for testing
 5: ALTER TABLE public.user_profiles DISABLE ROW LEVEL SECURITY;
 6: ALTER TABLE public.clips DISABLE ROW LEVEL SECURITY;  
 7: ALTER TABLE public.transcripts DISABLE ROW LEVEL SECURITY;
 8: ALTER TABLE public.analysis DISABLE ROW LEVEL SECURITY;
 9: -- Drop all policies entirely
10: DROP POLICY IF EXISTS "user_profiles_select" ON public.user_profiles;
11: DROP POLICY IF EXISTS "user_profiles_insert" ON public.user_profiles;
12: DROP POLICY IF EXISTS "user_profiles_update" ON public.user_profiles;
13: DROP POLICY IF EXISTS "clips_all" ON public.clips;
14: DROP POLICY IF EXISTS "transcripts_all" ON public.transcripts;
15: DROP POLICY IF EXISTS "analysis_all" ON public.analysis;
16: -- Drop any other existing policies
17: DROP POLICY IF EXISTS "Users can view own profile" ON user_profiles;
18: DROP POLICY IF EXISTS "Users can update own profile" ON user_profiles;
19: DROP POLICY IF EXISTS "Admins can view all profiles" ON user_profiles;
20: DROP POLICY IF EXISTS "Users can view own clips" ON clips;
21: DROP POLICY IF EXISTS "Users can insert own clips" ON clips;
22: DROP POLICY IF EXISTS "Users can update own clips" ON clips;
23: DROP POLICY IF EXISTS "Users can delete own clips" ON clips;
24: DROP POLICY IF EXISTS "Admins can view all clips" ON clips;
25: DROP POLICY IF EXISTS "Users can view own segments" ON segments;
26: DROP POLICY IF EXISTS "Users can insert own segments" ON segments;
27: DROP POLICY IF EXISTS "Users can update own segments" ON segments;
28: DROP POLICY IF EXISTS "Users can delete own segments" ON segments;
29: DROP POLICY IF EXISTS "Users can view own analysis" ON analysis;
30: DROP POLICY IF EXISTS "Users can insert own analysis" ON analysis;
31: DROP POLICY IF EXISTS "Users can view own vectors" ON vectors;
32: DROP POLICY IF EXISTS "Users can insert own vectors" ON vectors;
33: DROP POLICY IF EXISTS "Users can view own transcripts" ON transcripts;
34: DROP POLICY IF EXISTS "Users can insert own transcripts" ON transcripts;
35: -- Test simple query to confirm no recursion errors
36: SELECT count(*) as user_profile_count FROM public.user_profiles;
</file>

<file path="SUPABASE_FIX.sql">
 1: -- =====================================================
 2: -- SUPABASE USER PROFILE FIX - COPY AND PASTE THIS
 3: -- =====================================================
 4: -- Step 1: Fix the trigger function that's causing user creation to fail
 5: CREATE OR REPLACE FUNCTION handle_new_user()
 6: RETURNS TRIGGER 
 7: LANGUAGE plpgsql 
 8: SECURITY DEFINER
 9: AS $$
10: BEGIN
11:   -- Insert user profile with bulletproof error handling
12:   INSERT INTO public.user_profiles (id, profile_type, display_name)
13:   VALUES (
14:     NEW.id,
15:     'user', -- Always default to 'user' to avoid any metadata issues
16:     COALESCE(
17:       NEW.raw_user_meta_data->>'display_name',
18:       NEW.raw_user_meta_data->>'full_name', 
19:       NEW.raw_user_meta_data->>'name',
20:       split_part(NEW.email, '@', 1),
21:       'User'
22:     )
23:   );
24:   RETURN NEW;
25: EXCEPTION
26:   WHEN OTHERS THEN
27:     -- If profile creation fails, log warning but DON'T block user creation
28:     RAISE WARNING 'Failed to create user profile for user %: %', NEW.id, SQLERRM;
29:     RETURN NEW;
30: END;
31: $$;
32: -- Step 2: Recreate the trigger to make sure it's active
33: DROP TRIGGER IF EXISTS on_auth_user_created ON auth.users;
34: CREATE TRIGGER on_auth_user_created
35:   AFTER INSERT ON auth.users
36:   FOR EACH ROW EXECUTE FUNCTION handle_new_user();
37: -- Step 3: Create profiles for any existing users who don't have them
38: INSERT INTO public.user_profiles (id, profile_type, display_name, created_at)
39: SELECT 
40:   au.id,
41:   'user' as profile_type,
42:   COALESCE(
43:     au.raw_user_meta_data->>'display_name',
44:     au.raw_user_meta_data->>'full_name', 
45:     au.raw_user_meta_data->>'name',
46:     split_part(au.email, '@', 1),
47:     'User'
48:   ) as display_name,
49:   au.created_at
50: FROM auth.users au
51: WHERE NOT EXISTS (
52:   SELECT 1 FROM public.user_profiles up 
53:   WHERE up.id = au.id
54: )
55: ON CONFLICT (id) DO NOTHING;
56: -- Step 4: Check results
57: DO $$
58: DECLARE
59:   user_count INTEGER;
60:   profile_count INTEGER;
61: BEGIN
62:   SELECT COUNT(*) INTO user_count FROM auth.users;
63:   SELECT COUNT(*) INTO profile_count FROM public.user_profiles;
64:   RAISE NOTICE 'Users in auth.users: %', user_count;
65:   RAISE NOTICE 'Profiles in user_profiles: %', profile_count;
66:   IF user_count = profile_count THEN
67:     RAISE NOTICE '✅ SUCCESS: All users have profiles!';
68:   ELSE
69:     RAISE NOTICE '⚠️  WARNING: % users missing profiles', (user_count - profile_count);
70:   END IF;
71: END;
72: $$;
</file>

<file path="test_supabase.py">
 1: #!/usr/bin/env python3
 2: """
 3: Test script to verify Supabase integration for AI Ingesting Tool.
 4: """
 5: import os
 6: import sys
 7: sys.path.append(os.path.dirname(os.path.abspath(__file__)))
 8: from video_ingest_tool.supabase_config import verify_connection, get_database_status
 9: from video_ingest_tool.auth import AuthManager
10: from rich.console import Console
11: from rich.table import Table
12: console = Console()
13: def test_connection():
14:     """Test basic connection to Supabase."""
15:     console.print("[cyan]Testing Supabase connection...[/cyan]")
16:     if verify_connection():
17:         console.print("[green]✓ Connection successful[/green]")
18:         return True
19:     else:
20:         console.print("[red]✗ Connection failed[/red]")
21:         return False
22: def test_database_status():
23:     """Test database status and table existence."""
24:     console.print("[cyan]Checking database status...[/cyan]")
25:     status = get_database_status()
26:     if status['connection'] == 'success':
27:         console.print("[green]✓ Database connection successful[/green]")
28:         # Check tables
29:         table = Table(title="Database Tables")
30:         table.add_column("Table", style="cyan")
31:         table.add_column("Status", style="green")
32:         for table_name, table_status in status.get('tables', {}).items():
33:             status_text = "[green]Exists[/green]" if table_status == 'exists' else "[red]Missing[/red]"
34:             table.add_row(table_name, status_text)
35:         console.print(table)
36:         return True
37:     else:
38:         console.print(f"[red]✗ Database connection failed: {status.get('error', 'Unknown error')}[/red]")
39:         return False
40: def main():
41:     """Run all tests."""
42:     console.print("[bold]AI Ingesting Tool - Supabase Integration Test[/bold]")
43:     console.print("=" * 50)
44:     # Test connection
45:     if not test_connection():
46:         console.print("[red]Basic connection failed. Check your .env file.[/red]")
47:         return
48:     # Test database status
49:     test_database_status()
50:     console.print("\n[yellow]Next steps:[/yellow]")
51:     console.print("1. Run the database_setup.sql in your Supabase SQL Editor")
52:     console.print("2. Test authentication: python -m video_ingest_tool auth signup")
53:     console.print("3. Test login: python -m video_ingest_tool auth login")
54:     console.print("4. Check status: python -m video_ingest_tool auth status")
55: if __name__ == "__main__":
56:     main()
</file>

</files>
